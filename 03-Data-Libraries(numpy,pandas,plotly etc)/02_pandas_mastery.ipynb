{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¼ Pandas Mastery: Data Analysis Superpower\n",
    "\n",
    "<img src='https://pandas.pydata.org/static/img/pandas_white.svg' width='400' alt='Pandas Logo' style='background-color: #130754; padding: 20px;'>\n",
    "\n",
    "## ğŸ“š Welcome to Data Analysis Heaven!\n",
    "\n",
    "**Pandas** is the Swiss Army knife of data analysis in Python. If NumPy is the engine, Pandas is the luxury car built on top of it!\n",
    "\n",
    "### ğŸ¯ Why Pandas Rules the Data World:\n",
    "- **DataFrames** - Think Excel on steroids! ğŸ’ª\n",
    "- **Missing Data Handling** - Clean messy data like a pro\n",
    "- **Time Series** - Financial data analysis made easy\n",
    "- **Data Wrangling** - Merge, join, reshape with ease\n",
    "- **SQL-like Operations** - GROUP BY, JOIN, and more!\n",
    "- **File I/O** - Read/write CSV, Excel, JSON, SQL, and more\n",
    "\n",
    "### ğŸ“Š What We'll Master Today:\n",
    "1. **Series & DataFrames** - The building blocks\n",
    "2. **Data Loading & Inspection** - Get your data in\n",
    "3. **Indexing & Selection** - Access any data point\n",
    "4. **Data Cleaning** - Handle missing/duplicate data\n",
    "5. **Data Transformation** - Apply, map, and more\n",
    "6. **Grouping & Aggregation** - SQL-like operations\n",
    "7. **Merging & Joining** - Combine datasets\n",
    "8. **Time Series Analysis** - Work with dates\n",
    "9. **Visualization** - Quick plots\n",
    "10. **Real-World Projects** - Apply everything!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Let's Begin Our Journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(f\"ğŸ¼ Pandas Version: {pd.__version__}\")\n",
    "print(\"\\nâœ… Pandas loaded successfully! Let's analyze some data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 1: Series & DataFrames - The Building Blocks\n",
    "\n",
    "### ğŸ” Understanding Pandas Data Structures\n",
    "\n",
    "- **Series**: 1D labeled array (like a column in Excel)\n",
    "- **DataFrame**: 2D labeled table (like an Excel spreadsheet)\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*xfJGLLKJzJfzkRhLZE-anw.png' width='600' alt='Series vs DataFrame'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Creating a Series\n",
    "print(\"ğŸ“Š Creating Pandas Series\\n\" + \"=\"*40)\n",
    "\n",
    "# From a list\n",
    "temperatures = pd.Series([22, 24, 19, 23, 25, 18, 21])\n",
    "print(\"Simple Series:\")\n",
    "print(temperatures)\n",
    "\n",
    "# With custom index\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "temp_series = pd.Series([22, 24, 19, 23, 25, 18, 21], index=days, name='Temperature')\n",
    "print(\"\\nSeries with custom index:\")\n",
    "print(temp_series)\n",
    "\n",
    "# From dictionary\n",
    "sales_dict = {'iPhone': 500, 'Samsung': 450, 'Xiaomi': 300, 'OnePlus': 150}\n",
    "sales_series = pd.Series(sales_dict, name='Units Sold')\n",
    "print(\"\\nSeries from dictionary:\")\n",
    "print(sales_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Creating DataFrames\n",
    "print(\"ğŸ“Š Creating DataFrames\\n\" + \"=\"*40)\n",
    "\n",
    "# Method 1: From dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'],\n",
    "    'Salary': [70000, 85000, 95000, 65000, 78000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)\n",
    "\n",
    "# Method 2: From list of dictionaries\n",
    "records = [\n",
    "    {'Product': 'Laptop', 'Price': 1200, 'Quantity': 5},\n",
    "    {'Product': 'Mouse', 'Price': 25, 'Quantity': 50},\n",
    "    {'Product': 'Keyboard', 'Price': 75, 'Quantity': 30},\n",
    "    {'Product': 'Monitor', 'Price': 300, 'Quantity': 10}\n",
    "]\n",
    "\n",
    "products_df = pd.DataFrame(records)\n",
    "print(\"\\nDataFrame from list of dicts:\")\n",
    "print(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 DataFrame Information\n",
    "print(\"â„¹ï¸ DataFrame Information\\n\" + \"=\"*40)\n",
    "\n",
    "# Create a sample DataFrame\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.randn(100),\n",
    "    'B': np.random.randint(0, 50, 100),\n",
    "    'C': np.random.choice(['X', 'Y', 'Z'], 100),\n",
    "    'D': pd.date_range('2024-01-01', periods=100),\n",
    "    'E': np.random.random(100) * 1000\n",
    "})\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMemory usage:\")\n",
    "print(df.memory_usage())\n",
    "print(\"\\nBasic info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercise 1: Create Your First DataFrame\n",
    "\n",
    "Create a DataFrame containing information about 5 movies with columns:\n",
    "- Title, Year, Rating, Runtime (minutes), Revenue (millions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "\n",
    "# Solution:\n",
    "movies = pd.DataFrame({\n",
    "    'Title': ['Inception', 'The Matrix', 'Interstellar', 'The Dark Knight', 'Avatar'],\n",
    "    'Year': [2010, 1999, 2014, 2008, 2009],\n",
    "    'Rating': [8.8, 8.7, 8.6, 9.0, 7.8],\n",
    "    'Runtime': [148, 136, 169, 152, 162],\n",
    "    'Revenue': [836.8, 467.2, 677.5, 1004.6, 2847.2]\n",
    "})\n",
    "\n",
    "print(\"Movies DataFrame:\")\n",
    "print(movies)\n",
    "print(f\"\\nTotal revenue: ${movies['Revenue'].sum():.1f} million\")\n",
    "print(f\"Average rating: {movies['Rating'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 2: Data Loading & Inspection\n",
    "\n",
    "### ğŸ“ Working with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Creating Sample Data\n",
    "print(\"ğŸ“ Creating Sample Dataset\\n\" + \"=\"*40)\n",
    "\n",
    "# Generate a realistic sales dataset\n",
    "np.random.seed(42)\n",
    "n_records = 1000\n",
    "\n",
    "sales_data = pd.DataFrame({\n",
    "    'OrderID': range(1001, 1001 + n_records),\n",
    "    'Date': pd.date_range('2024-01-01', periods=n_records, freq='H'),\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'], n_records),\n",
    "    'Category': np.random.choice(['Electronics', 'Accessories'], n_records),\n",
    "    'Quantity': np.random.randint(1, 10, n_records),\n",
    "    'Price': np.random.uniform(50, 2000, n_records).round(2),\n",
    "    'Customer': ['Customer_' + str(i) for i in np.random.randint(1, 200, n_records)],\n",
    "    'City': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'], n_records),\n",
    "    'PaymentMethod': np.random.choice(['Credit', 'Debit', 'Cash', 'PayPal'], n_records)\n",
    "})\n",
    "\n",
    "# Add some missing values for realism\n",
    "sales_data.loc[sales_data.sample(50).index, 'City'] = np.nan\n",
    "sales_data.loc[sales_data.sample(30).index, 'PaymentMethod'] = np.nan\n",
    "\n",
    "print(\"Sample Sales Dataset Created!\")\n",
    "print(f\"Shape: {sales_data.shape}\")\n",
    "print(f\"Date range: {sales_data['Date'].min()} to {sales_data['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Data Inspection Methods\n",
    "print(\"ğŸ” Inspecting the Data\\n\" + \"=\"*40)\n",
    "\n",
    "# First few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(sales_data.head())\n",
    "\n",
    "# Last few rows\n",
    "print(\"\\nLast 3 rows:\")\n",
    "print(sales_data.tail(3))\n",
    "\n",
    "# Random sample\n",
    "print(\"\\nRandom 3 rows:\")\n",
    "print(sales_data.sample(3))\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nNumerical columns statistics:\")\n",
    "print(sales_data.describe())\n",
    "\n",
    "# Categorical columns\n",
    "print(\"\\nCategorical columns info:\")\n",
    "print(sales_data.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Missing Data Detection\n",
    "print(\"ğŸ” Missing Data Analysis\\n\" + \"=\"*40)\n",
    "\n",
    "# Check for missing values\n",
    "missing = sales_data.isnull().sum()\n",
    "missing_pct = (missing / len(sales_data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percentage': missing_pct.round(2)\n",
    "})\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing data pattern\n",
    "print(\"\\nRows with any missing values:\", sales_data.isnull().any(axis=1).sum())\n",
    "print(\"Complete rows:\", sales_data.notna().all(axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 3: Indexing & Selection - Access Your Data\n",
    "\n",
    "### ğŸ¯ Multiple Ways to Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Column Selection\n",
    "print(\"ğŸ“Š Column Selection\\n\" + \"=\"*40)\n",
    "\n",
    "# Single column (returns Series)\n",
    "products = sales_data['Product']\n",
    "print(\"Single column (Series):\")\n",
    "print(products.head())\n",
    "print(f\"Type: {type(products)}\")\n",
    "\n",
    "# Multiple columns (returns DataFrame)\n",
    "subset = sales_data[['Product', 'Price', 'Quantity']]\n",
    "print(\"\\nMultiple columns (DataFrame):\")\n",
    "print(subset.head())\n",
    "print(f\"Type: {type(subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Row Selection with loc and iloc\n",
    "print(\"ğŸ¯ Row Selection: loc vs iloc\\n\" + \"=\"*40)\n",
    "\n",
    "# Create a simple DataFrame with custom index\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
    "    'Score': [85, 92, 78, 95, 88],\n",
    "    'Grade': ['B', 'A', 'C', 'A', 'B']\n",
    "}, index=['ST001', 'ST002', 'ST003', 'ST004', 'ST005'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# loc: label-based selection\n",
    "print(\"\\nğŸ·ï¸ Using loc (label-based):\")\n",
    "print(\"Select row 'ST003':\")\n",
    "print(df.loc['ST003'])\n",
    "print(\"\\nSelect rows ST002 to ST004:\")\n",
    "print(df.loc['ST002':'ST004'])\n",
    "print(\"\\nSelect specific rows and columns:\")\n",
    "print(df.loc[['ST001', 'ST005'], ['Name', 'Score']])\n",
    "\n",
    "# iloc: integer position-based selection\n",
    "print(\"\\nğŸ”¢ Using iloc (position-based):\")\n",
    "print(\"Select row at position 2:\")\n",
    "print(df.iloc[2])\n",
    "print(\"\\nSelect rows 1 to 3:\")\n",
    "print(df.iloc[1:4])\n",
    "print(\"\\nSelect specific positions:\")\n",
    "print(df.iloc[[0, 4], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Boolean Indexing (Filtering)\n",
    "print(\"ğŸ” Boolean Indexing\\n\" + \"=\"*40)\n",
    "\n",
    "# Simple filter\n",
    "high_price = sales_data[sales_data['Price'] > 1000]\n",
    "print(f\"Products with price > $1000: {len(high_price)} records\")\n",
    "print(high_price[['Product', 'Price']].head())\n",
    "\n",
    "# Multiple conditions\n",
    "laptops_nyc = sales_data[\n",
    "    (sales_data['Product'] == 'Laptop') & \n",
    "    (sales_data['City'] == 'NYC')\n",
    "]\n",
    "print(f\"\\nLaptops sold in NYC: {len(laptops_nyc)} records\")\n",
    "\n",
    "# Using isin()\n",
    "tech_cities = sales_data[sales_data['City'].isin(['NYC', 'LA'])]\n",
    "print(f\"\\nSales in NYC or LA: {len(tech_cities)} records\")\n",
    "\n",
    "# Using query() method\n",
    "query_result = sales_data.query('Price > 500 and Quantity > 5')\n",
    "print(f\"\\nHigh value bulk orders: {len(query_result)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercise 2: Data Selection Challenge\n",
    "\n",
    "From the sales_data DataFrame:\n",
    "1. Find all Phone sales with quantity >= 5\n",
    "2. Select orders from January 2024\n",
    "3. Find the top 5 most expensive orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "\n",
    "# Solution:\n",
    "# 1. Phone sales with quantity >= 5\n",
    "phone_bulk = sales_data[(sales_data['Product'] == 'Phone') & (sales_data['Quantity'] >= 5)]\n",
    "print(f\"Phone bulk orders: {len(phone_bulk)} records\")\n",
    "\n",
    "# 2. January 2024 orders\n",
    "jan_orders = sales_data[sales_data['Date'].dt.month == 1]\n",
    "print(f\"\\nJanuary orders: {len(jan_orders)} records\")\n",
    "\n",
    "# 3. Top 5 most expensive orders\n",
    "sales_data['TotalValue'] = sales_data['Price'] * sales_data['Quantity']\n",
    "top_5 = sales_data.nlargest(5, 'TotalValue')[['OrderID', 'Product', 'TotalValue']]\n",
    "print(\"\\nTop 5 most expensive orders:\")\n",
    "print(top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 4: Data Cleaning - Handle Real-World Messiness\n",
    "\n",
    "### ğŸ§¹ Clean Your Data Like a Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Handling Missing Data\n",
    "print(\"ğŸ§¹ Handling Missing Data\\n\" + \"=\"*40)\n",
    "\n",
    "# Create sample data with missing values\n",
    "messy_data = sales_data.copy()\n",
    "\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(messy_data.isnull().sum()[messy_data.isnull().sum() > 0])\n",
    "\n",
    "# Method 1: Drop missing values\n",
    "clean_drop = messy_data.dropna()\n",
    "print(f\"\\nAfter dropna(): {len(clean_drop)} rows (lost {len(messy_data) - len(clean_drop)} rows)\")\n",
    "\n",
    "# Method 2: Fill with a value\n",
    "messy_data['City'].fillna('Unknown', inplace=True)\n",
    "messy_data['PaymentMethod'].fillna('Cash', inplace=True)\n",
    "\n",
    "print(\"\\nAfter fillna():\")\n",
    "print(messy_data.isnull().sum())\n",
    "\n",
    "# Method 3: Forward/Backward fill\n",
    "time_series = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=10),\n",
    "    'Value': [100, np.nan, np.nan, 150, 200, np.nan, 250, 300, np.nan, 350]\n",
    "})\n",
    "\n",
    "print(\"\\nTime series with missing values:\")\n",
    "print(time_series)\n",
    "\n",
    "print(\"\\nForward filled:\")\n",
    "print(time_series.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Removing Duplicates\n",
    "print(\"ğŸ”„ Removing Duplicates\\n\" + \"=\"*40)\n",
    "\n",
    "# Create data with duplicates\n",
    "dup_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 2, 3, 4, 4, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Bob', 'Charlie', 'David', 'David', 'David', 'Emma'],\n",
    "    'Score': [85, 90, 90, 75, 88, 88, 92, 95]\n",
    "})\n",
    "\n",
    "print(\"Original data with duplicates:\")\n",
    "print(dup_data)\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {dup_data.duplicated().sum()}\")\n",
    "print(\"\\nDuplicate rows details:\")\n",
    "print(dup_data[dup_data.duplicated(keep=False)])\n",
    "\n",
    "# Remove duplicates\n",
    "clean_data = dup_data.drop_duplicates()\n",
    "print(f\"\\nAfter removing duplicates: {len(clean_data)} rows\")\n",
    "print(clean_data)\n",
    "\n",
    "# Keep last occurrence\n",
    "keep_last = dup_data.drop_duplicates(subset=['ID'], keep='last')\n",
    "print(\"\\nKeeping last occurrence per ID:\")\n",
    "print(keep_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Data Type Conversions\n",
    "print(\"ğŸ”§ Data Type Conversions\\n\" + \"=\"*40)\n",
    "\n",
    "# Create sample data with wrong types\n",
    "wrong_types = pd.DataFrame({\n",
    "    'ID': ['001', '002', '003', '004'],\n",
    "    'Price': ['19.99', '29.99', '39.99', '49.99'],\n",
    "    'Date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04'],\n",
    "    'InStock': ['True', 'False', 'True', 'True']\n",
    "})\n",
    "\n",
    "print(\"Original data types:\")\n",
    "print(wrong_types.dtypes)\n",
    "print(\"\\nData:\")\n",
    "print(wrong_types)\n",
    "\n",
    "# Convert to appropriate types\n",
    "wrong_types['ID'] = wrong_types['ID'].astype(int)\n",
    "wrong_types['Price'] = wrong_types['Price'].astype(float)\n",
    "wrong_types['Date'] = pd.to_datetime(wrong_types['Date'])\n",
    "wrong_types['InStock'] = wrong_types['InStock'].map({'True': True, 'False': False})\n",
    "\n",
    "print(\"\\nCorrected data types:\")\n",
    "print(wrong_types.dtypes)\n",
    "print(\"\\nData after conversion:\")\n",
    "print(wrong_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 String Operations\n",
    "print(\"ğŸ“ String Operations\\n\" + \"=\"*40)\n",
    "\n",
    "# Create sample data\n",
    "text_data = pd.DataFrame({\n",
    "    'Name': ['  John Doe  ', 'jane smith', 'BOB JOHNSON', 'Alice_Brown'],\n",
    "    'Email': ['JOHN@GMAIL.COM', 'Jane@Yahoo.com', 'bob@HOTMAIL.COM', 'alice@outlook.com'],\n",
    "    'Phone': ['123-456-7890', '(555) 123-4567', '9876543210', '555.123.4567']\n",
    "})\n",
    "\n",
    "print(\"Original messy text data:\")\n",
    "print(text_data)\n",
    "\n",
    "# Clean the data\n",
    "text_data['Name'] = text_data['Name'].str.strip()  # Remove spaces\n",
    "text_data['Name'] = text_data['Name'].str.title()  # Title case\n",
    "text_data['Name'] = text_data['Name'].str.replace('_', ' ')  # Replace underscore\n",
    "\n",
    "text_data['Email'] = text_data['Email'].str.lower()  # Lowercase emails\n",
    "\n",
    "# Extract phone digits only\n",
    "text_data['Phone_Clean'] = text_data['Phone'].str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "print(\"\\nCleaned text data:\")\n",
    "print(text_data)\n",
    "\n",
    "# String contains\n",
    "gmail_users = text_data[text_data['Email'].str.contains('gmail')]\n",
    "print(f\"\\nGmail users: {len(gmail_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 5: Data Transformation - Apply, Map, and More\n",
    "\n",
    "### ğŸ”„ Transform Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Apply Functions\n",
    "print(\"ğŸ”§ Apply Functions\\n\" + \"=\"*40)\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Price': [100, 200, 150, 300, 250],\n",
    "    'Quantity': [2, 1, 3, 2, 4],\n",
    "    'Discount': [0.1, 0.15, 0.2, 0.05, 0.1]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Apply to Series\n",
    "df['Total'] = df['Price'] * df['Quantity']\n",
    "df['Final_Price'] = df['Total'].apply(lambda x: x * 0.9 if x > 500 else x)\n",
    "\n",
    "print(\"\\nAfter applying discount logic:\")\n",
    "print(df)\n",
    "\n",
    "# Apply to DataFrame rows\n",
    "def calculate_revenue(row):\n",
    "    base = row['Price'] * row['Quantity']\n",
    "    return base * (1 - row['Discount'])\n",
    "\n",
    "df['Revenue'] = df.apply(calculate_revenue, axis=1)\n",
    "\n",
    "print(\"\\nWith revenue calculation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Map and Replace\n",
    "print(\"ğŸ—ºï¸ Map and Replace\\n\" + \"=\"*40)\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Grade': ['A', 'B', 'C', 'A', 'B', 'D', 'A', 'C'],\n",
    "    'Status': ['Pass', 'Pass', 'Pass', 'Pass', 'Pass', 'Fail', 'Pass', 'Pass']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Map values\n",
    "grade_points = {'A': 4.0, 'B': 3.0, 'C': 2.0, 'D': 1.0, 'F': 0.0}\n",
    "df['GPA'] = df['Grade'].map(grade_points)\n",
    "\n",
    "# Replace values\n",
    "df['Status_Code'] = df['Status'].replace({'Pass': 1, 'Fail': 0})\n",
    "\n",
    "print(\"\\nAfter mapping and replacing:\")\n",
    "print(df)\n",
    "\n",
    "# Binning continuous values\n",
    "scores = pd.DataFrame({'Score': [45, 67, 89, 72, 91, 55, 78, 83, 62, 95]})\n",
    "bins = [0, 60, 70, 80, 90, 100]\n",
    "labels = ['F', 'D', 'C', 'B', 'A']\n",
    "scores['Grade'] = pd.cut(scores['Score'], bins=bins, labels=labels)\n",
    "\n",
    "print(\"\\nBinning scores into grades:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Creating New Features\n",
    "print(\"ğŸ—ï¸ Feature Engineering\\n\" + \"=\"*40)\n",
    "\n",
    "# Work with the sales data\n",
    "df = sales_data.copy()\n",
    "\n",
    "# Extract date features\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
    "df['Hour'] = df['Date'].dt.hour\n",
    "df['IsWeekend'] = df['Date'].dt.dayofweek.isin([5, 6])\n",
    "\n",
    "# Calculate features\n",
    "df['TotalValue'] = df['Price'] * df['Quantity']\n",
    "df['PriceCategory'] = pd.qcut(df['Price'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(df[['Date', 'Year', 'Month', 'Day', 'DayOfWeek', 'IsWeekend', 'PriceCategory']].head())\n",
    "\n",
    "# Aggregate features\n",
    "customer_stats = df.groupby('Customer').agg({\n",
    "    'OrderID': 'count',\n",
    "    'TotalValue': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nCustomer statistics (first 5):\")\n",
    "print(customer_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 6: Grouping & Aggregation - SQL-like Operations\n",
    "\n",
    "### ğŸ“Š GROUP BY in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Basic GroupBy\n",
    "print(\"ğŸ“Š GroupBy Operations\\n\" + \"=\"*40)\n",
    "\n",
    "# Group by single column\n",
    "product_groups = sales_data.groupby('Product')\n",
    "\n",
    "# Basic aggregations\n",
    "print(\"Sales by Product:\")\n",
    "print(product_groups['Price'].agg(['count', 'mean', 'sum']).round(2))\n",
    "\n",
    "# Multiple aggregations\n",
    "summary = sales_data.groupby('Product').agg({\n",
    "    'OrderID': 'count',\n",
    "    'Quantity': 'sum',\n",
    "    'Price': ['mean', 'min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns]\n",
    "print(\"\\nDetailed Product Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Multi-level GroupBy\n",
    "print(\"ğŸ“Š Multi-level GroupBy\\n\" + \"=\"*40)\n",
    "\n",
    "# Group by multiple columns\n",
    "city_product = sales_data.groupby(['City', 'Product'])['Price'].agg(['count', 'mean'])\n",
    "city_product = city_product.round(2)\n",
    "\n",
    "print(\"Sales by City and Product:\")\n",
    "print(city_product.head(10))\n",
    "\n",
    "# Pivot table alternative\n",
    "pivot = sales_data.pivot_table(\n",
    "    values='Price',\n",
    "    index='Product',\n",
    "    columns='City',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nPivot Table - Average Price by Product and City:\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Transform and Filter Groups\n",
    "print(\"ğŸ”„ Transform and Filter Groups\\n\" + \"=\"*40)\n",
    "\n",
    "# Add group statistics to original data\n",
    "df = sales_data.copy()\n",
    "df['TotalValue'] = df['Price'] * df['Quantity']\n",
    "\n",
    "# Add group means\n",
    "df['Product_Avg_Price'] = df.groupby('Product')['Price'].transform('mean')\n",
    "df['Price_vs_Avg'] = df['Price'] - df['Product_Avg_Price']\n",
    "\n",
    "print(\"Data with group statistics:\")\n",
    "print(df[['Product', 'Price', 'Product_Avg_Price', 'Price_vs_Avg']].head(10))\n",
    "\n",
    "# Filter groups\n",
    "# Keep only products with more than 100 sales\n",
    "popular_products = df.groupby('Product').filter(lambda x: len(x) > 100)\n",
    "print(f\"\\nProducts with >100 sales: {popular_products['Product'].nunique()}\")\n",
    "print(f\"Total records: {len(popular_products)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 7: Merging & Joining - Combine Your Data\n",
    "\n",
    "### ğŸ”— SQL-like JOINs in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Merge Operations\n",
    "print(\"ğŸ”— Merge Operations\\n\" + \"=\"*40)\n",
    "\n",
    "# Create sample DataFrames\n",
    "customers = pd.DataFrame({\n",
    "    'CustomerID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n",
    "    'City': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix']\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'OrderID': [101, 102, 103, 104, 105, 106],\n",
    "    'CustomerID': [1, 2, 1, 3, 5, 2],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone', 'Watch'],\n",
    "    'Amount': [1200, 800, 600, 1500, 900, 400]\n",
    "})\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)\n",
    "\n",
    "# Inner join (default)\n",
    "inner_join = pd.merge(customers, orders, on='CustomerID')\n",
    "print(\"\\nInner Join Result:\")\n",
    "print(inner_join)\n",
    "\n",
    "# Left join\n",
    "left_join = pd.merge(customers, orders, on='CustomerID', how='left')\n",
    "print(\"\\nLeft Join Result (all customers):\")\n",
    "print(left_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Concatenation\n",
    "print(\"ğŸ“š Concatenation\\n\" + \"=\"*40)\n",
    "\n",
    "# Create sample DataFrames\n",
    "q1_sales = pd.DataFrame({\n",
    "    'Month': ['Jan', 'Feb', 'Mar'],\n",
    "    'Sales': [100, 120, 140]\n",
    "})\n",
    "\n",
    "q2_sales = pd.DataFrame({\n",
    "    'Month': ['Apr', 'May', 'Jun'],\n",
    "    'Sales': [130, 150, 160]\n",
    "})\n",
    "\n",
    "# Vertical concatenation\n",
    "half_year = pd.concat([q1_sales, q2_sales], ignore_index=True)\n",
    "print(\"Vertical Concatenation:\")\n",
    "print(half_year)\n",
    "\n",
    "# Horizontal concatenation\n",
    "revenue = pd.DataFrame({\n",
    "    'Month': ['Jan', 'Feb', 'Mar'],\n",
    "    'Revenue': [10000, 12000, 14000]\n",
    "})\n",
    "\n",
    "combined = pd.concat([q1_sales, revenue[['Revenue']]], axis=1)\n",
    "print(\"\\nHorizontal Concatenation:\")\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 8: Time Series Analysis\n",
    "\n",
    "### â° Working with Dates and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Date Operations\n",
    "print(\"ğŸ“… Date Operations\\n\" + \"=\"*40)\n",
    "\n",
    "# Create time series data\n",
    "dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Sales': np.random.randn(365).cumsum() + 100,\n",
    "    'Temperature': np.sin(np.arange(365) * 2 * np.pi / 365) * 20 + 20\n",
    "})\n",
    "\n",
    "ts_data.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"Time Series Data:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# Resampling\n",
    "monthly = ts_data.resample('M').mean()\n",
    "print(\"\\nMonthly Averages:\")\n",
    "print(monthly.head())\n",
    "\n",
    "# Rolling windows\n",
    "ts_data['Sales_MA7'] = ts_data['Sales'].rolling(window=7).mean()\n",
    "ts_data['Sales_MA30'] = ts_data['Sales'].rolling(window=30).mean()\n",
    "\n",
    "print(\"\\nWith Moving Averages:\")\n",
    "print(ts_data[['Sales', 'Sales_MA7', 'Sales_MA30']].head(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Date Range Filtering\n",
    "print(\"ğŸ” Date Filtering\\n\" + \"=\"*40)\n",
    "\n",
    "# Filter by date range\n",
    "jan_data = ts_data['2024-01']\n",
    "print(f\"January 2024 data: {len(jan_data)} days\")\n",
    "print(jan_data.head())\n",
    "\n",
    "# Between dates\n",
    "q1_data = ts_data['2024-01-01':'2024-03-31']\n",
    "print(f\"\\nQ1 2024 data: {len(q1_data)} days\")\n",
    "\n",
    "# Shift operations\n",
    "ts_data['Sales_Yesterday'] = ts_data['Sales'].shift(1)\n",
    "ts_data['Sales_Change'] = ts_data['Sales'] - ts_data['Sales_Yesterday']\n",
    "\n",
    "print(\"\\nWith lag features:\")\n",
    "print(ts_data[['Sales', 'Sales_Yesterday', 'Sales_Change']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Section 9: Visualization with Pandas\n",
    "\n",
    "### ğŸ“ˆ Quick Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Basic Plotting\n",
    "print(\"ğŸ“ˆ Pandas Plotting\\n\" + \"=\"*40)\n",
    "\n",
    "# Note: In real notebook, these would display as plots\n",
    "# Here we'll describe what would be plotted\n",
    "\n",
    "# Line plot\n",
    "print(\"1. Line Plot: ts_data['Sales'].plot()\")\n",
    "print(\"   - Shows sales trend over time\")\n",
    "\n",
    "# Bar plot\n",
    "product_sales = sales_data.groupby('Product')['Price'].count()\n",
    "print(\"\\n2. Bar Plot: product_sales.plot(kind='bar')\")\n",
    "print(\"   - Shows count of sales per product\")\n",
    "\n",
    "# Histogram\n",
    "print(\"\\n3. Histogram: sales_data['Price'].plot(kind='hist', bins=30)\")\n",
    "print(\"   - Shows distribution of prices\")\n",
    "\n",
    "# Box plot\n",
    "print(\"\\n4. Box Plot: sales_data.boxplot(column='Price', by='Product')\")\n",
    "print(\"   - Shows price distribution by product\")\n",
    "\n",
    "# Scatter plot\n",
    "print(\"\\n5. Scatter Plot: sales_data.plot(x='Quantity', y='Price', kind='scatter')\")\n",
    "print(\"   - Shows relationship between quantity and price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Section 10: Real-World Projects\n",
    "\n",
    "### Project 1: Customer Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 1: Customer Analytics\n",
    "print(\"ğŸ“Š CUSTOMER ANALYTICS DASHBOARD\\n\" + \"=\"*50)\n",
    "\n",
    "# Prepare the data\n",
    "df = sales_data.copy()\n",
    "df['TotalValue'] = df['Price'] * df['Quantity']\n",
    "df['Month'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "# 1. Customer Segmentation\n",
    "customer_summary = df.groupby('Customer').agg({\n",
    "    'OrderID': 'count',\n",
    "    'TotalValue': 'sum',\n",
    "    'Date': lambda x: (df['Date'].max() - x.max()).days\n",
    "}).rename(columns={\n",
    "    'OrderID': 'Orders',\n",
    "    'TotalValue': 'Revenue',\n",
    "    'Date': 'DaysSinceLastOrder'\n",
    "})\n",
    "\n",
    "# Categorize customers\n",
    "customer_summary['Segment'] = pd.cut(\n",
    "    customer_summary['Revenue'],\n",
    "    bins=[0, 1000, 5000, 10000, float('inf')],\n",
    "    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    ")\n",
    "\n",
    "print(\"Customer Segmentation:\")\n",
    "print(customer_summary.head(10))\n",
    "\n",
    "# 2. Segment Analysis\n",
    "segment_analysis = customer_summary.groupby('Segment').agg({\n",
    "    'Orders': ['count', 'mean'],\n",
    "    'Revenue': ['sum', 'mean'],\n",
    "    'DaysSinceLastOrder': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nSegment Analysis:\")\n",
    "print(segment_analysis)\n",
    "\n",
    "# 3. Top Customers\n",
    "top_customers = customer_summary.nlargest(5, 'Revenue')[['Orders', 'Revenue', 'Segment']]\n",
    "print(\"\\nTop 5 Customers:\")\n",
    "print(top_customers)\n",
    "\n",
    "# 4. Churn Risk\n",
    "churn_risk = customer_summary[customer_summary['DaysSinceLastOrder'] > 30].sort_values(\n",
    "    'Revenue', ascending=False\n",
    ").head(10)[['Orders', 'Revenue', 'DaysSinceLastOrder']]\n",
    "\n",
    "print(\"\\nChurn Risk (No order in 30+ days):\")\n",
    "print(churn_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2: Sales Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 2: Sales Performance Report\n",
    "print(\"ğŸ“ˆ SALES PERFORMANCE REPORT\\n\" + \"=\"*50)\n",
    "\n",
    "# Monthly trends\n",
    "monthly_sales = df.groupby('Month').agg({\n",
    "    'OrderID': 'count',\n",
    "    'TotalValue': 'sum',\n",
    "    'Customer': 'nunique'\n",
    "}).rename(columns={\n",
    "    'OrderID': 'Orders',\n",
    "    'TotalValue': 'Revenue',\n",
    "    'Customer': 'UniqueCustomers'\n",
    "})\n",
    "\n",
    "# Calculate growth\n",
    "monthly_sales['Revenue_Growth'] = monthly_sales['Revenue'].pct_change() * 100\n",
    "monthly_sales['AOV'] = monthly_sales['Revenue'] / monthly_sales['Orders']\n",
    "\n",
    "print(\"Monthly Performance:\")\n",
    "print(monthly_sales.round(2))\n",
    "\n",
    "# Product performance\n",
    "product_performance = df.groupby('Product').agg({\n",
    "    'OrderID': 'count',\n",
    "    'TotalValue': 'sum',\n",
    "    'Quantity': 'sum'\n",
    "}).rename(columns={\n",
    "    'OrderID': 'Orders',\n",
    "    'TotalValue': 'Revenue',\n",
    "    'Quantity': 'Units'\n",
    "})\n",
    "\n",
    "product_performance['Avg_Order_Value'] = (\n",
    "    product_performance['Revenue'] / product_performance['Orders']\n",
    ").round(2)\n",
    "\n",
    "product_performance['Revenue_Share'] = (\n",
    "    product_performance['Revenue'] / product_performance['Revenue'].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nProduct Performance:\")\n",
    "print(product_performance.sort_values('Revenue', ascending=False))\n",
    "\n",
    "# City performance\n",
    "city_performance = df.dropna(subset=['City']).groupby('City').agg({\n",
    "    'TotalValue': ['sum', 'mean'],\n",
    "    'OrderID': 'count'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nCity Performance:\")\n",
    "print(city_performance)\n",
    "\n",
    "# Key Metrics\n",
    "print(\"\\nğŸ“Š KEY METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Revenue: ${df['TotalValue'].sum():,.2f}\")\n",
    "print(f\"Total Orders: {df['OrderID'].nunique():,}\")\n",
    "print(f\"Unique Customers: {df['Customer'].nunique()}\")\n",
    "print(f\"Average Order Value: ${df['TotalValue'].mean():.2f}\")\n",
    "print(f\"Best Selling Product: {product_performance.idxmax()['Revenue']}\")\n",
    "print(f\"Top City: {city_performance.idxmax()[('TotalValue', 'sum')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Advanced Pandas Techniques\n",
    "\n",
    "### ğŸš€ Pro-Level Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Method Chaining\n",
    "print(\"â›“ï¸ Method Chaining\\n\" + \"=\"*40)\n",
    "\n",
    "# Clean, elegant data processing\n",
    "result = (\n",
    "    sales_data\n",
    "    .assign(TotalValue=lambda x: x['Price'] * x['Quantity'])\n",
    "    .query('TotalValue > 1000')\n",
    "    .groupby('Product')\n",
    "    .agg({'TotalValue': 'sum', 'OrderID': 'count'})\n",
    "    .rename(columns={'TotalValue': 'Revenue', 'OrderID': 'Orders'})\n",
    "    .sort_values('Revenue', ascending=False)\n",
    "    .head()\n",
    ")\n",
    "\n",
    "print(\"Top products by revenue (>$1000 orders only):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: MultiIndex Operations\n",
    "print(\"ğŸ“š MultiIndex Operations\\n\" + \"=\"*40)\n",
    "\n",
    "# Create MultiIndex DataFrame\n",
    "multi_df = df.groupby(['City', 'Product'])['TotalValue'].sum().round(2)\n",
    "print(\"MultiIndex Series:\")\n",
    "print(multi_df.head(10))\n",
    "\n",
    "# Access MultiIndex data\n",
    "print(\"\\nAccess NYC data:\")\n",
    "print(multi_df.loc['NYC'])\n",
    "\n",
    "# Unstack to pivot\n",
    "pivoted = multi_df.unstack(fill_value=0)\n",
    "print(\"\\nUnstacked (Pivoted):\")\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Window Functions\n",
    "print(\"ğŸªŸ Window Functions\\n\" + \"=\"*40)\n",
    "\n",
    "# Ranking within groups\n",
    "df['Rank_in_City'] = df.groupby('City')['TotalValue'].rank(method='dense', ascending=False)\n",
    "\n",
    "# Cumulative sum within groups\n",
    "df['Cumulative_City_Revenue'] = df.groupby('City')['TotalValue'].cumsum()\n",
    "\n",
    "# Percentage of group total\n",
    "df['Pct_of_City_Total'] = df.groupby('City')['TotalValue'].transform(lambda x: x / x.sum() * 100)\n",
    "\n",
    "print(\"Window function results:\")\n",
    "print(df[['City', 'Product', 'TotalValue', 'Rank_in_City', \n",
    "          'Cumulative_City_Revenue', 'Pct_of_City_Total']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ† Final Challenge: Complete E-Commerce Analysis\n",
    "\n",
    "### Build a Complete Analytics Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project: E-Commerce Analytics Pipeline\n",
    "print(\"ğŸ¯ E-COMMERCE ANALYTICS PIPELINE\\n\" + \"=\"*50)\n",
    "\n",
    "class EcommerceAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data.copy()\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare and clean data\"\"\"\n",
    "        self.data['TotalValue'] = self.data['Price'] * self.data['Quantity']\n",
    "        self.data['Month'] = self.data['Date'].dt.to_period('M')\n",
    "        self.data['DayOfWeek'] = self.data['Date'].dt.day_name()\n",
    "        self.data['Hour'] = self.data['Date'].dt.hour\n",
    "    \n",
    "    def executive_summary(self):\n",
    "        \"\"\"Generate executive summary\"\"\"\n",
    "        summary = {\n",
    "            'Total Revenue': f\"${self.data['TotalValue'].sum():,.2f}\",\n",
    "            'Total Orders': self.data['OrderID'].nunique(),\n",
    "            'Unique Customers': self.data['Customer'].nunique(),\n",
    "            'Average Order Value': f\"${self.data['TotalValue'].mean():.2f}\",\n",
    "            'Products Sold': self.data['Quantity'].sum(),\n",
    "            'Conversion Rate': f\"{(self.data['Customer'].nunique() / 200 * 100):.1f}%\"\n",
    "        }\n",
    "        return pd.Series(summary)\n",
    "    \n",
    "    def customer_lifetime_value(self):\n",
    "        \"\"\"Calculate CLV\"\"\"\n",
    "        clv = self.data.groupby('Customer').agg({\n",
    "            'TotalValue': 'sum',\n",
    "            'OrderID': 'count',\n",
    "            'Date': lambda x: (self.data['Date'].max() - x.min()).days\n",
    "        }).rename(columns={\n",
    "            'TotalValue': 'CLV',\n",
    "            'OrderID': 'Orders',\n",
    "            'Date': 'CustomerAge_Days'\n",
    "        })\n",
    "        \n",
    "        clv['Avg_Order_Value'] = clv['CLV'] / clv['Orders']\n",
    "        return clv.sort_values('CLV', ascending=False)\n",
    "    \n",
    "    def product_analytics(self):\n",
    "        \"\"\"Product performance analysis\"\"\"\n",
    "        products = self.data.groupby('Product').agg({\n",
    "            'TotalValue': 'sum',\n",
    "            'Quantity': 'sum',\n",
    "            'OrderID': 'count',\n",
    "            'Customer': 'nunique'\n",
    "        }).rename(columns={\n",
    "            'TotalValue': 'Revenue',\n",
    "            'Quantity': 'Units_Sold',\n",
    "            'OrderID': 'Orders',\n",
    "            'Customer': 'Unique_Buyers'\n",
    "        })\n",
    "        \n",
    "        products['Avg_Price'] = products['Revenue'] / products['Units_Sold']\n",
    "        products['Market_Share'] = products['Revenue'] / products['Revenue'].sum() * 100\n",
    "        \n",
    "        return products.round(2)\n",
    "    \n",
    "    def time_analysis(self):\n",
    "        \"\"\"Time-based patterns\"\"\"\n",
    "        # Best hours\n",
    "        hourly = self.data.groupby('Hour')['TotalValue'].agg(['sum', 'count'])\n",
    "        best_hours = hourly.nlargest(3, 'sum')\n",
    "        \n",
    "        # Best days\n",
    "        daily = self.data.groupby('DayOfWeek')['TotalValue'].sum()\n",
    "        best_day = daily.idxmax()\n",
    "        \n",
    "        return {\n",
    "            'Best Hours': best_hours.index.tolist(),\n",
    "            'Best Day': best_day,\n",
    "            'Weekend vs Weekday': {\n",
    "                'Weekend': daily[['Saturday', 'Sunday']].sum(),\n",
    "                'Weekday': daily[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']].sum()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "analyzer = EcommerceAnalyzer(sales_data)\n",
    "\n",
    "print(\"ğŸ“Š EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(analyzer.executive_summary())\n",
    "\n",
    "print(\"\\nğŸ’° TOP CUSTOMER LIFETIME VALUES\")\n",
    "print(\"=\" * 50)\n",
    "print(analyzer.customer_lifetime_value().head())\n",
    "\n",
    "print(\"\\nğŸ“¦ PRODUCT ANALYTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(analyzer.product_analytics())\n",
    "\n",
    "print(\"\\nâ° TIME ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "time_insights = analyzer.time_analysis()\n",
    "print(f\"Best Hours for Sales: {time_insights['Best Hours']}\")\n",
    "print(f\"Best Day of Week: {time_insights['Best Day']}\")\n",
    "print(f\"Weekend Revenue: ${time_insights['Weekend vs Weekday']['Weekend']:,.2f}\")\n",
    "print(f\"Weekday Revenue: ${time_insights['Weekend vs Weekday']['Weekday']:,.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Summary & Next Steps\n",
    "\n",
    "### ğŸ† What You've Mastered:\n",
    "\n",
    "âœ… **Data Structures**\n",
    "- Series and DataFrames\n",
    "- MultiIndex operations\n",
    "\n",
    "âœ… **Data Manipulation**\n",
    "- Indexing and selection\n",
    "- Filtering and boolean indexing\n",
    "- Apply, map, and transform\n",
    "\n",
    "âœ… **Data Cleaning**\n",
    "- Missing data handling\n",
    "- Duplicate removal\n",
    "- Type conversions\n",
    "\n",
    "âœ… **Data Analysis**\n",
    "- GroupBy operations\n",
    "- Aggregations\n",
    "- Pivot tables\n",
    "\n",
    "âœ… **Data Combination**\n",
    "- Merging and joining\n",
    "- Concatenation\n",
    "\n",
    "âœ… **Time Series**\n",
    "- Date operations\n",
    "- Resampling\n",
    "- Rolling windows\n",
    "\n",
    "âœ… **Real-World Applications**\n",
    "- Customer analytics\n",
    "- Sales performance\n",
    "- E-commerce analysis\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "1. **Practice with Real Datasets**: Kaggle, UCI ML Repository\n",
    "2. **Learn Visualization**: Matplotlib, Seaborn, Plotly\n",
    "3. **SQL Integration**: Read/write from databases\n",
    "4. **Big Data**: Dask, PySpark for larger datasets\n",
    "5. **Machine Learning**: Feed data to scikit-learn\n",
    "\n",
    "### ğŸ’¡ Pro Tips:\n",
    "\n",
    "- **Use vectorized operations** instead of loops\n",
    "- **Chain methods** for cleaner code\n",
    "- **Profile memory usage** with `.memory_usage()`\n",
    "- **Use categories** for string columns to save memory\n",
    "- **Read documentation**: pandas.pydata.org\n",
    "\n",
    "### ğŸ“š Resources:\n",
    "\n",
    "- Official Pandas Documentation\n",
    "- Pandas Cookbook by Ted Petrou\n",
    "- Python for Data Analysis by Wes McKinney\n",
    "- Kaggle Learn Pandas Course\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've mastered Pandas - the Swiss Army knife of data analysis!\n",
    "\n",
    "With Pandas, you can now:\n",
    "- **Clean messy data** ğŸ§¹\n",
    "- **Analyze complex datasets** ğŸ“Š\n",
    "- **Generate business insights** ğŸ’¡\n",
    "- **Prepare data for ML** ğŸ¤–\n",
    "- **Create reports** ğŸ“ˆ\n",
    "\n",
    "**Keep practicing, keep analyzing, and keep discovering insights with Pandas!** ğŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŠ Course Complete!\n",
    "print(\"ğŸŠ\" * 20)\n",
    "print(\"\\n    ğŸ† PANDAS MASTERY ACHIEVED! ğŸ†\")\n",
    "print(\"\\n    You're now ready to:\")\n",
    "print(\"    â†’ Analyze any dataset\")\n",
    "print(\"    â†’ Clean messy data\")\n",
    "print(\"    â†’ Generate insights\")\n",
    "print(\"    â†’ Build data pipelines\")\n",
    "print(\"\\n    Next: Visualization with Matplotlib! ğŸ“ˆ\")\n",
    "print(\"\\n\" + \"ğŸŠ\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}