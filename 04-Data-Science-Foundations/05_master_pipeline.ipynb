{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Master Pipeline: End-to-End Data Science Project Pipeline\n",
        "![](https://images.unsplash.com/photo-1639322537228-f710d846310a?w=1200&h=300&fit=crop)\n",
        "## 🚀 Complete Data Science Workflow\n",
        "This is where everything comes together! We'll build:\n",
        "\n",
        "- Complete preprocessing pipeline\n",
        "- Feature engineering automation\n",
        "- Model-ready transformations\n",
        "- Production-grade code\n",
        "- Real business case study\n",
        "\n",
        "## 📚 What We'll Build:\n",
        "1. Data Pipeline Architecture - Modular design\n",
        "2. Custom Transformers - Sklearn compatible\n",
        "3. Feature Engineering Pipeline - Automated features\n",
        "4. Preprocessing Pipeline - End-to-end cleaning\n",
        "5. Validation Framework - Quality checks\n",
        "6. Model Preparation - Ready for ML\n",
        "7. Pipeline Persistence - Save and load\n",
        "8. Production Deployment - API ready\n",
        "9. Monitoring & Logging - Track everything\n",
        "10. Complete Project - Customer churn prediction\n",
        "\n",
        "## 🏗️ Let's Build Production Pipelines!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Master Pipeline - Ready to Build!\n",
            "\n",
            "💡 Building production-ready data pipelines!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"🎯 Master Pipeline - Ready to Build!\")\n",
        "print(\"\\n💡 Building production-ready data pipelines!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 1: Custom Transformers\n",
        "### 🎯 Building Sklearn-Compatible Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 CUSTOM TRANSFORMERS\n",
            "========================================\n",
            "✅ Custom transformers created:\n",
            "  • OutlierRemover\n",
            "  • MissingIndicator\n",
            "  • FeatureEngineer\n"
          ]
        }
      ],
      "source": [
        "print(\"🔧 CUSTOM TRANSFORMERS\\n\" + \"=\"*40)\n",
        "\n",
        "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Remove outliers using IQR method - works with both DataFrames and arrays\"\"\"\n",
        "    \n",
        "    def __init__(self, factor=1.5):\n",
        "        self.factor = factor\n",
        "        self.bounds_ = {}\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "            self.feature_names_ = list(range(X.shape[1]))\n",
        "        else:\n",
        "            X_df = X\n",
        "            self.feature_names_ = X.columns.tolist()\n",
        "        \n",
        "        for i, column in enumerate(X_df.columns):\n",
        "            if X_df.iloc[:, i].dtype in ['int64', 'float64', 'float32', 'int32']:\n",
        "                Q1 = X_df.iloc[:, i].quantile(0.25)\n",
        "                Q3 = X_df.iloc[:, i].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                self.bounds_[i] = (Q1 - self.factor * IQR, Q3 + self.factor * IQR)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X.copy()\n",
        "        \n",
        "        for col_idx, (lower, upper) in self.bounds_.items():\n",
        "            X_df.iloc[:, col_idx] = X_df.iloc[:, col_idx].clip(lower, upper)\n",
        "        \n",
        "        logger.info(f\"Outliers capped for {len(self.bounds_)} columns\")\n",
        "        \n",
        "        if isinstance(X, np.ndarray):\n",
        "            return X_df.values\n",
        "        else:\n",
        "            return X_df\n",
        "\n",
        "\n",
        "class MissingIndicator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Add binary indicators for missing values\"\"\"\n",
        "    \n",
        "    def __init__(self, threshold=0.1):\n",
        "        self.threshold = threshold\n",
        "        self.columns_ = []\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X\n",
        "        \n",
        "        missing_pct = X_df.isnull().sum() / len(X_df)\n",
        "        self.columns_ = list(missing_pct[missing_pct > self.threshold].index)\n",
        "        \n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            self.feature_names_ = X.columns.tolist()\n",
        "        else:\n",
        "            self.feature_names_ = list(range(X.shape[1]))\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X.copy()\n",
        "        \n",
        "        for col in self.columns_:\n",
        "            X_df[f'missing_indicator_{col}'] = X_df.iloc[:, col].isnull().astype(int)\n",
        "        \n",
        "        logger.info(f\"Added {len(self.columns_)} missing indicators\")\n",
        "        \n",
        "        if isinstance(X, np.ndarray):\n",
        "            return X_df.values\n",
        "        else:\n",
        "            return X_df\n",
        "\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Create new features from existing ones\"\"\"\n",
        "    \n",
        "    def __init__(self, create_interactions=True, create_ratios=True):\n",
        "        self.create_interactions = create_interactions\n",
        "        self.create_ratios = create_ratios\n",
        "        self.numeric_columns_ = []\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X\n",
        "            \n",
        "        self.numeric_columns_ = []\n",
        "        for i, col in enumerate(X_df.columns):\n",
        "            if X_df.iloc[:, i].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
        "                self.numeric_columns_.append(i)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X.copy()\n",
        "        \n",
        "        for col_idx in self.numeric_columns_:\n",
        "            if col_idx < X_df.shape[1]:\n",
        "                col_name = X_df.columns[col_idx]\n",
        "                X_df[f'{col_name}_squared'] = X_df.iloc[:, col_idx] ** 2\n",
        "                X_df[f'{col_name}_log'] = np.log1p(np.abs(X_df.iloc[:, col_idx]))\n",
        "        \n",
        "        if self.create_interactions and len(self.numeric_columns_) > 1:\n",
        "            cols_for_interaction = self.numeric_columns_[:min(5, len(self.numeric_columns_))]\n",
        "            for i, col1_idx in enumerate(cols_for_interaction[:-1]):\n",
        "                for col2_idx in cols_for_interaction[i+1:i+2]:\n",
        "                    if col1_idx < X_df.shape[1] and col2_idx < X_df.shape[1]:\n",
        "                        col1_name = X_df.columns[col1_idx]\n",
        "                        col2_name = X_df.columns[col2_idx]\n",
        "                        X_df[f'{col1_name}_x_{col2_name}'] = X_df.iloc[:, col1_idx] * X_df.iloc[:, col2_idx]\n",
        "        \n",
        "        if self.create_ratios and len(self.numeric_columns_) > 1:\n",
        "            cols_for_ratio = self.numeric_columns_[:min(3, len(self.numeric_columns_))]\n",
        "            for i, col1_idx in enumerate(cols_for_ratio[:-1]):\n",
        "                for col2_idx in cols_for_ratio[i+1:i+2]:\n",
        "                    if col1_idx < X_df.shape[1] and col2_idx < X_df.shape[1]:\n",
        "                        col1_name = X_df.columns[col1_idx]\n",
        "                        col2_name = X_df.columns[col2_idx]\n",
        "                        X_df[f'{col1_name}_div_{col2_name}'] = X_df.iloc[:, col1_idx] / (X_df.iloc[:, col2_idx] + 1e-8)\n",
        "        \n",
        "        logger.info(f\"Created {X_df.shape[1] - X.shape[1]} new features\")\n",
        "        \n",
        "        if isinstance(X, np.ndarray):\n",
        "            return X_df.values\n",
        "        else:\n",
        "            return X_df\n",
        "\n",
        "\n",
        "print(\"✅ Custom transformers created:\")\n",
        "print(\"  • OutlierRemover\")\n",
        "print(\"  • MissingIndicator\")\n",
        "print(\"  • FeatureEngineer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 2: Pipeline Architecture\n",
        "### 🎯 Building Modular Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 CREATING SAMPLE DATASET\n",
            "========================================\n",
            "Dataset shape: (5000, 18)\n",
            "Churn rate: 31.0%\n",
            "Missing values: 1500\n",
            "\n",
            "First 5 rows:\n",
            "   customer_id        age  tenure_months  monthly_charges  total_charges  \\\n",
            "0            1  52.450712       4.420973        28.998892            NaN   \n",
            "1            2  42.926035       5.055371        42.243909     250.571324   \n",
            "2            3  54.715328      14.820140        22.840260     282.019941   \n",
            "3            4  67.845448       8.094345        49.860711     346.586247   \n",
            "4            5  41.487699       6.823322        93.380864     562.060393   \n",
            "\n",
            "   num_services   contract_type payment_method internet_service tech_support  \\\n",
            "0             4  Month-to-month     Electronic      Fiber optic           No   \n",
            "1             2        Two year  Bank transfer              DSL           No   \n",
            "2             3  Month-to-month     Electronic               No           No   \n",
            "3             2        One year    Credit card      Fiber optic           No   \n",
            "4             1  Month-to-month  Bank transfer              DSL           No   \n",
            "\n",
            "  online_security device_protection streaming_tv streaming_movies  \\\n",
            "0             Yes                No          Yes              Yes   \n",
            "1             Yes                No           No              Yes   \n",
            "2             Yes                No          Yes               No   \n",
            "3              No                No           No               No   \n",
            "4              No               Yes          Yes               No   \n",
            "\n",
            "   satisfaction_score  support_tickets  late_payments  churn  \n",
            "0                 NaN              2.0              0      0  \n",
            "1            4.079777              3.0              0      0  \n",
            "2            3.974223              2.0              0      0  \n",
            "3            3.819199              0.0              2      0  \n",
            "4            1.042771              4.0              1      1  \n"
          ]
        }
      ],
      "source": [
        "print(\"📊 CREATING SAMPLE DATASET\\n\" + \"=\"*40)\n",
        "\n",
        "# Generate realistic customer churn dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 5000\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'customer_id': range(1, n_samples + 1),\n",
        "    'age': np.random.normal(45, 15, n_samples).clip(18, 80),\n",
        "    'tenure_months': np.random.exponential(24, n_samples),\n",
        "    'monthly_charges': np.random.gamma(2, 40, n_samples),\n",
        "    'total_charges': np.random.lognormal(7, 1.5, n_samples),\n",
        "    'num_services': np.random.poisson(3, n_samples),\n",
        "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.5, 0.3, 0.2]),\n",
        "    'payment_method': np.random.choice(['Electronic', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
        "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples, p=[0.3, 0.5, 0.2]),\n",
        "    'tech_support': np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7]),\n",
        "    'online_security': np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6]),\n",
        "    'device_protection': np.random.choice(['Yes', 'No'], n_samples, p=[0.35, 0.65]),\n",
        "    'streaming_tv': np.random.choice(['Yes', 'No'], n_samples, p=[0.45, 0.55]),\n",
        "    'streaming_movies': np.random.choice(['Yes', 'No'], n_samples, p=[0.45, 0.55]),\n",
        "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
        "    'support_tickets': np.random.poisson(2, n_samples),\n",
        "    'late_payments': np.random.poisson(0.5, n_samples)\n",
        "})\n",
        "\n",
        "data['total_charges'] = data['monthly_charges'] * data['tenure_months'] * np.random.uniform(0.8, 1.2, n_samples)\n",
        "\n",
        "churn_probability = (\n",
        "    0.1 +\n",
        "    0.3 * (data['contract_type'] == 'Month-to-month') +\n",
        "    0.2 * (data['satisfaction_score'] < 2.5) +\n",
        "    0.1 * (data['support_tickets'] > 5) +\n",
        "    0.1 * (data['late_payments'] > 2) +\n",
        "    0.1 * (data['tenure_months'] < 12) -\n",
        "    0.2 * (data['contract_type'] == 'Two year') -\n",
        "    0.1 * (data['online_security'] == 'Yes')\n",
        ")\n",
        "\n",
        "data['churn'] = (np.random.random(n_samples) < churn_probability).astype(int)\n",
        "\n",
        "missing_cols = ['satisfaction_score', 'support_tickets', 'total_charges']\n",
        "for col in missing_cols:\n",
        "    missing_idx = np.random.choice(data.index, size=int(0.1 * len(data)), replace=False)\n",
        "    data.loc[missing_idx, col] = np.nan\n",
        "\n",
        "outlier_idx = np.random.choice(data.index, size=50, replace=False)\n",
        "data.loc[outlier_idx, 'monthly_charges'] *= 5\n",
        "data.loc[outlier_idx[:25], 'total_charges'] *= 10\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Churn rate: {data['churn'].mean()*100:.1f}%\")\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 BUILDING PREPROCESSING PIPELINE\n",
            "========================================\n",
            "Training set: (4000, 16)\n",
            "Test set: (1000, 16)\n",
            "\n",
            "Numeric features (8): ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'num_services']...\n",
            "Categorical features (8): ['contract_type', 'payment_method', 'internet_service', 'tech_support', 'online_security']...\n",
            "\n",
            "📋 Pipeline Structure:\n",
            "1. Numeric Pipeline:\n",
            "   → Median Imputation\n",
            "   → Outlier Capping\n",
            "   → Robust Scaling\n",
            "\n",
            "2. Categorical Pipeline:\n",
            "   → Constant Imputation\n",
            "   → One-Hot Encoding\n",
            "\n",
            "3. Model:\n",
            "   → Random Forest Classifier\n",
            "\n",
            "⏳ Training pipeline...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:03:24,861 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:03:26,682 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:03:26,802 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model Performance:\n",
            "Accuracy: 0.735\n",
            "Precision: 0.615\n",
            "Recall: 0.387\n",
            "F1-Score: 0.475\n",
            "ROC-AUC: 0.786\n"
          ]
        }
      ],
      "source": [
        "print(\"🔧 BUILDING PREPROCESSING PIPELINE\\n\" + \"=\"*40)\n",
        "\n",
        "X = data.drop(['customer_id', 'churn'], axis=1)\n",
        "y = data['churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('outliers', OutlierRemover(factor=1.5)),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_pipeline, numeric_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ])\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "print(\"\\n📋 Pipeline Structure:\")\n",
        "print(\"1. Numeric Pipeline:\")\n",
        "print(\"   → Median Imputation\")\n",
        "print(\"   → Outlier Capping\")\n",
        "print(\"   → Robust Scaling\")\n",
        "print(\"\\n2. Categorical Pipeline:\")\n",
        "print(\"   → Constant Imputation\")\n",
        "print(\"   → One-Hot Encoding\")\n",
        "print(\"\\n3. Model:\")\n",
        "print(\"   → Random Forest Classifier\")\n",
        "\n",
        "print(\"\\n⏳ Training pipeline...\")\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n📊 Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.3f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.3f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 3: Cross-Validation and Optimization\n",
        "### 🎯 Optimizing Pipeline Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 PIPELINE OPTIMIZATION\n",
            "========================================\n",
            "🔍 Starting Grid Search...\n",
            "Total combinations: 108\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:42,090 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:44,086 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:44,195 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Grid Search Complete!\n",
            "Best Score: 0.771\n",
            "\n",
            "Best Parameters:\n",
            "  classifier__max_depth: 5\n",
            "  classifier__min_samples_split: 5\n",
            "  classifier__n_estimators: 200\n",
            "  preprocessor__num__imputer__strategy: median\n",
            "  preprocessor__num__scaler: StandardScaler()\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:44,404 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Best Model Performance on Test Set:\n",
            "Accuracy: 0.722\n",
            "ROC-AUC: 0.803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:45,940 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,061 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,149 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,301 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,470 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,647 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,735 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,814 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,957 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:48,116 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,256 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,345 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,431 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,569 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,729 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,020 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,108 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,195 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,326 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,485 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,594 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,665 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,730 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,841 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Cross-Validation Results (5-fold):\n",
            "       test_accuracy  test_precision  test_recall  test_roc_auc\n",
            "count          5.000           5.000        5.000         5.000\n",
            "mean           0.709           0.649        0.132         0.776\n",
            "std            0.005           0.034        0.021         0.023\n",
            "min            0.704           0.600        0.105         0.751\n",
            "25%            0.705           0.634        0.117         0.758\n",
            "50%            0.709           0.654        0.137         0.777\n",
            "75%            0.710           0.674        0.145         0.789\n",
            "max            0.716           0.684        0.157         0.807\n"
          ]
        }
      ],
      "source": [
        "print(\"🔍 PIPELINE OPTIMIZATION\\n\" + \"=\"*40)\n",
        "\n",
        "optimization_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__num__scaler': [StandardScaler(), RobustScaler()],\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [5, 10, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "print(\"🔍 Starting Grid Search...\")\n",
        "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    optimization_pipeline,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n✅ Grid Search Complete!\")\n",
        "print(f\"Best Score: {grid_search.best_score_:.3f}\")\n",
        "print(\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n📊 Best Model Performance on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.3f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_best):.3f}\")\n",
        "\n",
        "cv_scores = cross_validate(\n",
        "    best_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring=['accuracy', 'precision', 'recall', 'roc_auc'],\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "cv_results = pd.DataFrame(cv_scores)\n",
        "print(\"\\n📊 Cross-Validation Results (5-fold):\")\n",
        "print(cv_results[['test_accuracy', 'test_precision', 'test_recall', 'test_roc_auc']].describe().round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 4: Pipeline Persistence\n",
        "### 🎯 Saving and Loading Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 SAVING PIPELINE\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,469 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Pipeline saved: churn_prediction_pipeline.pkl\n",
            "✅ Metadata saved: pipeline_metadata.json\n",
            "\n",
            "📋 Metadata:\n",
            "{\n",
            "  \"model_name\": \"Churn Prediction Pipeline\",\n",
            "  \"version\": \"1.0.0\",\n",
            "  \"created_date\": \"2025-10-08T18:06:53.300196\",\n",
            "  \"features\": [\n",
            "    \"age\",\n",
            "    \"tenure_months\",\n",
            "    \"monthly_charges\",\n",
            "    \"total_charges\",\n",
            "    \"num_services\",\n",
            "    \"contract_type\",\n",
            "    \"payment_method\",\n",
            "    \"internet_service\",\n",
            "    \"tech_support\",\n",
            "    \"online_security\",\n",
            "    \"device_protection\",\n",
            "    \"streaming_tv\",\n",
            "    \"streaming_movies\",\n",
            "    \"satisfaction_score\",\n",
            "    \"support_tickets\",\n",
            "    \"late_payments\"\n",
            "  ],\n",
            "  \"numeric_features\": [\n",
            "    \"age\",\n",
            "    \"tenure_months\",\n",
            "    \"monthly_charges\",\n",
            "    \"total_charges\",\n",
            "    \"num_services\",\n",
            "    \"satisfaction_score\",\n",
            "    \"support_tickets\",\n",
            "    \"late_payments\"\n",
            "  ],\n",
            "  \"categorical_features\": [\n",
            "    \"contract_type\",\n",
            "    \"payment_method\",\n",
            "    \"internet_service\",\n",
            "    \"tech_support\",\n",
            "    \"online_security\",\n",
            "    \"device_protection\",\n",
            "    \"streaming_tv\",\n",
            "    \"streaming_movies\"\n",
            "  ],\n",
            "  \"performance_metrics\": {\n",
            "    \"accuracy\": 0.722,\n",
            "    \"roc_auc\": 0.8027021972884525,\n",
            "    \"precision\": 0.7222222222222222,\n",
            "    \"recall\": 0.16774193548387098\n",
            "  },\n",
            "  \"training_samples\": 4000,\n",
            "  \"test_samples\": 1000\n",
            "}\n",
            "\n",
            "📥 Loading pipeline...\n",
            "✅ Pipeline loaded successfully!\n",
            "\n",
            "🧪 Test predictions: [0 0 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"💾 SAVING PIPELINE\\n\" + \"=\"*40)\n",
        "\n",
        "pipeline_filename = 'churn_prediction_pipeline.pkl'\n",
        "metadata_filename = 'pipeline_metadata.json'\n",
        "\n",
        "joblib.dump(best_model, pipeline_filename)\n",
        "print(f\"✅ Pipeline saved: {pipeline_filename}\")\n",
        "\n",
        "metadata = {\n",
        "    'model_name': 'Churn Prediction Pipeline',\n",
        "    'version': '1.0.0',\n",
        "    'created_date': datetime.now().isoformat(),\n",
        "    'features': X_train.columns.tolist(),\n",
        "    'numeric_features': numeric_features,\n",
        "    'categorical_features': categorical_features,\n",
        "    'performance_metrics': {\n",
        "        'accuracy': float(accuracy_score(y_test, y_pred_best)),\n",
        "        'roc_auc': float(roc_auc_score(y_test, y_pred_proba_best)),\n",
        "        'precision': float(precision_score(y_test, y_pred_best)),\n",
        "        'recall': float(recall_score(y_test, y_pred_best))\n",
        "    },\n",
        "    'training_samples': len(X_train),\n",
        "    'test_samples': len(X_test)\n",
        "}\n",
        "\n",
        "with open(metadata_filename, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"✅ Metadata saved: {metadata_filename}\")\n",
        "print(\"\\n📋 Metadata:\")\n",
        "print(json.dumps(metadata, indent=2))\n",
        "\n",
        "print(\"\\n📥 Loading pipeline...\")\n",
        "loaded_pipeline = joblib.load(pipeline_filename)\n",
        "with open(metadata_filename, 'r') as f:\n",
        "    loaded_metadata = json.load(f)\n",
        "\n",
        "print(\"✅ Pipeline loaded successfully!\")\n",
        "\n",
        "test_pred = loaded_pipeline.predict(X_test.iloc[:5])\n",
        "print(f\"\\n🧪 Test predictions: {test_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 5: Production Deployment\n",
        "### 🎯 API-Ready Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,640 - INFO - Model loaded: Churn Prediction Pipeline v1.0.0\n",
            "2025-10-08 18:06:53,662 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,696 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,716 - INFO - Prediction completed for 1 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 PRODUCTION DEPLOYMENT\n",
            "========================================\n",
            "📊 Single Customer Prediction:\n",
            "  churn_prediction: No\n",
            "  churn_probability: 0.2519526684780314\n",
            "  risk_level: Low\n",
            "  timestamp: 2025-10-08T18:06:53.716235\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,737 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,776 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,799 - INFO - Prediction completed for 10 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Batch Prediction (10 customers):\n",
            "  Churn rate: 10.0%\n",
            "  Average probability: 0.333\n"
          ]
        }
      ],
      "source": [
        "print(\"🚀 PRODUCTION DEPLOYMENT\\n\" + \"=\"*40)\n",
        "\n",
        "class ChurnPredictor:\n",
        "    \"\"\"Production-ready churn prediction class\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, metadata_path):\n",
        "        self.model = joblib.load(model_path)\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "        self.features = self.metadata['features']\n",
        "        logger.info(f\"Model loaded: {self.metadata['model_name']} v{self.metadata['version']}\")\n",
        "    \n",
        "    def validate_input(self, data):\n",
        "        \"\"\"Validate input data\"\"\"\n",
        "        if not isinstance(data, pd.DataFrame):\n",
        "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
        "        \n",
        "        missing_features = set(self.features) - set(data.columns)\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing features: {missing_features}\")\n",
        "        \n",
        "        return data[self.features]\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        try:\n",
        "            data = self.validate_input(data)\n",
        "            predictions = self.model.predict(data)\n",
        "            probabilities = self.model.predict_proba(data)[:, 1]\n",
        "            \n",
        "            response = {\n",
        "                'predictions': predictions.tolist(),\n",
        "                'probabilities': probabilities.tolist(),\n",
        "                'model_version': self.metadata['version'],\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            logger.info(f\"Prediction completed for {len(data)} samples\")\n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Prediction error: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def predict_single(self, customer_data):\n",
        "        \"\"\"Predict for a single customer\"\"\"\n",
        "        df = pd.DataFrame([customer_data])\n",
        "        result = self.predict(df)\n",
        "        \n",
        "        return {\n",
        "            'churn_prediction': 'Yes' if result['predictions'][0] == 1 else 'No',\n",
        "            'churn_probability': result['probabilities'][0],\n",
        "            'risk_level': self._get_risk_level(result['probabilities'][0]),\n",
        "            'timestamp': result['timestamp']\n",
        "        }\n",
        "    \n",
        "    def _get_risk_level(self, probability):\n",
        "        \"\"\"Categorize risk level\"\"\"\n",
        "        if probability < 0.3:\n",
        "            return 'Low'\n",
        "        elif probability < 0.7:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'High'\n",
        "    \n",
        "    def get_model_info(self):\n",
        "        \"\"\"Get model information\"\"\"\n",
        "        return self.metadata\n",
        "\n",
        "\n",
        "predictor = ChurnPredictor(pipeline_filename, metadata_filename)\n",
        "\n",
        "test_customer = X_test.iloc[0].to_dict()\n",
        "result = predictor.predict_single(test_customer)\n",
        "\n",
        "print(\"📊 Single Customer Prediction:\")\n",
        "for key, value in result.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "batch_result = predictor.predict(X_test.iloc[:10])\n",
        "print(f\"\\n📊 Batch Prediction ({len(batch_result['predictions'])} customers):\")\n",
        "print(f\"  Churn rate: {np.mean(batch_result['predictions'])*100:.1f}%\")\n",
        "print(f\"  Average probability: {np.mean(batch_result['probabilities']):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 6: Monitoring and Validation\n",
        "### 🎯 Pipeline Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,918 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,979 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 PIPELINE MONITORING\n",
            "========================================\n",
            "🔍 Drift Detection Test:\n",
            "⚠️ Drift detected in features:\n",
            "  total_charges: DRIFT DETECTED\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:54,030 - INFO - Prediction completed for 1000 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Prediction Validation:\n",
            "  total_predictions: 1000\n",
            "  null_predictions: 0\n",
            "  out_of_range: 0\n",
            "  mean_prediction: 0.30788873472528455\n",
            "  std_prediction: 0.12338705851272763\n",
            "  status: PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"📊 PIPELINE MONITORING\\n\" + \"=\"*40)\n",
        "\n",
        "class PipelineMonitor:\n",
        "    \"\"\"Monitor pipeline performance and data drift\"\"\"\n",
        "    \n",
        "    def __init__(self, reference_data):\n",
        "        self.reference_data = reference_data\n",
        "        self.reference_stats = self._calculate_stats(reference_data)\n",
        "        self.monitoring_history = []\n",
        "    \n",
        "    def _calculate_stats(self, data):\n",
        "        \"\"\"Calculate statistics for monitoring\"\"\"\n",
        "        stats = {}\n",
        "        \n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            stats[col] = {\n",
        "                'mean': data[col].mean(),\n",
        "                'std': data[col].std(),\n",
        "                'min': data[col].min(),\n",
        "                'max': data[col].max(),\n",
        "                'missing_pct': data[col].isnull().mean()\n",
        "            }\n",
        "        \n",
        "        categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            stats[col] = {\n",
        "                'unique_values': data[col].nunique(),\n",
        "                'mode': data[col].mode()[0] if not data[col].mode().empty else None,\n",
        "                'missing_pct': data[col].isnull().mean()\n",
        "            }\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    def detect_drift(self, new_data, threshold=0.1):\n",
        "        \"\"\"Detect data drift\"\"\"\n",
        "        new_stats = self._calculate_stats(new_data)\n",
        "        drift_report = {}\n",
        "        \n",
        "        for feature in self.reference_stats:\n",
        "            if feature in new_stats:\n",
        "                ref = self.reference_stats[feature]\n",
        "                new = new_stats[feature]\n",
        "                \n",
        "                if 'mean' in ref:\n",
        "                    mean_change = abs(new['mean'] - ref['mean']) / (ref['mean'] + 1e-8)\n",
        "                    std_change = abs(new['std'] - ref['std']) / (ref['std'] + 1e-8)\n",
        "                    \n",
        "                    if mean_change > threshold or std_change > threshold:\n",
        "                        drift_report[feature] = {\n",
        "                            'mean_change': mean_change,\n",
        "                            'std_change': std_change,\n",
        "                            'status': 'DRIFT DETECTED'\n",
        "                        }\n",
        "        \n",
        "        return drift_report\n",
        "    \n",
        "    def validate_predictions(self, predictions, expected_range=(0, 1)):\n",
        "        \"\"\"Validate prediction outputs\"\"\"\n",
        "        validation_report = {\n",
        "            'total_predictions': len(predictions),\n",
        "            'null_predictions': np.isnan(predictions).sum(),\n",
        "            'out_of_range': ((predictions < expected_range[0]) | (predictions > expected_range[1])).sum(),\n",
        "            'mean_prediction': np.mean(predictions),\n",
        "            'std_prediction': np.std(predictions)\n",
        "        }\n",
        "        \n",
        "        if validation_report['null_predictions'] > 0:\n",
        "            validation_report['status'] = 'FAILED - Null predictions found'\n",
        "        elif validation_report['out_of_range'] > 0:\n",
        "            validation_report['status'] = 'WARNING - Out of range predictions'\n",
        "        else:\n",
        "            validation_report['status'] = 'PASSED'\n",
        "        \n",
        "        return validation_report\n",
        "\n",
        "\n",
        "monitor = PipelineMonitor(X_train)\n",
        "\n",
        "print(\"🔍 Drift Detection Test:\")\n",
        "drift_report = monitor.detect_drift(X_test)\n",
        "if drift_report:\n",
        "    print(\"⚠️ Drift detected in features:\")\n",
        "    for feature, details in drift_report.items():\n",
        "        print(f\"  {feature}: {details['status']}\")\n",
        "else:\n",
        "    print(\"✅ No significant drift detected\")\n",
        "\n",
        "test_predictions = predictor.predict(X_test)['probabilities']\n",
        "validation_report = monitor.validate_predictions(np.array(test_predictions))\n",
        "\n",
        "print(\"\\n📊 Prediction Validation:\")\n",
        "for key, value in validation_report.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📌 Section 7: API Simulation\n",
        "### 🎯 REST API Endpoint Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:54,140 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌐 API ENDPOINT SIMULATION\n",
            "========================================\n",
            "📡 Testing API Endpoint:\n",
            "\n",
            "Request:\n",
            "  Sending 3 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:54,246 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:54,293 - INFO - Prediction completed for 3 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "  Status: success\n",
            "  Code: 200\n",
            "  Predictions: [0, 0, 0]\n",
            "  Probabilities: [0.252, 0.33, 0.359]\n",
            "  Model Version: 1.0.0\n",
            "\n",
            "🔍 Testing Error Handling:\n",
            "  Error Status: error\n",
            "  Error Message: Invalid request. Missing data field.\n"
          ]
        }
      ],
      "source": [
        "print(\"🌐 API ENDPOINT SIMULATION\\n\" + \"=\"*40)\n",
        "\n",
        "def api_predict(request_data):\n",
        "    \"\"\"Simulate API endpoint for predictions\"\"\"\n",
        "    \n",
        "    try:\n",
        "        if not request_data or 'data' not in request_data:\n",
        "            return {\n",
        "                'status': 'error',\n",
        "                'message': 'Invalid request. Missing data field.',\n",
        "                'code': 400\n",
        "            }\n",
        "        \n",
        "        df = pd.DataFrame(request_data['data'])\n",
        "        results = predictor.predict(df)\n",
        "        \n",
        "        response = {\n",
        "            'status': 'success',\n",
        "            'code': 200,\n",
        "            'data': {\n",
        "                'predictions': results['predictions'],\n",
        "                'probabilities': results['probabilities'],\n",
        "                'model_version': loaded_metadata['version'],\n",
        "                'timestamp': results['timestamp']\n",
        "            },\n",
        "            'metadata': {\n",
        "                'samples_processed': len(results['predictions']),\n",
        "                'model_name': loaded_metadata['model_name']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'message': str(e),\n",
        "            'code': 500\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"📡 Testing API Endpoint:\\n\")\n",
        "\n",
        "test_request = {\n",
        "    'data': X_test.iloc[:3].to_dict('records')\n",
        "}\n",
        "\n",
        "print(\"Request:\")\n",
        "print(f\"  Sending {len(test_request['data'])} samples\")\n",
        "\n",
        "response = api_predict(test_request)\n",
        "\n",
        "print(\"\\nResponse:\")\n",
        "print(f\"  Status: {response['status']}\")\n",
        "print(f\"  Code: {response['code']}\")\n",
        "if response['status'] == 'success':\n",
        "    print(f\"  Predictions: {response['data']['predictions']}\")\n",
        "    print(f\"  Probabilities: {[round(p, 3) for p in response['data']['probabilities']]}\")\n",
        "    print(f\"  Model Version: {response['data']['model_version']}\")\n",
        "\n",
        "print(\"\\n🔍 Testing Error Handling:\")\n",
        "bad_request = {'invalid': 'data'}\n",
        "error_response = api_predict(bad_request)\n",
        "print(f\"  Error Status: {error_response['status']}\")\n",
        "print(f\"  Error Message: {error_response['message']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Summary & Next Steps\n",
        "\n",
        "### 🏆 What You've Built:\n",
        "\n",
        "✅ **Custom Transformers**\n",
        "- OutlierRemover\n",
        "- MissingIndicator  \n",
        "- FeatureEngineer\n",
        "\n",
        "✅ **Production Pipeline**\n",
        "- Modular architecture\n",
        "- Automated preprocessing\n",
        "- Feature engineering\n",
        "- Model training\n",
        "\n",
        "✅ **Advanced Features**\n",
        "- Cross-validation\n",
        "- Hyperparameter tuning\n",
        "- Performance monitoring\n",
        "- Data drift detection\n",
        "\n",
        "✅ **Deployment Ready**\n",
        "- Pipeline persistence\n",
        "- API endpoints\n",
        "- Error handling\n",
        "- Documentation\n",
        "\n",
        "### 🚀 Next Steps:\n",
        "1. Deploy to Production - Use Flask/FastAPI\n",
        "2. Add More Models - Ensemble methods\n",
        "3. Implement A/B Testing - Compare models\n",
        "4. Setup CI/CD - Automated deployment\n",
        "5. Add Real-time Monitoring - Dashboard\n",
        "\n",
        "### 💡 Key Takeaways:\n",
        "- Pipelines ensure reproducibility\n",
        "- Automation reduces errors\n",
        "- Monitoring prevents degradation\n",
        "- Documentation enables collaboration\n",
        "\n",
        "## 🎉 Congratulations!\n",
        "You've built a complete production-ready ML pipeline!\n",
        "\n",
        "This is how real data science projects are deployed.\n",
        "\n",
        "**Keep building, keep deploying, keep improving!** 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊\n",
            "\n",
            "    🏆 MASTER PIPELINE COMPLETE! 🏆\n",
            "\n",
            "    You've mastered:\n",
            "    ✅ Custom Transformers\n",
            "    ✅ Pipeline Architecture\n",
            "    ✅ Feature Engineering\n",
            "    ✅ Model Deployment\n",
            "    ✅ Production Best Practices\n",
            "\n",
            "    Ready for: Machine Learning Algorithms!\n",
            "\n",
            "🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊🎊\n"
          ]
        }
      ],
      "source": [
        "print(\"🎊\" * 20)\n",
        "print(\"\\n    🏆 MASTER PIPELINE COMPLETE! 🏆\")\n",
        "print(\"\\n    You've mastered:\")\n",
        "print(\"    ✅ Custom Transformers\")\n",
        "print(\"    ✅ Pipeline Architecture\")\n",
        "print(\"    ✅ Feature Engineering\")\n",
        "print(\"    ✅ Model Deployment\")\n",
        "print(\"    ✅ Production Best Practices\")\n",
        "print(\"\\n    Ready for: Machine Learning Algorithms!\")\n",
        "print(\"\\n\" + \"🎊\" * 20)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML_Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
