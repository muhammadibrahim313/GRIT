{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Master Pipeline: End-to-End Data Science Project Pipeline\n",
        "![](https://images.unsplash.com/photo-1639322537228-f710d846310a?w=1200&h=300&fit=crop)\n",
        "## üöÄ Complete Data Science Workflow\n",
        "This is where everything comes together! We'll build:\n",
        "\n",
        "- Complete preprocessing pipeline\n",
        "- Feature engineering automation\n",
        "- Model-ready transformations\n",
        "- Production-grade code\n",
        "- Real business case study\n",
        "\n",
        "## üìö What We'll Build:\n",
        "1. Data Pipeline Architecture - Modular design\n",
        "2. Custom Transformers - Sklearn compatible\n",
        "3. Feature Engineering Pipeline - Automated features\n",
        "4. Preprocessing Pipeline - End-to-end cleaning\n",
        "5. Validation Framework - Quality checks\n",
        "6. Model Preparation - Ready for ML\n",
        "7. Pipeline Persistence - Save and load\n",
        "8. Production Deployment - API ready\n",
        "9. Monitoring & Logging - Track everything\n",
        "10. Complete Project - Customer churn prediction\n",
        "\n",
        "## üèóÔ∏è Let's Build Production Pipelines!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Master Pipeline - Ready to Build!\n",
            "\n",
            "üí° Building production-ready data pipelines!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üéØ Master Pipeline - Ready to Build!\")\n",
        "print(\"\\nüí° Building production-ready data pipelines!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 1: Custom Transformers\n",
        "### üéØ Building Sklearn-Compatible Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß CUSTOM TRANSFORMERS\n",
            "========================================\n",
            "‚úÖ Custom transformers created:\n",
            "  ‚Ä¢ OutlierRemover\n",
            "  ‚Ä¢ MissingIndicator\n",
            "  ‚Ä¢ FeatureEngineer\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß CUSTOM TRANSFORMERS\\n\" + \"=\"*40)\n",
        "\n",
        "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Remove outliers using IQR method - works with both DataFrames and arrays\"\"\"\n",
        "    \n",
        "    def __init__(self, factor=1.5):\n",
        "        self.factor = factor\n",
        "        self.bounds_ = {}\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "            self.feature_names_ = list(range(X.shape[1]))\n",
        "        else:\n",
        "            X_df = X\n",
        "            self.feature_names_ = X.columns.tolist()\n",
        "        \n",
        "        for i, column in enumerate(X_df.columns):\n",
        "            if X_df.iloc[:, i].dtype in ['int64', 'float64', 'float32', 'int32']:\n",
        "                Q1 = X_df.iloc[:, i].quantile(0.25)\n",
        "                Q3 = X_df.iloc[:, i].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                self.bounds_[i] = (Q1 - self.factor * IQR, Q3 + self.factor * IQR)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X.copy()\n",
        "        \n",
        "        for col_idx, (lower, upper) in self.bounds_.items():\n",
        "            X_df.iloc[:, col_idx] = X_df.iloc[:, col_idx].clip(lower, upper)\n",
        "        \n",
        "        logger.info(f\"Outliers capped for {len(self.bounds_)} columns\")\n",
        "        \n",
        "        if isinstance(X, np.ndarray):\n",
        "            return X_df.values\n",
        "        else:\n",
        "            return X_df\n",
        "\n",
        "\n",
        "class MissingIndicator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Add binary indicators for missing values\"\"\"\n",
        "    \n",
        "    def __init__(self, threshold=0.1):\n",
        "        self.threshold = threshold\n",
        "        self.columns_ = []\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X\n",
        "        \n",
        "        missing_pct = X_df.isnull().sum() / len(X_df)\n",
        "        self.columns_ = list(missing_pct[missing_pct > self.threshold].index)\n",
        "        \n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            self.feature_names_ = X.columns.tolist()\n",
        "        else:\n",
        "            self.feature_names_ = list(range(X.shape[1]))\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X.copy()\n",
        "        \n",
        "        for col in self.columns_:\n",
        "            X_df[f'missing_indicator_{col}'] = X_df.iloc[:, col].isnull().astype(int)\n",
        "        \n",
        "        logger.info(f\"Added {len(self.columns_)} missing indicators\")\n",
        "        \n",
        "        if isinstance(X, np.ndarray):\n",
        "            return X_df.values\n",
        "        else:\n",
        "            return X_df\n",
        "\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Create new features from existing ones\"\"\"\n",
        "    \n",
        "    def __init__(self, create_interactions=True, create_ratios=True):\n",
        "        self.create_interactions = create_interactions\n",
        "        self.create_ratios = create_ratios\n",
        "        self.numeric_columns_ = []\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X\n",
        "            \n",
        "        self.numeric_columns_ = []\n",
        "        for i, col in enumerate(X_df.columns):\n",
        "            if X_df.iloc[:, i].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
        "                self.numeric_columns_.append(i)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            X_df = X.copy()\n",
        "        \n",
        "        for col_idx in self.numeric_columns_:\n",
        "            if col_idx < X_df.shape[1]:\n",
        "                col_name = X_df.columns[col_idx]\n",
        "                X_df[f'{col_name}_squared'] = X_df.iloc[:, col_idx] ** 2\n",
        "                X_df[f'{col_name}_log'] = np.log1p(np.abs(X_df.iloc[:, col_idx]))\n",
        "        \n",
        "        if self.create_interactions and len(self.numeric_columns_) > 1:\n",
        "            cols_for_interaction = self.numeric_columns_[:min(5, len(self.numeric_columns_))]\n",
        "            for i, col1_idx in enumerate(cols_for_interaction[:-1]):\n",
        "                for col2_idx in cols_for_interaction[i+1:i+2]:\n",
        "                    if col1_idx < X_df.shape[1] and col2_idx < X_df.shape[1]:\n",
        "                        col1_name = X_df.columns[col1_idx]\n",
        "                        col2_name = X_df.columns[col2_idx]\n",
        "                        X_df[f'{col1_name}_x_{col2_name}'] = X_df.iloc[:, col1_idx] * X_df.iloc[:, col2_idx]\n",
        "        \n",
        "        if self.create_ratios and len(self.numeric_columns_) > 1:\n",
        "            cols_for_ratio = self.numeric_columns_[:min(3, len(self.numeric_columns_))]\n",
        "            for i, col1_idx in enumerate(cols_for_ratio[:-1]):\n",
        "                for col2_idx in cols_for_ratio[i+1:i+2]:\n",
        "                    if col1_idx < X_df.shape[1] and col2_idx < X_df.shape[1]:\n",
        "                        col1_name = X_df.columns[col1_idx]\n",
        "                        col2_name = X_df.columns[col2_idx]\n",
        "                        X_df[f'{col1_name}_div_{col2_name}'] = X_df.iloc[:, col1_idx] / (X_df.iloc[:, col2_idx] + 1e-8)\n",
        "        \n",
        "        logger.info(f\"Created {X_df.shape[1] - X.shape[1]} new features\")\n",
        "        \n",
        "        if isinstance(X, np.ndarray):\n",
        "            return X_df.values\n",
        "        else:\n",
        "            return X_df\n",
        "\n",
        "\n",
        "print(\"‚úÖ Custom transformers created:\")\n",
        "print(\"  ‚Ä¢ OutlierRemover\")\n",
        "print(\"  ‚Ä¢ MissingIndicator\")\n",
        "print(\"  ‚Ä¢ FeatureEngineer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 2: Pipeline Architecture\n",
        "### üéØ Building Modular Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä CREATING SAMPLE DATASET\n",
            "========================================\n",
            "Dataset shape: (5000, 18)\n",
            "Churn rate: 31.0%\n",
            "Missing values: 1500\n",
            "\n",
            "First 5 rows:\n",
            "   customer_id        age  tenure_months  monthly_charges  total_charges  \\\n",
            "0            1  52.450712       4.420973        28.998892            NaN   \n",
            "1            2  42.926035       5.055371        42.243909     250.571324   \n",
            "2            3  54.715328      14.820140        22.840260     282.019941   \n",
            "3            4  67.845448       8.094345        49.860711     346.586247   \n",
            "4            5  41.487699       6.823322        93.380864     562.060393   \n",
            "\n",
            "   num_services   contract_type payment_method internet_service tech_support  \\\n",
            "0             4  Month-to-month     Electronic      Fiber optic           No   \n",
            "1             2        Two year  Bank transfer              DSL           No   \n",
            "2             3  Month-to-month     Electronic               No           No   \n",
            "3             2        One year    Credit card      Fiber optic           No   \n",
            "4             1  Month-to-month  Bank transfer              DSL           No   \n",
            "\n",
            "  online_security device_protection streaming_tv streaming_movies  \\\n",
            "0             Yes                No          Yes              Yes   \n",
            "1             Yes                No           No              Yes   \n",
            "2             Yes                No          Yes               No   \n",
            "3              No                No           No               No   \n",
            "4              No               Yes          Yes               No   \n",
            "\n",
            "   satisfaction_score  support_tickets  late_payments  churn  \n",
            "0                 NaN              2.0              0      0  \n",
            "1            4.079777              3.0              0      0  \n",
            "2            3.974223              2.0              0      0  \n",
            "3            3.819199              0.0              2      0  \n",
            "4            1.042771              4.0              1      1  \n"
          ]
        }
      ],
      "source": [
        "print(\"üìä CREATING SAMPLE DATASET\\n\" + \"=\"*40)\n",
        "\n",
        "# Generate realistic customer churn dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 5000\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'customer_id': range(1, n_samples + 1),\n",
        "    'age': np.random.normal(45, 15, n_samples).clip(18, 80),\n",
        "    'tenure_months': np.random.exponential(24, n_samples),\n",
        "    'monthly_charges': np.random.gamma(2, 40, n_samples),\n",
        "    'total_charges': np.random.lognormal(7, 1.5, n_samples),\n",
        "    'num_services': np.random.poisson(3, n_samples),\n",
        "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.5, 0.3, 0.2]),\n",
        "    'payment_method': np.random.choice(['Electronic', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
        "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples, p=[0.3, 0.5, 0.2]),\n",
        "    'tech_support': np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7]),\n",
        "    'online_security': np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6]),\n",
        "    'device_protection': np.random.choice(['Yes', 'No'], n_samples, p=[0.35, 0.65]),\n",
        "    'streaming_tv': np.random.choice(['Yes', 'No'], n_samples, p=[0.45, 0.55]),\n",
        "    'streaming_movies': np.random.choice(['Yes', 'No'], n_samples, p=[0.45, 0.55]),\n",
        "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
        "    'support_tickets': np.random.poisson(2, n_samples),\n",
        "    'late_payments': np.random.poisson(0.5, n_samples)\n",
        "})\n",
        "\n",
        "data['total_charges'] = data['monthly_charges'] * data['tenure_months'] * np.random.uniform(0.8, 1.2, n_samples)\n",
        "\n",
        "churn_probability = (\n",
        "    0.1 +\n",
        "    0.3 * (data['contract_type'] == 'Month-to-month') +\n",
        "    0.2 * (data['satisfaction_score'] < 2.5) +\n",
        "    0.1 * (data['support_tickets'] > 5) +\n",
        "    0.1 * (data['late_payments'] > 2) +\n",
        "    0.1 * (data['tenure_months'] < 12) -\n",
        "    0.2 * (data['contract_type'] == 'Two year') -\n",
        "    0.1 * (data['online_security'] == 'Yes')\n",
        ")\n",
        "\n",
        "data['churn'] = (np.random.random(n_samples) < churn_probability).astype(int)\n",
        "\n",
        "missing_cols = ['satisfaction_score', 'support_tickets', 'total_charges']\n",
        "for col in missing_cols:\n",
        "    missing_idx = np.random.choice(data.index, size=int(0.1 * len(data)), replace=False)\n",
        "    data.loc[missing_idx, col] = np.nan\n",
        "\n",
        "outlier_idx = np.random.choice(data.index, size=50, replace=False)\n",
        "data.loc[outlier_idx, 'monthly_charges'] *= 5\n",
        "data.loc[outlier_idx[:25], 'total_charges'] *= 10\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Churn rate: {data['churn'].mean()*100:.1f}%\")\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß BUILDING PREPROCESSING PIPELINE\n",
            "========================================\n",
            "Training set: (4000, 16)\n",
            "Test set: (1000, 16)\n",
            "\n",
            "Numeric features (8): ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'num_services']...\n",
            "Categorical features (8): ['contract_type', 'payment_method', 'internet_service', 'tech_support', 'online_security']...\n",
            "\n",
            "üìã Pipeline Structure:\n",
            "1. Numeric Pipeline:\n",
            "   ‚Üí Median Imputation\n",
            "   ‚Üí Outlier Capping\n",
            "   ‚Üí Robust Scaling\n",
            "\n",
            "2. Categorical Pipeline:\n",
            "   ‚Üí Constant Imputation\n",
            "   ‚Üí One-Hot Encoding\n",
            "\n",
            "3. Model:\n",
            "   ‚Üí Random Forest Classifier\n",
            "\n",
            "‚è≥ Training pipeline...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:03:24,861 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:03:26,682 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:03:26,802 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Model Performance:\n",
            "Accuracy: 0.735\n",
            "Precision: 0.615\n",
            "Recall: 0.387\n",
            "F1-Score: 0.475\n",
            "ROC-AUC: 0.786\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß BUILDING PREPROCESSING PIPELINE\\n\" + \"=\"*40)\n",
        "\n",
        "X = data.drop(['customer_id', 'churn'], axis=1)\n",
        "y = data['churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('outliers', OutlierRemover(factor=1.5)),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_pipeline, numeric_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ])\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "print(\"\\nüìã Pipeline Structure:\")\n",
        "print(\"1. Numeric Pipeline:\")\n",
        "print(\"   ‚Üí Median Imputation\")\n",
        "print(\"   ‚Üí Outlier Capping\")\n",
        "print(\"   ‚Üí Robust Scaling\")\n",
        "print(\"\\n2. Categorical Pipeline:\")\n",
        "print(\"   ‚Üí Constant Imputation\")\n",
        "print(\"   ‚Üí One-Hot Encoding\")\n",
        "print(\"\\n3. Model:\")\n",
        "print(\"   ‚Üí Random Forest Classifier\")\n",
        "\n",
        "print(\"\\n‚è≥ Training pipeline...\")\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nüìä Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.3f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.3f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 3: Cross-Validation and Optimization\n",
        "### üéØ Optimizing Pipeline Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç PIPELINE OPTIMIZATION\n",
            "========================================\n",
            "üîç Starting Grid Search...\n",
            "Total combinations: 108\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:42,090 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:44,086 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:44,195 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Grid Search Complete!\n",
            "Best Score: 0.771\n",
            "\n",
            "Best Parameters:\n",
            "  classifier__max_depth: 5\n",
            "  classifier__min_samples_split: 5\n",
            "  classifier__n_estimators: 200\n",
            "  preprocessor__num__imputer__strategy: median\n",
            "  preprocessor__num__scaler: StandardScaler()\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:44,404 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Best Model Performance on Test Set:\n",
            "Accuracy: 0.722\n",
            "ROC-AUC: 0.803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:45,940 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,061 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,149 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,301 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:46,470 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,647 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,735 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,814 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:47,957 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:48,116 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,256 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,345 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,431 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,569 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:49,729 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,020 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,108 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,195 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,326 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:51,485 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,594 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,665 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,730 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:52,841 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Cross-Validation Results (5-fold):\n",
            "       test_accuracy  test_precision  test_recall  test_roc_auc\n",
            "count          5.000           5.000        5.000         5.000\n",
            "mean           0.709           0.649        0.132         0.776\n",
            "std            0.005           0.034        0.021         0.023\n",
            "min            0.704           0.600        0.105         0.751\n",
            "25%            0.705           0.634        0.117         0.758\n",
            "50%            0.709           0.654        0.137         0.777\n",
            "75%            0.710           0.674        0.145         0.789\n",
            "max            0.716           0.684        0.157         0.807\n"
          ]
        }
      ],
      "source": [
        "print(\"üîç PIPELINE OPTIMIZATION\\n\" + \"=\"*40)\n",
        "\n",
        "optimization_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__num__scaler': [StandardScaler(), RobustScaler()],\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [5, 10, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "print(\"üîç Starting Grid Search...\")\n",
        "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    optimization_pipeline,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n‚úÖ Grid Search Complete!\")\n",
        "print(f\"Best Score: {grid_search.best_score_:.3f}\")\n",
        "print(\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nüìä Best Model Performance on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.3f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_best):.3f}\")\n",
        "\n",
        "cv_scores = cross_validate(\n",
        "    best_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring=['accuracy', 'precision', 'recall', 'roc_auc'],\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "cv_results = pd.DataFrame(cv_scores)\n",
        "print(\"\\nüìä Cross-Validation Results (5-fold):\")\n",
        "print(cv_results[['test_accuracy', 'test_precision', 'test_recall', 'test_roc_auc']].describe().round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 4: Pipeline Persistence\n",
        "### üéØ Saving and Loading Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ SAVING PIPELINE\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,469 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Pipeline saved: churn_prediction_pipeline.pkl\n",
            "‚úÖ Metadata saved: pipeline_metadata.json\n",
            "\n",
            "üìã Metadata:\n",
            "{\n",
            "  \"model_name\": \"Churn Prediction Pipeline\",\n",
            "  \"version\": \"1.0.0\",\n",
            "  \"created_date\": \"2025-10-08T18:06:53.300196\",\n",
            "  \"features\": [\n",
            "    \"age\",\n",
            "    \"tenure_months\",\n",
            "    \"monthly_charges\",\n",
            "    \"total_charges\",\n",
            "    \"num_services\",\n",
            "    \"contract_type\",\n",
            "    \"payment_method\",\n",
            "    \"internet_service\",\n",
            "    \"tech_support\",\n",
            "    \"online_security\",\n",
            "    \"device_protection\",\n",
            "    \"streaming_tv\",\n",
            "    \"streaming_movies\",\n",
            "    \"satisfaction_score\",\n",
            "    \"support_tickets\",\n",
            "    \"late_payments\"\n",
            "  ],\n",
            "  \"numeric_features\": [\n",
            "    \"age\",\n",
            "    \"tenure_months\",\n",
            "    \"monthly_charges\",\n",
            "    \"total_charges\",\n",
            "    \"num_services\",\n",
            "    \"satisfaction_score\",\n",
            "    \"support_tickets\",\n",
            "    \"late_payments\"\n",
            "  ],\n",
            "  \"categorical_features\": [\n",
            "    \"contract_type\",\n",
            "    \"payment_method\",\n",
            "    \"internet_service\",\n",
            "    \"tech_support\",\n",
            "    \"online_security\",\n",
            "    \"device_protection\",\n",
            "    \"streaming_tv\",\n",
            "    \"streaming_movies\"\n",
            "  ],\n",
            "  \"performance_metrics\": {\n",
            "    \"accuracy\": 0.722,\n",
            "    \"roc_auc\": 0.8027021972884525,\n",
            "    \"precision\": 0.7222222222222222,\n",
            "    \"recall\": 0.16774193548387098\n",
            "  },\n",
            "  \"training_samples\": 4000,\n",
            "  \"test_samples\": 1000\n",
            "}\n",
            "\n",
            "üì• Loading pipeline...\n",
            "‚úÖ Pipeline loaded successfully!\n",
            "\n",
            "üß™ Test predictions: [0 0 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"üíæ SAVING PIPELINE\\n\" + \"=\"*40)\n",
        "\n",
        "pipeline_filename = 'churn_prediction_pipeline.pkl'\n",
        "metadata_filename = 'pipeline_metadata.json'\n",
        "\n",
        "joblib.dump(best_model, pipeline_filename)\n",
        "print(f\"‚úÖ Pipeline saved: {pipeline_filename}\")\n",
        "\n",
        "metadata = {\n",
        "    'model_name': 'Churn Prediction Pipeline',\n",
        "    'version': '1.0.0',\n",
        "    'created_date': datetime.now().isoformat(),\n",
        "    'features': X_train.columns.tolist(),\n",
        "    'numeric_features': numeric_features,\n",
        "    'categorical_features': categorical_features,\n",
        "    'performance_metrics': {\n",
        "        'accuracy': float(accuracy_score(y_test, y_pred_best)),\n",
        "        'roc_auc': float(roc_auc_score(y_test, y_pred_proba_best)),\n",
        "        'precision': float(precision_score(y_test, y_pred_best)),\n",
        "        'recall': float(recall_score(y_test, y_pred_best))\n",
        "    },\n",
        "    'training_samples': len(X_train),\n",
        "    'test_samples': len(X_test)\n",
        "}\n",
        "\n",
        "with open(metadata_filename, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Metadata saved: {metadata_filename}\")\n",
        "print(\"\\nüìã Metadata:\")\n",
        "print(json.dumps(metadata, indent=2))\n",
        "\n",
        "print(\"\\nüì• Loading pipeline...\")\n",
        "loaded_pipeline = joblib.load(pipeline_filename)\n",
        "with open(metadata_filename, 'r') as f:\n",
        "    loaded_metadata = json.load(f)\n",
        "\n",
        "print(\"‚úÖ Pipeline loaded successfully!\")\n",
        "\n",
        "test_pred = loaded_pipeline.predict(X_test.iloc[:5])\n",
        "print(f\"\\nüß™ Test predictions: {test_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 5: Production Deployment\n",
        "### üéØ API-Ready Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,640 - INFO - Model loaded: Churn Prediction Pipeline v1.0.0\n",
            "2025-10-08 18:06:53,662 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,696 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,716 - INFO - Prediction completed for 1 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ PRODUCTION DEPLOYMENT\n",
            "========================================\n",
            "üìä Single Customer Prediction:\n",
            "  churn_prediction: No\n",
            "  churn_probability: 0.2519526684780314\n",
            "  risk_level: Low\n",
            "  timestamp: 2025-10-08T18:06:53.716235\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,737 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,776 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,799 - INFO - Prediction completed for 10 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Batch Prediction (10 customers):\n",
            "  Churn rate: 10.0%\n",
            "  Average probability: 0.333\n"
          ]
        }
      ],
      "source": [
        "print(\"üöÄ PRODUCTION DEPLOYMENT\\n\" + \"=\"*40)\n",
        "\n",
        "class ChurnPredictor:\n",
        "    \"\"\"Production-ready churn prediction class\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, metadata_path):\n",
        "        self.model = joblib.load(model_path)\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "        self.features = self.metadata['features']\n",
        "        logger.info(f\"Model loaded: {self.metadata['model_name']} v{self.metadata['version']}\")\n",
        "    \n",
        "    def validate_input(self, data):\n",
        "        \"\"\"Validate input data\"\"\"\n",
        "        if not isinstance(data, pd.DataFrame):\n",
        "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
        "        \n",
        "        missing_features = set(self.features) - set(data.columns)\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing features: {missing_features}\")\n",
        "        \n",
        "        return data[self.features]\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        try:\n",
        "            data = self.validate_input(data)\n",
        "            predictions = self.model.predict(data)\n",
        "            probabilities = self.model.predict_proba(data)[:, 1]\n",
        "            \n",
        "            response = {\n",
        "                'predictions': predictions.tolist(),\n",
        "                'probabilities': probabilities.tolist(),\n",
        "                'model_version': self.metadata['version'],\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            logger.info(f\"Prediction completed for {len(data)} samples\")\n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Prediction error: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def predict_single(self, customer_data):\n",
        "        \"\"\"Predict for a single customer\"\"\"\n",
        "        df = pd.DataFrame([customer_data])\n",
        "        result = self.predict(df)\n",
        "        \n",
        "        return {\n",
        "            'churn_prediction': 'Yes' if result['predictions'][0] == 1 else 'No',\n",
        "            'churn_probability': result['probabilities'][0],\n",
        "            'risk_level': self._get_risk_level(result['probabilities'][0]),\n",
        "            'timestamp': result['timestamp']\n",
        "        }\n",
        "    \n",
        "    def _get_risk_level(self, probability):\n",
        "        \"\"\"Categorize risk level\"\"\"\n",
        "        if probability < 0.3:\n",
        "            return 'Low'\n",
        "        elif probability < 0.7:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'High'\n",
        "    \n",
        "    def get_model_info(self):\n",
        "        \"\"\"Get model information\"\"\"\n",
        "        return self.metadata\n",
        "\n",
        "\n",
        "predictor = ChurnPredictor(pipeline_filename, metadata_filename)\n",
        "\n",
        "test_customer = X_test.iloc[0].to_dict()\n",
        "result = predictor.predict_single(test_customer)\n",
        "\n",
        "print(\"üìä Single Customer Prediction:\")\n",
        "for key, value in result.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "batch_result = predictor.predict(X_test.iloc[:10])\n",
        "print(f\"\\nüìä Batch Prediction ({len(batch_result['predictions'])} customers):\")\n",
        "print(f\"  Churn rate: {np.mean(batch_result['predictions'])*100:.1f}%\")\n",
        "print(f\"  Average probability: {np.mean(batch_result['probabilities']):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 6: Monitoring and Validation\n",
        "### üéØ Pipeline Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:53,918 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:53,979 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä PIPELINE MONITORING\n",
            "========================================\n",
            "üîç Drift Detection Test:\n",
            "‚ö†Ô∏è Drift detected in features:\n",
            "  total_charges: DRIFT DETECTED\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:54,030 - INFO - Prediction completed for 1000 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Prediction Validation:\n",
            "  total_predictions: 1000\n",
            "  null_predictions: 0\n",
            "  out_of_range: 0\n",
            "  mean_prediction: 0.30788873472528455\n",
            "  std_prediction: 0.12338705851272763\n",
            "  status: PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"üìä PIPELINE MONITORING\\n\" + \"=\"*40)\n",
        "\n",
        "class PipelineMonitor:\n",
        "    \"\"\"Monitor pipeline performance and data drift\"\"\"\n",
        "    \n",
        "    def __init__(self, reference_data):\n",
        "        self.reference_data = reference_data\n",
        "        self.reference_stats = self._calculate_stats(reference_data)\n",
        "        self.monitoring_history = []\n",
        "    \n",
        "    def _calculate_stats(self, data):\n",
        "        \"\"\"Calculate statistics for monitoring\"\"\"\n",
        "        stats = {}\n",
        "        \n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            stats[col] = {\n",
        "                'mean': data[col].mean(),\n",
        "                'std': data[col].std(),\n",
        "                'min': data[col].min(),\n",
        "                'max': data[col].max(),\n",
        "                'missing_pct': data[col].isnull().mean()\n",
        "            }\n",
        "        \n",
        "        categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            stats[col] = {\n",
        "                'unique_values': data[col].nunique(),\n",
        "                'mode': data[col].mode()[0] if not data[col].mode().empty else None,\n",
        "                'missing_pct': data[col].isnull().mean()\n",
        "            }\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    def detect_drift(self, new_data, threshold=0.1):\n",
        "        \"\"\"Detect data drift\"\"\"\n",
        "        new_stats = self._calculate_stats(new_data)\n",
        "        drift_report = {}\n",
        "        \n",
        "        for feature in self.reference_stats:\n",
        "            if feature in new_stats:\n",
        "                ref = self.reference_stats[feature]\n",
        "                new = new_stats[feature]\n",
        "                \n",
        "                if 'mean' in ref:\n",
        "                    mean_change = abs(new['mean'] - ref['mean']) / (ref['mean'] + 1e-8)\n",
        "                    std_change = abs(new['std'] - ref['std']) / (ref['std'] + 1e-8)\n",
        "                    \n",
        "                    if mean_change > threshold or std_change > threshold:\n",
        "                        drift_report[feature] = {\n",
        "                            'mean_change': mean_change,\n",
        "                            'std_change': std_change,\n",
        "                            'status': 'DRIFT DETECTED'\n",
        "                        }\n",
        "        \n",
        "        return drift_report\n",
        "    \n",
        "    def validate_predictions(self, predictions, expected_range=(0, 1)):\n",
        "        \"\"\"Validate prediction outputs\"\"\"\n",
        "        validation_report = {\n",
        "            'total_predictions': len(predictions),\n",
        "            'null_predictions': np.isnan(predictions).sum(),\n",
        "            'out_of_range': ((predictions < expected_range[0]) | (predictions > expected_range[1])).sum(),\n",
        "            'mean_prediction': np.mean(predictions),\n",
        "            'std_prediction': np.std(predictions)\n",
        "        }\n",
        "        \n",
        "        if validation_report['null_predictions'] > 0:\n",
        "            validation_report['status'] = 'FAILED - Null predictions found'\n",
        "        elif validation_report['out_of_range'] > 0:\n",
        "            validation_report['status'] = 'WARNING - Out of range predictions'\n",
        "        else:\n",
        "            validation_report['status'] = 'PASSED'\n",
        "        \n",
        "        return validation_report\n",
        "\n",
        "\n",
        "monitor = PipelineMonitor(X_train)\n",
        "\n",
        "print(\"üîç Drift Detection Test:\")\n",
        "drift_report = monitor.detect_drift(X_test)\n",
        "if drift_report:\n",
        "    print(\"‚ö†Ô∏è Drift detected in features:\")\n",
        "    for feature, details in drift_report.items():\n",
        "        print(f\"  {feature}: {details['status']}\")\n",
        "else:\n",
        "    print(\"‚úÖ No significant drift detected\")\n",
        "\n",
        "test_predictions = predictor.predict(X_test)['probabilities']\n",
        "validation_report = monitor.validate_predictions(np.array(test_predictions))\n",
        "\n",
        "print(\"\\nüìä Prediction Validation:\")\n",
        "for key, value in validation_report.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Section 7: API Simulation\n",
        "### üéØ REST API Endpoint Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:54,140 - INFO - Outliers capped for 8 columns\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê API ENDPOINT SIMULATION\n",
            "========================================\n",
            "üì° Testing API Endpoint:\n",
            "\n",
            "Request:\n",
            "  Sending 3 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:06:54,246 - INFO - Outliers capped for 8 columns\n",
            "2025-10-08 18:06:54,293 - INFO - Prediction completed for 3 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "  Status: success\n",
            "  Code: 200\n",
            "  Predictions: [0, 0, 0]\n",
            "  Probabilities: [0.252, 0.33, 0.359]\n",
            "  Model Version: 1.0.0\n",
            "\n",
            "üîç Testing Error Handling:\n",
            "  Error Status: error\n",
            "  Error Message: Invalid request. Missing data field.\n"
          ]
        }
      ],
      "source": [
        "print(\"üåê API ENDPOINT SIMULATION\\n\" + \"=\"*40)\n",
        "\n",
        "def api_predict(request_data):\n",
        "    \"\"\"Simulate API endpoint for predictions\"\"\"\n",
        "    \n",
        "    try:\n",
        "        if not request_data or 'data' not in request_data:\n",
        "            return {\n",
        "                'status': 'error',\n",
        "                'message': 'Invalid request. Missing data field.',\n",
        "                'code': 400\n",
        "            }\n",
        "        \n",
        "        df = pd.DataFrame(request_data['data'])\n",
        "        results = predictor.predict(df)\n",
        "        \n",
        "        response = {\n",
        "            'status': 'success',\n",
        "            'code': 200,\n",
        "            'data': {\n",
        "                'predictions': results['predictions'],\n",
        "                'probabilities': results['probabilities'],\n",
        "                'model_version': loaded_metadata['version'],\n",
        "                'timestamp': results['timestamp']\n",
        "            },\n",
        "            'metadata': {\n",
        "                'samples_processed': len(results['predictions']),\n",
        "                'model_name': loaded_metadata['model_name']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'message': str(e),\n",
        "            'code': 500\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"üì° Testing API Endpoint:\\n\")\n",
        "\n",
        "test_request = {\n",
        "    'data': X_test.iloc[:3].to_dict('records')\n",
        "}\n",
        "\n",
        "print(\"Request:\")\n",
        "print(f\"  Sending {len(test_request['data'])} samples\")\n",
        "\n",
        "response = api_predict(test_request)\n",
        "\n",
        "print(\"\\nResponse:\")\n",
        "print(f\"  Status: {response['status']}\")\n",
        "print(f\"  Code: {response['code']}\")\n",
        "if response['status'] == 'success':\n",
        "    print(f\"  Predictions: {response['data']['predictions']}\")\n",
        "    print(f\"  Probabilities: {[round(p, 3) for p in response['data']['probabilities']]}\")\n",
        "    print(f\"  Model Version: {response['data']['model_version']}\")\n",
        "\n",
        "print(\"\\nüîç Testing Error Handling:\")\n",
        "bad_request = {'invalid': 'data'}\n",
        "error_response = api_predict(bad_request)\n",
        "print(f\"  Error Status: {error_response['status']}\")\n",
        "print(f\"  Error Message: {error_response['message']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary & Next Steps\n",
        "\n",
        "### üèÜ What You've Built:\n",
        "\n",
        "‚úÖ **Custom Transformers**\n",
        "- OutlierRemover\n",
        "- MissingIndicator  \n",
        "- FeatureEngineer\n",
        "\n",
        "‚úÖ **Production Pipeline**\n",
        "- Modular architecture\n",
        "- Automated preprocessing\n",
        "- Feature engineering\n",
        "- Model training\n",
        "\n",
        "‚úÖ **Advanced Features**\n",
        "- Cross-validation\n",
        "- Hyperparameter tuning\n",
        "- Performance monitoring\n",
        "- Data drift detection\n",
        "\n",
        "‚úÖ **Deployment Ready**\n",
        "- Pipeline persistence\n",
        "- API endpoints\n",
        "- Error handling\n",
        "- Documentation\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "1. Deploy to Production - Use Flask/FastAPI\n",
        "2. Add More Models - Ensemble methods\n",
        "3. Implement A/B Testing - Compare models\n",
        "4. Setup CI/CD - Automated deployment\n",
        "5. Add Real-time Monitoring - Dashboard\n",
        "\n",
        "### üí° Key Takeaways:\n",
        "- Pipelines ensure reproducibility\n",
        "- Automation reduces errors\n",
        "- Monitoring prevents degradation\n",
        "- Documentation enables collaboration\n",
        "\n",
        "## üéâ Congratulations!\n",
        "You've built a complete production-ready ML pipeline!\n",
        "\n",
        "This is how real data science projects are deployed.\n",
        "\n",
        "**Keep building, keep deploying, keep improving!** üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéä\n",
            "\n",
            "    üèÜ MASTER PIPELINE COMPLETE! üèÜ\n",
            "\n",
            "    You've mastered:\n",
            "    ‚úÖ Custom Transformers\n",
            "    ‚úÖ Pipeline Architecture\n",
            "    ‚úÖ Feature Engineering\n",
            "    ‚úÖ Model Deployment\n",
            "    ‚úÖ Production Best Practices\n",
            "\n",
            "    Ready for: Machine Learning Algorithms!\n",
            "\n",
            "üéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéäüéä\n"
          ]
        }
      ],
      "source": [
        "print(\"üéä\" * 20)\n",
        "print(\"\\n    üèÜ MASTER PIPELINE COMPLETE! üèÜ\")\n",
        "print(\"\\n    You've mastered:\")\n",
        "print(\"    ‚úÖ Custom Transformers\")\n",
        "print(\"    ‚úÖ Pipeline Architecture\")\n",
        "print(\"    ‚úÖ Feature Engineering\")\n",
        "print(\"    ‚úÖ Model Deployment\")\n",
        "print(\"    ‚úÖ Production Best Practices\")\n",
        "print(\"\\n    Ready for: Machine Learning Algorithms!\")\n",
        "print(\"\\n\" + \"üéä\" * 20)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML_Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
