{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Master Pipeline: End-to-End Data Science Project\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*KzmIUYPmxgEHhXX7SlbP4w.jpeg' width='600' alt='Pipeline'>\n",
    "\n",
    "## ðŸš€ Complete Data Science Workflow\n",
    "\n",
    "**This is where everything comes together!** We'll build:\n",
    "- **Complete preprocessing pipeline**\n",
    "- **Feature engineering automation**\n",
    "- **Model-ready transformations**\n",
    "- **Production-grade code**\n",
    "- **Real business case study**\n",
    "\n",
    "### ðŸ“š What We'll Build:\n",
    "1. **Data Pipeline Architecture** - Modular design\n",
    "2. **Custom Transformers** - Sklearn compatible\n",
    "3. **Feature Engineering Pipeline** - Automated features\n",
    "4. **Preprocessing Pipeline** - End-to-end cleaning\n",
    "5. **Validation Framework** - Quality checks\n",
    "6. **Model Preparation** - Ready for ML\n",
    "7. **Pipeline Persistence** - Save and load\n",
    "8. **Production Deployment** - API ready\n",
    "9. **Monitoring & Logging** - Track everything\n",
    "10. **Complete Project** - Customer churn prediction\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Let's Build Production Pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Master  Pipeline - Ready to Build!\n",
      "\n",
      "ðŸ’¡ Building  production-ready data pipelines!\n"
     ]
    }
   ],
   "source": [
    "# Import all required  libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ðŸŽ¯ Master  Pipeline - Ready to Build!\")\n",
    "print(\"\\nðŸ’¡ Building  production-ready data pipelines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 1: Custom Transformers\n",
    "\n",
    "### ðŸŽ¯ Building Sklearn-Compatible Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ CUSTOM  TRANSFORMERS\n",
      "========================================\n",
      "âœ…  Custom transformers created:\n",
      "  â€¢ OutlierRemover\n",
      "  â€¢ MissingIndicator\n",
      "  â€¢ FeatureEngineer\n",
      "  â€¢ CategoryEncoder\n"
     ]
    }
   ],
   "source": [
    "# 1.1  Custom Transformer Classes\n",
    "print(\"ðŸ”§ CUSTOM  TRANSFORMERS\\n\" + \"=\"*40)\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Remove outliers using IQR method\"\"\"\n",
    "    \n",
    "    def __init__(self, factor=1.5):\n",
    "        self.factor = factor\n",
    "        self.bounds_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for column in X.columns:\n",
    "            if X[column].dtype in ['int64', 'float64']:\n",
    "                Q1 = X[column].quantile(0.25)\n",
    "                Q3 = X[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                self.bounds_[column] = (\n",
    "                    Q1 - self.factor * IQR,\n",
    "                    Q3 + self.factor * IQR\n",
    "                )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for column, (lower, upper) in self.bounds_.items():\n",
    "            X_copy[column] = X_copy[column].clip(lower, upper)\n",
    "        logger.info(f\"Outliers capped for {len(self.bounds_)} columns\")\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class MissingIndicator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Add binary indicators for missing values\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.1):\n",
    "        self.threshold = threshold\n",
    "        self.columns_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        missing_pct = X.isnull().sum() / len(X)\n",
    "        self.columns_ = missing_pct[missing_pct > self.threshold].index.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns_:\n",
    "            X_copy[f'{col}_was_missing'] = X[col].isnull().astype(int)\n",
    "        logger.info(f\"Added {len(self.columns_)} missing indicators\")\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create new features from existing ones\"\"\"\n",
    "    \n",
    "    def __init__(self, create_interactions=True, create_ratios=True):\n",
    "        self.create_interactions = create_interactions\n",
    "        self.create_ratios = create_ratios\n",
    "        self.numeric_columns_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Create polynomial features\n",
    "        for col in self.numeric_columns_:\n",
    "            if col in X.columns:\n",
    "                X_copy[f'{col}_squared'] = X[col] ** 2\n",
    "                X_copy[f'{col}_log'] = np.log1p(np.abs(X[col]))\n",
    "        \n",
    "        # Create interaction features\n",
    "        if self.create_interactions and len(self.numeric_columns_) > 1:\n",
    "            for i, col1 in enumerate(self.numeric_columns_[:-1]):\n",
    "                for col2 in self.numeric_columns_[i+1:]:\n",
    "                    if col1 in X.columns and col2 in X.columns:\n",
    "                        X_copy[f'{col1}_x_{col2}'] = X[col1] * X[col2]\n",
    "        \n",
    "        # Create ratio features\n",
    "        if self.create_ratios and len(self.numeric_columns_) > 1:\n",
    "            for i, col1 in enumerate(self.numeric_columns_[:-1]):\n",
    "                for col2 in self.numeric_columns_[i+1:]:\n",
    "                    if col1 in X.columns and col2 in X.columns:\n",
    "                        # Avoid division by zero\n",
    "                        X_copy[f'{col1}_div_{col2}'] = X[col1] / (X[col2] + 1e-8)\n",
    "        \n",
    "        logger.info(f\"Created {len(X_copy.columns) - len(X.columns)} new features\")\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class CategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Smart categorical encoding based on cardinality\"\"\"\n",
    "    \n",
    "    def __init__(self, max_onehot=10):\n",
    "        self.max_onehot = max_onehot\n",
    "        self.encoders_ = {}\n",
    "        self.encoding_type_ = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in X.select_dtypes(include=['object']).columns:\n",
    "            n_unique = X[col].nunique()\n",
    "            \n",
    "            if n_unique <= 2:\n",
    "                # Binary encoding\n",
    "                self.encoders_[col] = LabelEncoder()\n",
    "                self.encoders_[col].fit(X[col].fillna('missing'))\n",
    "                self.encoding_type_[col] = 'binary'\n",
    "            elif n_unique <= self.max_onehot:\n",
    "                # One-hot encoding\n",
    "                self.encoders_[col] = pd.get_dummies\n",
    "                self.encoding_type_[col] = 'onehot'\n",
    "            else:\n",
    "                # Target encoding (simplified version)\n",
    "                if y is not None:\n",
    "                    mean_target = y.mean()\n",
    "                    self.encoders_[col] = X[col].map(\n",
    "                        X.groupby(col)[y.name].mean()\n",
    "                    ).fillna(mean_target)\n",
    "                    self.encoding_type_[col] = 'target'\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for col, encoding_type in self.encoding_type_.items():\n",
    "            if encoding_type == 'binary':\n",
    "                X_copy[col] = self.encoders_[col].transform(X[col].fillna('missing'))\n",
    "            elif encoding_type == 'onehot':\n",
    "                dummies = pd.get_dummies(X[col], prefix=col)\n",
    "                X_copy = pd.concat([X_copy.drop(col, axis=1), dummies], axis=1)\n",
    "        \n",
    "        logger.info(f\"Encoded {len(self.encoding_type_)} categorical columns\")\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "print(\"âœ…  Custom transformers created:\")\n",
    "print(\"  â€¢ OutlierRemover\")\n",
    "print(\"  â€¢ MissingIndicator\")\n",
    "print(\"  â€¢ FeatureEngineer\")\n",
    "print(\"  â€¢ CategoryEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 2: Pipeline Architecture\n",
    "\n",
    "### ðŸŽ¯ Building Modular Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š CREATING SAMPLE DATASET\n",
      "========================================\n",
      "Dataset shape: (5000, 18)\n",
      "Churn rate: 31.0%\n",
      "Missing values: 1500\n",
      "\n",
      "First 5 rows:\n",
      "   customer_id        age  tenure_months  monthly_charges  total_charges  \\\n",
      "0            1  52.450712       4.420973        28.998892            NaN   \n",
      "1            2  42.926035       5.055371        42.243909     250.571324   \n",
      "2            3  54.715328      14.820140        22.840260     282.019941   \n",
      "3            4  67.845448       8.094345        49.860711     346.586247   \n",
      "4            5  41.487699       6.823322        93.380864     562.060393   \n",
      "\n",
      "   num_services   contract_type payment_method internet_service tech_support  \\\n",
      "0             4  Month-to-month     Electronic      Fiber optic           No   \n",
      "1             2        Two year  Bank transfer              DSL           No   \n",
      "2             3  Month-to-month     Electronic               No           No   \n",
      "3             2        One year    Credit card      Fiber optic           No   \n",
      "4             1  Month-to-month  Bank transfer              DSL           No   \n",
      "\n",
      "  online_security device_protection streaming_tv streaming_movies  \\\n",
      "0             Yes                No          Yes              Yes   \n",
      "1             Yes                No           No              Yes   \n",
      "2             Yes                No          Yes               No   \n",
      "3              No                No           No               No   \n",
      "4              No               Yes          Yes               No   \n",
      "\n",
      "   satisfaction_score  support_tickets  late_payments  churn  \n",
      "0                 NaN              2.0              0      0  \n",
      "1            4.079777              3.0              0      0  \n",
      "2            3.974223              2.0              0      0  \n",
      "3            3.819199              0.0              2      0  \n",
      "4            1.042771              4.0              1      1  \n"
     ]
    }
   ],
   "source": [
    "# 2.1 Create Sample Dataset\n",
    "print(\"ðŸ“Š CREATING SAMPLE DATASET\\n\" + \"=\"*40)\n",
    "\n",
    "# Generate realistic customer churn dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.normal(45, 15, n_samples).clip(18, 80),\n",
    "    'tenure_months': np.random.exponential(24, n_samples),\n",
    "    'monthly_charges': np.random.gamma(2, 40, n_samples),\n",
    "    'total_charges': np.random.lognormal(7, 1.5, n_samples),\n",
    "    'num_services': np.random.poisson(3, n_samples),\n",
    "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['Electronic', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples, p=[0.3, 0.5, 0.2]),\n",
    "    'tech_support': np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7]),\n",
    "    'online_security': np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6]),\n",
    "    'device_protection': np.random.choice(['Yes', 'No'], n_samples, p=[0.35, 0.65]),\n",
    "    'streaming_tv': np.random.choice(['Yes', 'No'], n_samples, p=[0.45, 0.55]),\n",
    "    'streaming_movies': np.random.choice(['Yes', 'No'], n_samples, p=[0.45, 0.55]),\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
    "    'support_tickets': np.random.poisson(2, n_samples),\n",
    "    'late_payments': np.random.poisson(0.5, n_samples)\n",
    "})\n",
    "\n",
    "# Add correlations and create target\n",
    "data['total_charges'] = data['monthly_charges'] * data['tenure_months'] * np.random.uniform(0.8, 1.2, n_samples)\n",
    "\n",
    "# Create churn target with logical relationships\n",
    "churn_probability = (\n",
    "    0.1 +  # Base probability\n",
    "    0.3 * (data['contract_type'] == 'Month-to-month') +\n",
    "    0.2 * (data['satisfaction_score'] < 2.5) +\n",
    "    0.1 * (data['support_tickets'] > 5) +\n",
    "    0.1 * (data['late_payments'] > 2) +\n",
    "    0.1 * (data['tenure_months'] < 12) -\n",
    "    0.2 * (data['contract_type'] == 'Two year') -\n",
    "    0.1 * (data['online_security'] == 'Yes')\n",
    ")\n",
    "\n",
    "data['churn'] = (np.random.random(n_samples) < churn_probability).astype(int)\n",
    "\n",
    "# Add missing values randomly\n",
    "missing_cols = ['satisfaction_score', 'support_tickets', 'total_charges']\n",
    "for col in missing_cols:\n",
    "    missing_idx = np.random.choice(data.index, size=int(0.1 * len(data)), replace=False)\n",
    "    data.loc[missing_idx, col] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "outlier_idx = np.random.choice(data.index, size=50, replace=False)\n",
    "data.loc[outlier_idx, 'monthly_charges'] *= 5\n",
    "data.loc[outlier_idx[:25], 'total_charges'] *= 10\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Churn rate: {data['churn'].mean()*100:.1f}%\")\n",
    "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "customer_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tenure_months",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "monthly_charges",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_charges",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_services",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "contract_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "payment_method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "internet_service",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tech_support",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "online_security",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "device_protection",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "streaming_tv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "streaming_movies",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "satisfaction_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "support_tickets",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "late_payments",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "churn",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "ref": "52f84ba9-dddc-425a-b6d7-480577f4f164",
       "rows": [
        [
         "0",
         "1",
         "52.450712295168486",
         "4.420973483664913",
         "28.99889199518018",
         null,
         "4",
         "Month-to-month",
         "Electronic",
         "Fiber optic",
         "No",
         "Yes",
         "No",
         "Yes",
         "Yes",
         null,
         "2.0",
         "0",
         "0"
        ],
        [
         "1",
         "2",
         "42.92603548243223",
         "5.055371308302019",
         "42.243908678569106",
         "250.57132361603857",
         "2",
         "Two year",
         "Bank transfer",
         "DSL",
         "No",
         "Yes",
         "No",
         "No",
         "Yes",
         "4.0797769646223845",
         "3.0",
         "0",
         "0"
        ],
        [
         "2",
         "3",
         "54.715328071510385",
         "14.820140296399597",
         "22.84026042335263",
         "282.01994130303706",
         "3",
         "Month-to-month",
         "Electronic",
         "No",
         "No",
         "Yes",
         "No",
         "Yes",
         "No",
         "3.974223018425849",
         "2.0",
         "0",
         "0"
        ],
        [
         "3",
         "4",
         "67.84544784612038",
         "8.094344573343204",
         "49.860711437467124",
         "346.58624743157657",
         "2",
         "One year",
         "Credit card",
         "Fiber optic",
         "No",
         "No",
         "No",
         "No",
         "No",
         "3.8191993617837863",
         "0.0",
         "2",
         "0"
        ],
        [
         "4",
         "5",
         "41.48769937914996",
         "6.823322140669283",
         "93.38086391127948",
         "562.0603929537828",
         "1",
         "Month-to-month",
         "Bank transfer",
         "DSL",
         "No",
         "No",
         "Yes",
         "Yes",
         "No",
         "1.0427708845608326",
         "4.0",
         "1",
         "1"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>monthly_charges</th>\n",
       "      <th>total_charges</th>\n",
       "      <th>num_services</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>internet_service</th>\n",
       "      <th>tech_support</th>\n",
       "      <th>online_security</th>\n",
       "      <th>device_protection</th>\n",
       "      <th>streaming_tv</th>\n",
       "      <th>streaming_movies</th>\n",
       "      <th>satisfaction_score</th>\n",
       "      <th>support_tickets</th>\n",
       "      <th>late_payments</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>52.450712</td>\n",
       "      <td>4.420973</td>\n",
       "      <td>28.998892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42.926035</td>\n",
       "      <td>5.055371</td>\n",
       "      <td>42.243909</td>\n",
       "      <td>250.571324</td>\n",
       "      <td>2</td>\n",
       "      <td>Two year</td>\n",
       "      <td>Bank transfer</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.079777</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54.715328</td>\n",
       "      <td>14.820140</td>\n",
       "      <td>22.840260</td>\n",
       "      <td>282.019941</td>\n",
       "      <td>3</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3.974223</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>67.845448</td>\n",
       "      <td>8.094345</td>\n",
       "      <td>49.860711</td>\n",
       "      <td>346.586247</td>\n",
       "      <td>2</td>\n",
       "      <td>One year</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3.819199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41.487699</td>\n",
       "      <td>6.823322</td>\n",
       "      <td>93.380864</td>\n",
       "      <td>562.060393</td>\n",
       "      <td>1</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Bank transfer</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1.042771</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id        age  tenure_months  monthly_charges  total_charges  \\\n",
       "0            1  52.450712       4.420973        28.998892            NaN   \n",
       "1            2  42.926035       5.055371        42.243909     250.571324   \n",
       "2            3  54.715328      14.820140        22.840260     282.019941   \n",
       "3            4  67.845448       8.094345        49.860711     346.586247   \n",
       "4            5  41.487699       6.823322        93.380864     562.060393   \n",
       "\n",
       "   num_services   contract_type payment_method internet_service tech_support  \\\n",
       "0             4  Month-to-month     Electronic      Fiber optic           No   \n",
       "1             2        Two year  Bank transfer              DSL           No   \n",
       "2             3  Month-to-month     Electronic               No           No   \n",
       "3             2        One year    Credit card      Fiber optic           No   \n",
       "4             1  Month-to-month  Bank transfer              DSL           No   \n",
       "\n",
       "  online_security device_protection streaming_tv streaming_movies  \\\n",
       "0             Yes                No          Yes              Yes   \n",
       "1             Yes                No           No              Yes   \n",
       "2             Yes                No          Yes               No   \n",
       "3              No                No           No               No   \n",
       "4              No               Yes          Yes               No   \n",
       "\n",
       "   satisfaction_score  support_tickets  late_payments  churn  \n",
       "0                 NaN              2.0              0      0  \n",
       "1            4.079777              3.0              0      0  \n",
       "2            3.974223              2.0              0      0  \n",
       "3            3.819199              0.0              2      0  \n",
       "4            1.042771              4.0              1      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ BUILDING PREPROCESSING PIPELINE\n",
      "========================================\n",
      "Training set: (4000, 16)\n",
      "Test set: (1000, 16)\n",
      "\n",
      "Numeric features (8): ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'num_services']...\n",
      "Categorical features (8): ['contract_type', 'payment_method', 'internet_service', 'tech_support', 'online_security']...\n",
      "\n",
      "ðŸ“‹ Pipeline Structure:\n",
      "1. Numeric Pipeline:\n",
      "   â†’ Median Imputation\n",
      "   â†’ Outlier Capping\n",
      "   â†’ Robust Scaling\n",
      "\n",
      "2. Categorical Pipeline:\n",
      "   â†’ Constant Imputation\n",
      "   â†’ One-Hot Encoding\n",
      "\n",
      "3. Model:\n",
      "   â†’ Random Forest Classifier\n",
      "\n",
      "â³ Training pipeline...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâ³ Training pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m full_pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     64\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m full_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:655\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m     )\n\u001b[0;32m    654\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 655\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params, raw_params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    587\u001b[0m )\n\u001b[1;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    590\u001b[0m     cloned_transformer,\n\u001b[0;32m    591\u001b[0m     X,\n\u001b[0;32m    592\u001b[0m     y,\n\u001b[0;32m    593\u001b[0m     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    594\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    595\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    596\u001b[0m     params\u001b[38;5;241m=\u001b[39mstep_params,\n\u001b[0;32m    597\u001b[0m )\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1544\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:996\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[1;32m--> 996\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_func_on_transformers(\n\u001b[0;32m    997\u001b[0m     X,\n\u001b[0;32m    998\u001b[0m     y,\n\u001b[0;32m    999\u001b[0m     _fit_transform_one,\n\u001b[0;32m   1000\u001b[0m     column_as_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1001\u001b[0m     routed_params\u001b[38;5;241m=\u001b[39mrouted_params,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:897\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[1;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    886\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    887\u001b[0m             delayed(func)(\n\u001b[0;32m    888\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    894\u001b[0m             )\n\u001b[0;32m    895\u001b[0m         )\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(jobs)\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig), warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m=\u001b[39m warning_filters\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1544\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:719\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    718\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 719\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    721\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    587\u001b[0m )\n\u001b[1;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    590\u001b[0m     cloned_transformer,\n\u001b[0;32m    591\u001b[0m     X,\n\u001b[0;32m    592\u001b[0m     y,\n\u001b[0;32m    593\u001b[0m     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    594\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    595\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    596\u001b[0m     params\u001b[38;5;241m=\u001b[39mstep_params,\n\u001b[0;32m    597\u001b[0m )\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1544\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:897\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mOutlierRemover.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m X[column]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     14\u001b[0m             Q1 \u001b[38;5;241m=\u001b[39m X[column]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# 2.2 Build Preprocessing Pipeline\n",
    "print(\"ðŸ”§ BUILDING PREPROCESSING PIPELINE\\n\" + \"=\"*40)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['customer_id', 'churn'], axis=1)\n",
    "y = data['churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Define column groups\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features[:5]}...\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
    "\n",
    "# Create preprocessing pipelines for numeric and categorical data\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outliers', OutlierRemover(factor=1.5)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\nðŸ“‹ Pipeline Structure:\")\n",
    "print(\"1. Numeric Pipeline:\")\n",
    "print(\"   â†’ Median Imputation\")\n",
    "print(\"   â†’ Outlier Capping\")\n",
    "print(\"   â†’ Robust Scaling\")\n",
    "print(\"\\n2. Categorical Pipeline:\")\n",
    "print(\"   â†’ Constant Imputation\")\n",
    "print(\"   â†’ One-Hot Encoding\")\n",
    "print(\"\\n3. Model:\")\n",
    "print(\"   â†’ Random Forest Classifier\")\n",
    "\n",
    "# Fit the pipeline\n",
    "print(\"\\nâ³ Training pipeline...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\n ðŸ“Š Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 3: Advanced Pipeline Features\n",
    "\n",
    "### ðŸŽ¯ Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 09:05:15,878 - INFO - Added 3 missing indicators\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ ADVANCED FEATURE ENGINEERING\n",
      "========================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'Two year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m\n\u001b[0;32m     77\u001b[0m advanced_numeric_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     78\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_indicator\u001b[39m\u001b[38;5;124m'\u001b[39m, MissingIndicator(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)),\n\u001b[0;32m     79\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler())\n\u001b[0;32m     83\u001b[0m ])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Test advanced pipeline\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m X_train_advanced \u001b[38;5;241m=\u001b[39m advanced_numeric_pipeline\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter engineering: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_advanced\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:719\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    718\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 719\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    721\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[0;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[0;32m    587\u001b[0m )\n\u001b[1;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    590\u001b[0m     cloned_transformer,\n\u001b[0;32m    591\u001b[0m     X,\n\u001b[0;32m    592\u001b[0m     y,\n\u001b[0;32m    593\u001b[0m     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    594\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    595\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    596\u001b[0m     params\u001b[38;5;241m=\u001b[39mstep_params,\n\u001b[0;32m    597\u001b[0m )\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1544\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:894\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    879\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    880\u001b[0m             (\n\u001b[0;32m    881\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m         )\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\impute\\_base.py:436\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    420\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\RC\\miniconda3\\envs\\ML_Python\\Lib\\site-packages\\sklearn\\impute\\_base.py:361\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[0;32m    356\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[0;32m    359\u001b[0m         )\n\u001b[0;32m    360\u001b[0m     )\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'Two year'"
     ]
    }
   ],
   "source": [
    "# 3.1 Advanced Feature Engineering Pipeline\n",
    "print(\"ðŸš€ ADVANCED FEATURE ENGINEERING\\n\" + \"=\"*40)\n",
    "\n",
    "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Advanced feature engineering with domain knowledge\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Domain-specific features for telecom churn\n",
    "        if 'monthly_charges' in X.columns and 'tenure_months' in X.columns:\n",
    "            X_copy['avg_charges_per_month'] = X['total_charges'] / (X['tenure_months'] + 1)\n",
    "            X_copy['charge_increase_rate'] = (X['monthly_charges'] - X_copy['avg_charges_per_month']) / (X_copy['avg_charges_per_month'] + 1)\n",
    "        \n",
    "        if 'num_services' in X.columns:\n",
    "            X_copy['service_diversity'] = X['num_services'] / X['num_services'].max()\n",
    "        \n",
    "        if 'support_tickets' in X.columns and 'tenure_months' in X.columns:\n",
    "            X_copy['tickets_per_month'] = X['support_tickets'] / (X['tenure_months'] + 1)\n",
    "        \n",
    "        if 'satisfaction_score' in X.columns:\n",
    "            X_copy['is_satisfied'] = (X['satisfaction_score'] >= 3.5).astype(int)\n",
    "            X_copy['very_dissatisfied'] = (X['satisfaction_score'] < 2).astype(int)\n",
    "        \n",
    "        # Contract value features\n",
    "        if 'contract_type' in X.columns:\n",
    "            X_copy['is_month_to_month'] = (X['contract_type'] == 'Month-to-month').astype(int)\n",
    "            X_copy['has_long_contract'] = (X['contract_type'].isin(['One year', 'Two year'])).astype(int)\n",
    "        \n",
    "        # Service bundle features\n",
    "        service_cols = ['internet_service', 'tech_support', 'online_security', \n",
    "                       'device_protection', 'streaming_tv', 'streaming_movies']\n",
    "        available_services = [col for col in service_cols if col in X.columns]\n",
    "        if available_services:\n",
    "            X_copy['total_services'] = sum([(X[col] == 'Yes').astype(int) \n",
    "                                           for col in available_services \n",
    "                                           if col != 'internet_service'])\n",
    "        \n",
    "        # Risk indicators\n",
    "        if 'late_payments' in X.columns:\n",
    "            X_copy['payment_risk'] = (X['late_payments'] > 0).astype(int)\n",
    "            X_copy['high_payment_risk'] = (X['late_payments'] > 2).astype(int)\n",
    "        \n",
    "        logger.info(f\"Created {len(X_copy.columns) - len(X.columns)} domain-specific features\")\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Select best features based on importance\"\"\"\n",
    "    \n",
    "    def __init__(self, method='mutual_info', k=20):\n",
    "        self.method = method\n",
    "        self.k = k\n",
    "        self.selected_features_ = None\n",
    "        self.scores_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'mutual_info':\n",
    "            selector = SelectKBest(mutual_info_classif, k=min(self.k, X.shape[1]))\n",
    "        else:\n",
    "            selector = SelectKBest(f_classif, k=min(self.k, X.shape[1]))\n",
    "        \n",
    "        selector.fit(X, y)\n",
    "        self.selected_features_ = X.columns[selector.get_support()].tolist()\n",
    "        self.scores_ = selector.scores_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        logger.info(f\"Selected {len(self.selected_features_)} features from {X.shape[1]}\")\n",
    "        return X[self.selected_features_]\n",
    "\n",
    "\n",
    "# Create advanced pipeline\n",
    "advanced_numeric_pipeline = Pipeline([\n",
    "    ('missing_indicator', MissingIndicator(threshold=0.05)),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('outliers', OutlierRemover(factor=2.0)),\n",
    "    ('engineer', AdvancedFeatureEngineer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Test advanced pipeline\n",
    "X_train_advanced = advanced_numeric_pipeline.fit_transform(X_train)\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After engineering: {X_train_advanced.shape[1]}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a model to get feature importances\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_advanced, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_names = [f\"feature_{i}\" for i in range(X_train_advanced.shape[1])]\n",
    "importances = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = importances.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Top 10 Features:\")\n",
    "print(importances.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 4: Cross-Validation and Hyperparameter Tuning\n",
    "\n",
    "### ðŸŽ¯ Optimizing Pipeline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Cross-Validation and Grid Search\n",
    "print(\"ðŸ” PIPELINE OPTIMIZATION\\n\" + \"=\"*40)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a simpler pipeline for faster optimization\n",
    "optimization_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'preprocessor__num__scaler': [StandardScaler(), RobustScaler()],\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [5, 10, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"ðŸ” Starting Grid Search...\")\n",
    "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    optimization_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… Grid Search Complete!\")\n",
    "print(f\"Best Score: {grid_search.best_score_:.3f}\")\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nðŸ“Š Best Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_best):.3f}\")\n",
    "\n",
    "# Cross-validation scores\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_scores = cross_validate(\n",
    "    best_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=['accuracy', 'precision', 'recall', 'roc_auc'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Display CV results\n",
    "cv_results = pd.DataFrame(cv_scores)\n",
    "print(\"\\nðŸ“Š Cross-Validation Results (5-fold):\")\n",
    "print(cv_results[['test_accuracy', 'test_precision', 'test_recall', 'test_roc_auc']].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 5: Pipeline Persistence\n",
    "\n",
    "### ðŸŽ¯ Saving and Loading Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Save and Load Pipeline\n",
    "print(\"ðŸ’¾ PIPELINE PERSISTENCE\\n\" + \"=\"*40)\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'model_name': 'churn_prediction_pipeline',\n",
    "    'version': '1.0.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'model_type': type(best_model).__name__,\n",
    "    'features': list(X_train.columns),\n",
    "    'performance': {\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba_best),\n",
    "        'accuracy': accuracy_score(y_test, y_pred_best),\n",
    "        'precision': precision_score(y_test, y_pred_best),\n",
    "        'recall': recall_score(y_test, y_pred_best)\n",
    "    },\n",
    "    'training_data_shape': X_train.shape,\n",
    "    'parameters': grid_search.best_params_\n",
    "}\n",
    "\n",
    "# Save pipeline using joblib\n",
    "pipeline_filename = 'churn_prediction_pipeline.pkl'\n",
    "metadata_filename = 'pipeline_metadata.json'\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(best_model, pipeline_filename)\n",
    "print(f\"âœ… Pipeline saved to {pipeline_filename}\")\n",
    "\n",
    "# Save metadata\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"âœ… Metadata saved to {metadata_filename}\")\n",
    "\n",
    "# Display metadata\n",
    "print(\"\\nðŸ“‹ Model Metadata:\")\n",
    "for key, value in model_metadata.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    elif isinstance(value, list) and len(value) > 5:\n",
    "        print(f\"{key}: [{value[0]}, {value[1]}, ... ] ({len(value)} items)\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Load and test pipeline\n",
    "print(\"\\nðŸ”„ Loading pipeline...\")\n",
    "loaded_pipeline = joblib.load(pipeline_filename)\n",
    "\n",
    "# Test loaded pipeline\n",
    "sample_data = X_test.iloc[:5]\n",
    "predictions = loaded_pipeline.predict(sample_data)\n",
    "probabilities = loaded_pipeline.predict_proba(sample_data)[:, 1]\n",
    "\n",
    "print(\"\\nâœ… Pipeline loaded successfully!\")\n",
    "print(\"\\nSample predictions:\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:5].values,\n",
    "    'Predicted': predictions,\n",
    "    'Probability': probabilities.round(3)\n",
    "})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 6: Production Deployment\n",
    "\n",
    "### ðŸŽ¯ API-Ready Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Create Production-Ready Prediction Function\n",
    "print(\"ðŸš€ PRODUCTION DEPLOYMENT\\n\" + \"=\"*40)\n",
    "\n",
    "class ChurnPredictor:\n",
    "    \"\"\"Production-ready churn prediction class\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, metadata_path):\n",
    "        self.model = joblib.load(model_path)\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        self.features = self.metadata['features']\n",
    "        logger.info(f\"Model loaded: {self.metadata['model_name']} v{self.metadata['version']}\")\n",
    "    \n",
    "    def validate_input(self, data):\n",
    "        \"\"\"Validate input data\"\"\"\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "        \n",
    "        missing_features = set(self.features) - set(data.columns)\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features: {missing_features}\")\n",
    "        \n",
    "        return data[self.features]\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            data = self.validate_input(data)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.model.predict(data)\n",
    "            probabilities = self.model.predict_proba(data)[:, 1]\n",
    "            \n",
    "            # Create response\n",
    "            response = {\n",
    "                'predictions': predictions.tolist(),\n",
    "                'probabilities': probabilities.tolist(),\n",
    "                'model_version': self.metadata['version'],\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Prediction completed for {len(data)} samples\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_single(self, customer_data):\n",
    "        \"\"\"Predict for a single customer\"\"\"\n",
    "        df = pd.DataFrame([customer_data])\n",
    "        result = self.predict(df)\n",
    "        \n",
    "        return {\n",
    "            'churn_prediction': 'Yes' if result['predictions'][0] == 1 else 'No',\n",
    "            'churn_probability': result['probabilities'][0],\n",
    "            'risk_level': self._get_risk_level(result['probabilities'][0]),\n",
    "            'timestamp': result['timestamp']\n",
    "        }\n",
    "    \n",
    "    def _get_risk_level(self, probability):\n",
    "        \"\"\"Categorize risk level\"\"\"\n",
    "        if probability < 0.3:\n",
    "            return 'Low'\n",
    "        elif probability < 0.7:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'High'\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information\"\"\"\n",
    "        return self.metadata\n",
    "\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = ChurnPredictor(pipeline_filename, metadata_filename)\n",
    "\n",
    "# Test single prediction\n",
    "test_customer = X_test.iloc[0].to_dict()\n",
    "result = predictor.predict_single(test_customer)\n",
    "\n",
    "print(\"ðŸ“Š Single Customer Prediction:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Batch prediction\n",
    "batch_result = predictor.predict(X_test.iloc[:10])\n",
    "print(f\"\\nðŸ“Š Batch Prediction ({len(batch_result['predictions'])} customers):\")\n",
    "print(f\"  Churn rate: {np.mean(batch_result['predictions'])*100:.1f}%\")\n",
    "print(f\"  Average probability: {np.mean(batch_result['probabilities']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 7: Monitoring and Validation\n",
    "\n",
    "### ðŸŽ¯ Pipeline Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Monitoring and Validation\n",
    "print(\"ðŸ“Š PIPELINE MONITORING\\n\" + \"=\"*40)\n",
    "\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Monitor pipeline performance and data drift\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data):\n",
    "        self.reference_data = reference_data\n",
    "        self.reference_stats = self._calculate_stats(reference_data)\n",
    "        self.monitoring_history = []\n",
    "    \n",
    "    def _calculate_stats(self, data):\n",
    "        \"\"\"Calculate statistics for monitoring\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # Numeric features\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            stats[col] = {\n",
    "                'mean': data[col].mean(),\n",
    "                'std': data[col].std(),\n",
    "                'min': data[col].min(),\n",
    "                'max': data[col].max(),\n",
    "                'missing_pct': data[col].isnull().mean()\n",
    "            }\n",
    "        \n",
    "        # Categorical features\n",
    "        categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            stats[col] = {\n",
    "                'unique_values': data[col].nunique(),\n",
    "                'mode': data[col].mode()[0] if not data[col].mode().empty else None,\n",
    "                'missing_pct': data[col].isnull().mean()\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def detect_drift(self, new_data, threshold=0.1):\n",
    "        \"\"\"Detect data drift\"\"\"\n",
    "        new_stats = self._calculate_stats(new_data)\n",
    "        drift_report = {}\n",
    "        \n",
    "        for feature in self.reference_stats:\n",
    "            if feature in new_stats:\n",
    "                ref = self.reference_stats[feature]\n",
    "                new = new_stats[feature]\n",
    "                \n",
    "                if 'mean' in ref:  # Numeric feature\n",
    "                    mean_change = abs(new['mean'] - ref['mean']) / (ref['mean'] + 1e-8)\n",
    "                    std_change = abs(new['std'] - ref['std']) / (ref['std'] + 1e-8)\n",
    "                    \n",
    "                    if mean_change > threshold or std_change > threshold:\n",
    "                        drift_report[feature] = {\n",
    "                            'mean_change': mean_change,\n",
    "                            'std_change': std_change,\n",
    "                            'status': 'DRIFT DETECTED'\n",
    "                        }\n",
    "        \n",
    "        return drift_report\n",
    "    \n",
    "    def validate_predictions(self, predictions, expected_range=(0, 1)):\n",
    "        \"\"\"Validate prediction outputs\"\"\"\n",
    "        validation_report = {\n",
    "            'total_predictions': len(predictions),\n",
    "            'null_predictions': np.isnan(predictions).sum(),\n",
    "            'out_of_range': ((predictions < expected_range[0]) | \n",
    "                           (predictions > expected_range[1])).sum(),\n",
    "            'mean_prediction': np.mean(predictions),\n",
    "            'std_prediction': np.std(predictions)\n",
    "        }\n",
    "        \n",
    "        # Validation status\n",
    "        if validation_report['null_predictions'] > 0:\n",
    "            validation_report['status'] = 'FAILED - Null predictions found'\n",
    "        elif validation_report['out_of_range'] > 0:\n",
    "            validation_report['status'] = 'WARNING - Out of range predictions'\n",
    "        else:\n",
    "            validation_report['status'] = 'PASSED'\n",
    "        \n",
    "        return validation_report\n",
    "    \n",
    "    def log_monitoring_event(self, event_type, details):\n",
    "        \"\"\"Log monitoring events\"\"\"\n",
    "        event = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'event_type': event_type,\n",
    "            'details': details\n",
    "        }\n",
    "        self.monitoring_history.append(event)\n",
    "        logger.info(f\"Monitoring event: {event_type}\")\n",
    "        return event\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = PipelineMonitor(X_train)\n",
    "\n",
    "# Test drift detection\n",
    "print(\"ðŸ” Drift Detection Test:\")\n",
    "drift_report = monitor.detect_drift(X_test)\n",
    "if drift_report:\n",
    "    print(\"âš ï¸ Drift detected in features:\")\n",
    "    for feature, details in drift_report.items():\n",
    "        print(f\"  {feature}: {details['status']}\")\n",
    "else:\n",
    "    print(\"âœ… No significant drift detected\")\n",
    "\n",
    "# Validate predictions\n",
    "test_predictions = predictor.predict(X_test)['probabilities']\n",
    "validation_report = monitor.validate_predictions(test_predictions)\n",
    "\n",
    "print(\"\\nðŸ“Š Prediction Validation:\")\n",
    "for key, value in validation_report.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Performance tracking\n",
    "def track_model_performance(y_true, y_pred, y_proba):\n",
    "    \"\"\"Track model performance metrics over time\"\"\"\n",
    "    \n",
    "    performance_metrics = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1_score': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "# Track current performance\n",
    "current_performance = track_model_performance(y_test, y_pred_best, y_pred_proba_best)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Current Model Performance:\")\n",
    "for metric, value in current_performance.items():\n",
    "    if metric != 'confusion_matrix' and metric != 'timestamp':\n",
    "        print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 8: Complete Production Pipeline\n",
    "\n",
    "### ðŸŽ¯ Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Complete Production Pipeline Class\n",
    "print(\"ðŸ—ï¸ COMPLETE PRODUCTION PIPELINE\\n\" + \"=\"*40)\n",
    "\n",
    "class ProductionPipeline:\n",
    "    \"\"\"Complete production-ready ML pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or self._default_config()\n",
    "        self.pipeline = None\n",
    "        self.monitor = None\n",
    "        self.metadata = {}\n",
    "        self.performance_history = []\n",
    "        \n",
    "    def _default_config(self):\n",
    "        return {\n",
    "            'model_name': 'production_pipeline',\n",
    "            'version': '1.0.0',\n",
    "            'test_size': 0.2,\n",
    "            'random_state': 42,\n",
    "            'cv_folds': 5,\n",
    "            'scoring_metric': 'roc_auc',\n",
    "            'drift_threshold': 0.1,\n",
    "            'feature_engineering': True,\n",
    "            'hyperparameter_tuning': True\n",
    "        }\n",
    "    \n",
    "    def build_pipeline(self, X, y):\n",
    "        \"\"\"Build complete preprocessing and model pipeline\"\"\"\n",
    "        \n",
    "        logger.info(\"Building production pipeline...\")\n",
    "        \n",
    "        # Identify feature types\n",
    "        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        # Create preprocessing pipelines\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        # Combine preprocessing\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        # Create full pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(random_state=self.config['random_state']))\n",
    "        ])\n",
    "        \n",
    "        # Store metadata\n",
    "        self.metadata['features'] = {\n",
    "            'numeric': numeric_features,\n",
    "            'categorical': categorical_features,\n",
    "            'total': len(numeric_features) + len(categorical_features)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Pipeline built with {self.metadata['features']['total']} features\")\n",
    "        return self\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the pipeline\"\"\"\n",
    "        \n",
    "        logger.info(\"Starting training...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.config['test_size'],\n",
    "            random_state=self.config['random_state'],\n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        # Build pipeline if not exists\n",
    "        if self.pipeline is None:\n",
    "            self.build_pipeline(X_train, y_train)\n",
    "        \n",
    "        # Train pipeline\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate\n",
    "        y_pred = self.pipeline.predict(X_val)\n",
    "        y_proba = self.pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'precision': precision_score(y_val, y_pred),\n",
    "            'recall': recall_score(y_val, y_pred),\n",
    "            'f1_score': f1_score(y_val, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_val, y_proba)\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        self.metadata['training'] = {\n",
    "            'train_size': len(X_train),\n",
    "            'val_size': len(X_val),\n",
    "            'validation_metrics': metrics,\n",
    "            'trained_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Initialize monitor\n",
    "        self.monitor = PipelineMonitor(X_train)\n",
    "        \n",
    "        logger.info(f\"Training complete. ROC-AUC: {metrics['roc_auc']:.3f}\")\n",
    "        return metrics\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions with monitoring\"\"\"\n",
    "        \n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Pipeline not trained. Call train() first.\")\n",
    "        \n",
    "        # Check for drift\n",
    "        if self.monitor:\n",
    "            drift_report = self.monitor.detect_drift(X, self.config['drift_threshold'])\n",
    "            if drift_report:\n",
    "                logger.warning(f\"Data drift detected in {len(drift_report)} features\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.pipeline.predict(X)\n",
    "        probabilities = self.pipeline.predict_proba(X)[:, 1]\n",
    "        \n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'probabilities': probabilities,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save pipeline and metadata\"\"\"\n",
    "        \n",
    "        # Save pipeline\n",
    "        joblib.dump(self.pipeline, f\"{filepath}_pipeline.pkl\")\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(f\"{filepath}_metadata.json\", 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Pipeline saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load pipeline and metadata\"\"\"\n",
    "        \n",
    "        self.pipeline = joblib.load(f\"{filepath}_pipeline.pkl\")\n",
    "        \n",
    "        with open(f\"{filepath}_metadata.json\", 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Pipeline loaded from {filepath}\")\n",
    "        return self\n",
    "\n",
    "\n",
    "# Test complete pipeline\n",
    "print(\"\\nðŸ§ª Testing Complete Pipeline:\")\n",
    "\n",
    "# Initialize\n",
    "prod_pipeline = ProductionPipeline()\n",
    "\n",
    "# Train\n",
    "metrics = prod_pipeline.train(X, y)\n",
    "\n",
    "print(\"\\nðŸ“Š Training Results:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "test_results = prod_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"\\nâœ… Predictions made for {len(test_results['predictions'])} samples\")\n",
    "print(f\"  Average probability: {np.mean(test_results['probabilities']):.3f}\")\n",
    "\n",
    "# Save pipeline\n",
    "prod_pipeline.save('production')\n",
    "print(\"\\nðŸ’¾ Pipeline saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 9: API Simulation\n",
    "\n",
    "### ðŸŽ¯ REST API Endpoint Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 API Endpoint Simulation\n",
    "print(\"ðŸŒ API ENDPOINT SIMULATION\\n\" + \"=\"*40)\n",
    "\n",
    "def api_predict(request_data):\n",
    "    \"\"\"Simulate API endpoint for predictions\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Validate request\n",
    "        if not request_data or 'data' not in request_data:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'Invalid request. Missing data field.',\n",
    "                'code': 400\n",
    "            }\n",
    "        \n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(request_data['data'])\n",
    "        \n",
    "        # Make predictions\n",
    "        results = prod_pipeline.predict(df)\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            'status': 'success',\n",
    "            'code': 200,\n",
    "            'data': {\n",
    "                'predictions': results['predictions'].tolist(),\n",
    "                'probabilities': results['probabilities'].tolist(),\n",
    "                'model_version': prod_pipeline.config['version'],\n",
    "                'timestamp': results['timestamp']\n",
    "            },\n",
    "            'metadata': {\n",
    "                'samples_processed': len(results['predictions']),\n",
    "                'model_name': prod_pipeline.config['model_name']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'message': str(e),\n",
    "            'code': 500\n",
    "        }\n",
    "\n",
    "\n",
    "# Test API endpoint\n",
    "print(\"ðŸ“¡ Testing API Endpoint:\\n\")\n",
    "\n",
    "# Prepare test request\n",
    "test_request = {\n",
    "    'data': X_test.iloc[:3].to_dict('records')\n",
    "}\n",
    "\n",
    "print(\"Request:\")\n",
    "print(f\"  Sending {len(test_request['data'])} samples\")\n",
    "\n",
    "# Make API call\n",
    "response = api_predict(test_request)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(f\"  Status: {response['status']}\")\n",
    "print(f\"  Code: {response['code']}\")\n",
    "if response['status'] == 'success':\n",
    "    print(f\"  Predictions: {response['data']['predictions']}\")\n",
    "    print(f\"  Probabilities: {[round(p, 3) for p in response['data']['probabilities']]}\")\n",
    "    print(f\"  Model Version: {response['data']['model_version']}\")\n",
    "\n",
    "# Test error handling\n",
    "print(\"\\nðŸ” Testing Error Handling:\")\n",
    "bad_request = {'invalid': 'data'}\n",
    "error_response = api_predict(bad_request)\n",
    "print(f\"  Error Status: {error_response['status']}\")\n",
    "print(f\"  Error Message: {error_response['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Œ Section 10: Pipeline Documentation\n",
    "\n",
    "### ðŸŽ¯ Complete Documentation and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Generate Pipeline Documentation\n",
    "print(\"ðŸ“š PIPELINE DOCUMENTATION\\n\" + \"=\"*40)\n",
    "\n",
    "def generate_documentation(pipeline, metadata):\n",
    "    \"\"\"Generate comprehensive pipeline documentation\"\"\"\n",
    "    \n",
    "    doc = f\"\"\"\n",
    "# Production Pipeline Documentation\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Pipeline Overview\n",
    "- **Name**: {metadata.get('model_name', 'N/A')}\n",
    "- **Version**: {metadata.get('version', 'N/A')}\n",
    "- **Purpose**: Customer Churn Prediction\n",
    "- **Type**: Binary Classification\n",
    "\n",
    "## Features\n",
    "### Numeric Features ({len(metadata['features']['numeric'])})\n",
    "{', '.join(metadata['features']['numeric'][:5])}...\n",
    "\n",
    "### Categorical Features ({len(metadata['features']['categorical'])})\n",
    "{', '.join(metadata['features']['categorical'][:5])}...\n",
    "\n",
    "## Model Performance\n",
    "### Validation Metrics\n",
    "\"\"\"\n",
    "    \n",
    "    if 'training' in metadata and 'validation_metrics' in metadata['training']:\n",
    "        for metric, value in metadata['training']['validation_metrics'].items():\n",
    "            doc += f\"- **{metric.upper()}**: {value:.3f}\\n\"\n",
    "    \n",
    "    doc += f\"\"\"\n",
    "\n",
    "## Pipeline Components\n",
    "1. **Data Validation**\n",
    "   - Missing value checks\n",
    "   - Data type validation\n",
    "   - Range validation\n",
    "\n",
    "2. **Preprocessing**\n",
    "   - Numeric: Median imputation, Robust scaling\n",
    "   - Categorical: Mode imputation, One-hot encoding\n",
    "   - Outlier handling: IQR-based capping\n",
    "\n",
    "3. **Feature Engineering**\n",
    "   - Polynomial features\n",
    "   - Interaction terms\n",
    "   - Domain-specific features\n",
    "\n",
    "4. **Model**\n",
    "   - Algorithm: Random Forest Classifier\n",
    "   - Hyperparameters: Optimized via GridSearch\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Python\n",
    "```python\n",
    "# Load pipeline\n",
    "pipeline = joblib.load('pipeline.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(data)\n",
    "probabilities = pipeline.predict_proba(data)[:, 1]\n",
    "```\n",
    "\n",
    "### API\n",
    "```bash\n",
    "curl -X POST http://api.example.com/predict \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\"data\": [{...customer_features...}]}'\n",
    "```\n",
    "\n",
    "## Monitoring\n",
    "- Data drift detection enabled\n",
    "- Performance tracking enabled\n",
    "- Logging configured\n",
    "\n",
    "## Maintenance\n",
    "- **Retraining Schedule**: Monthly\n",
    "- **Performance Threshold**: ROC-AUC < 0.75\n",
    "- **Data Requirements**: Minimum 1000 samples\n",
    "\n",
    "## Contact\n",
    "- **Team**: Data Science Team\n",
    "- **Email**: datascience@example.com\n",
    "\"\"\"\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Generate documentation\n",
    "documentation = generate_documentation(prod_pipeline.pipeline, prod_pipeline.metadata)\n",
    "\n",
    "# Save documentation\n",
    "with open('pipeline_documentation.md', 'w') as f:\n",
    "    f.write(documentation)\n",
    "\n",
    "print(\"ðŸ“„ Documentation generated and saved!\")\n",
    "print(\"\\nDocumentation Preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(documentation[:800] + \"...\")\n",
    "\n",
    "# Best practices checklist\n",
    "print(\"\\nâœ… BEST PRACTICES CHECKLIST:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "checklist = [\n",
    "    \"âœ“ Data validation implemented\",\n",
    "    \"âœ“ Missing value handling\",\n",
    "    \"âœ“ Outlier detection and treatment\",\n",
    "    \"âœ“ Feature scaling applied\",\n",
    "    \"âœ“ Categorical encoding handled\",\n",
    "    \"âœ“ Cross-validation performed\",\n",
    "    \"âœ“ Hyperparameter tuning done\",\n",
    "    \"âœ“ Model persistence implemented\",\n",
    "    \"âœ“ Monitoring framework in place\",\n",
    "    \"âœ“ API endpoint created\",\n",
    "    \"âœ“ Documentation generated\",\n",
    "    \"âœ“ Error handling implemented\",\n",
    "    \"âœ“ Logging configured\",\n",
    "    \"âœ“ Version control ready\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Summary & Next Steps\n",
    "\n",
    "### ðŸ† What You've Built:\n",
    "\n",
    "âœ… **Custom Transformers**\n",
    "- OutlierRemover\n",
    "- MissingIndicator\n",
    "- FeatureEngineer\n",
    "- CategoryEncoder\n",
    "\n",
    "âœ… **Production Pipeline**\n",
    "- Modular architecture\n",
    "- Automated preprocessing\n",
    "- Feature engineering\n",
    "- Model training\n",
    "\n",
    "âœ… **Advanced Features**\n",
    "- Cross-validation\n",
    "- Hyperparameter tuning\n",
    "- Performance monitoring\n",
    "- Data drift detection\n",
    "\n",
    "âœ… **Deployment Ready**\n",
    "- Pipeline persistence\n",
    "- API endpoints\n",
    "- Error handling\n",
    "- Documentation\n",
    "\n",
    "âœ… **Best Practices**\n",
    "- Logging\n",
    "- Validation\n",
    "- Testing\n",
    "- Monitoring\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "\n",
    "1. **Deploy to Production** - Use Flask/FastAPI\n",
    "2. **Add More Models** - Ensemble methods\n",
    "3. **Implement A/B Testing** - Compare models\n",
    "4. **Setup CI/CD** - Automated deployment\n",
    "5. **Add Real-time Monitoring** - Dashboard\n",
    "\n",
    "### ðŸ’¡ Key Takeaways:\n",
    "\n",
    "- **Pipelines ensure reproducibility**\n",
    "- **Automation reduces errors**\n",
    "- **Monitoring prevents degradation**\n",
    "- **Documentation enables collaboration**\n",
    "\n",
    "### ðŸ“š Resources:\n",
    "\n",
    "- Scikit-learn Pipeline Documentation\n",
    "- MLOps Best Practices\n",
    "- Production ML Systems\n",
    "- Feature Engineering Techniques\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've built a complete production-ready ML pipeline!\n",
    "\n",
    "This is how real data science projects are deployed.\n",
    "\n",
    "**Keep building, keep deploying, keep improving!** ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽŠ Module Complete!\n",
    "print(\"ðŸŽŠ\" * 20)\n",
    "print(\"\\n    ðŸ† MASTER PIPELINE COMPLETE! ðŸ†\")\n",
    "print(\"\\n    You've mastered:\")\n",
    "print(\"    âœ… Custom Transformers\")\n",
    "print(\"    âœ… Pipeline Architecture\")\n",
    "print(\"    âœ… Feature Engineering\")\n",
    "print(\"    âœ… Model Deployment\")\n",
    "print(\"    âœ… Production Best Practices\")\n",
    "print(\"\\n    Ready for: Machine Learning Algorithms!\")\n",
    "print(\"\\n\" + \"ðŸŽŠ\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
