{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Preprocessing Mastery: From Messy to Model-Ready\n",
    "\n",
    "<img src='https://daxg39y63pxwu.cloudfront.net/images/blog/data-preprocessing-techniques-and-steps/data_preprocessing.webp' width='600' alt='Data Preprocessing'>\n",
    "\n",
    "\n",
    "## üéØ The Most Important Step in Data Science\n",
    "\n",
    "**80% of data science is data cleaning!** This notebook will teach you:\n",
    "- Handle **missing data** like a pro\n",
    "- Deal with **outliers** effectively\n",
    "- **Transform** data for better models\n",
    "- **Engineer** powerful features\n",
    "- Build **reproducible pipelines**\n",
    "\n",
    "### üìö What We'll Master Today:\n",
    "1. **Data Quality Assessment** - Understanding your mess\n",
    "2. **Missing Data Strategies** - Smart imputation\n",
    "3. **Outlier Detection & Treatment** - Finding anomalies\n",
    "4. **Data Transformation** - Scaling and normalization\n",
    "5. **Feature Engineering** - Creating powerful features\n",
    "6. **Encoding Categorical Data** - Handling text data\n",
    "7. **Feature Selection** - Choosing what matters\n",
    "8. **Data Balancing** - Handling imbalanced data\n",
    "9. **Pipeline Creation** - Automating everything\n",
    "10. **Real Project** - End-to-end preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Clean Some Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üßπ Data Preprocessing Mastery - Ready to Clean!\")\n",
    "print(\"\\nüí° Remember: Garbage in, garbage out! Quality preprocessing = Quality models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 1: Data Quality Assessment\n",
    "\n",
    "### üéØ Know Your Data's Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create a Messy Dataset (Realistic!)\n",
    "print(\"üé≠ CREATING REALISTIC MESSY DATA\\n\" + \"=\"*40)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create messy customer data\n",
    "data = {\n",
    "    'customer_id': range(1000, 1000 + n_samples),\n",
    "    'age': np.random.normal(35, 15, n_samples),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'credit_score': np.random.normal(650, 100, n_samples),\n",
    "    'purchase_amount': np.random.exponential(100, n_samples),\n",
    "    'membership_days': np.random.uniform(0, 1000, n_samples),\n",
    "    'gender': np.random.choice(['M', 'F', 'Male', 'Female', 'male', 'female', None], n_samples),\n",
    "    'city': np.random.choice(['New York', 'new york', 'NY', 'Los Angeles', 'LA', 'Chicago', None], n_samples),\n",
    "    'email': [f'user{i}@email.com' if np.random.random() > 0.1 else None for i in range(n_samples)],\n",
    "    'phone': [f'555-{np.random.randint(1000, 9999)}' if np.random.random() > 0.15 else None for i in range(n_samples)],\n",
    "    'has_children': np.random.choice(['Yes', 'No', 'Y', 'N', '1', '0', None], n_samples),\n",
    "    'satisfaction_score': np.random.choice([1, 2, 3, 4, 5, None], n_samples),\n",
    "    'last_purchase_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
    "    'premium_customer': np.random.choice([True, False], n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add more messiness\n",
    "# Add missing values randomly\n",
    "for col in ['age', 'income', 'credit_score']:\n",
    "    missing_idx = np.random.choice(df.index, size=int(0.1 * len(df)), replace=False)\n",
    "    df.loc[missing_idx, col] = np.nan\n",
    "\n",
    "# Add outliers\n",
    "outlier_idx = np.random.choice(df.index, size=50, replace=False)\n",
    "df.loc[outlier_idx, 'income'] = df.loc[outlier_idx, 'income'] * 10\n",
    "df.loc[outlier_idx[:25], 'age'] = np.random.uniform(100, 120, 25)\n",
    "\n",
    "# Add duplicates\n",
    "duplicate_rows = df.sample(20)\n",
    "df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "# Add inconsistent data\n",
    "df.loc[df['age'] < 0, 'age'] = np.abs(df.loc[df['age'] < 0, 'age'])\n",
    "\n",
    "print(\"üìä Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(f\"\\nDuplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã First 5 Rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Data Quality Report\n",
    "print(\"üìä DATA QUALITY ASSESSMENT\\n\" + \"=\"*40)\n",
    "\n",
    "def data_quality_report(df):\n",
    "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "    \n",
    "    report = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Type': df.dtypes.values,\n",
    "        'Missing': df.isnull().sum().values,\n",
    "        'Missing%': (df.isnull().sum() / len(df) * 100).round(2).values,\n",
    "        'Unique': df.nunique().values,\n",
    "        'Unique%': (df.nunique() / len(df) * 100).round(2).values\n",
    "    })\n",
    "    \n",
    "    # Add statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    stats_dict = {\n",
    "        'Mean': [],\n",
    "        'Std': [],\n",
    "        'Min': [],\n",
    "        'Max': [],\n",
    "        'Outliers': []\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in numeric_cols:\n",
    "            stats_dict['Mean'].append(df[col].mean())\n",
    "            stats_dict['Std'].append(df[col].std())\n",
    "            stats_dict['Min'].append(df[col].min())\n",
    "            stats_dict['Max'].append(df[col].max())\n",
    "            \n",
    "            # Count outliers (using IQR method)\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            stats_dict['Outliers'].append(outliers)\n",
    "        else:\n",
    "            for key in stats_dict:\n",
    "                stats_dict[key].append('-')\n",
    "    \n",
    "    for key, values in stats_dict.items():\n",
    "        report[key] = values\n",
    "    \n",
    "    return report\n",
    "\n",
    "quality_report = data_quality_report(df)\n",
    "print(quality_report.to_string())\n",
    "\n",
    "# Visualize data quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Missing values heatmap\n",
    "ax = axes[0, 0]\n",
    "missing_data = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "ax.barh(range(len(missing_data)), missing_data.values)\n",
    "ax.set_yticks(range(len(missing_data)))\n",
    "ax.set_yticklabels(missing_data.index)\n",
    "ax.set_xlabel('Number of Missing Values')\n",
    "ax.set_title('Missing Values by Column')\n",
    "\n",
    "# Data types distribution\n",
    "ax = axes[0, 1]\n",
    "type_counts = df.dtypes.value_counts()\n",
    "ax.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "ax.set_title('Data Types Distribution')\n",
    "\n",
    "# Correlation heatmap (numeric only)\n",
    "ax = axes[1, 0]\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Correlation Matrix')\n",
    "\n",
    "# Distribution of a numeric column\n",
    "ax = axes[1, 1]\n",
    "ax.hist(df['income'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Income Distribution (with outliers)')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 2: Handling Missing Data\n",
    "\n",
    "### üéØ Smart Strategies for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Missing Data Patterns\n",
    "print(\"üîç MISSING DATA ANALYSIS\\n\" + \"=\"*40)\n",
    "\n",
    "# Visualize missing data patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Missing data matrix\n",
    "ax = axes[0]\n",
    "missing_matrix = df.isnull().astype(int)\n",
    "ax.imshow(missing_matrix.T, cmap='RdYlBu', aspect='auto', interpolation='none')\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Missing Data Pattern (Yellow = Missing)')\n",
    "ax.set_yticks(range(len(df.columns)))\n",
    "ax.set_yticklabels(df.columns, fontsize=8)\n",
    "\n",
    "# Missing data correlation\n",
    "ax = axes[1]\n",
    "missing_corr = missing_matrix.corr()\n",
    "mask = np.triu(np.ones_like(missing_corr), k=1)\n",
    "sns.heatmap(missing_corr, mask=mask, annot=False, cmap='coolwarm', \n",
    "            center=0, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Missing Data Correlation')\n",
    "\n",
    "# Missing data by column\n",
    "ax = axes[2]\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "ax.bar(range(len(missing_pct)), missing_pct.values, color='coral')\n",
    "ax.set_xticks(range(len(missing_pct)))\n",
    "ax.set_xticklabels(missing_pct.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Missing %')\n",
    "ax.set_title('Missing Data Percentage')\n",
    "ax.axhline(y=5, color='red', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "ax.axhline(y=20, color='darkred', linestyle='--', alpha=0.5, label='20% threshold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Missing data types\n",
    "print(\"\\nüìä Missing Data Types:\")\n",
    "print(\"‚Ä¢ MCAR (Missing Completely At Random): No pattern\")\n",
    "print(\"‚Ä¢ MAR (Missing At Random): Pattern related to other variables\")\n",
    "print(\"‚Ä¢ MNAR (Missing Not At Random): Pattern related to missing value itself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Imputation Strategies\n",
    "print(\"üîß IMPUTATION STRATEGIES\\n\" + \"=\"*40)\n",
    "\n",
    "# Create a copy for imputation\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# Strategy 1: Simple Imputation\n",
    "print(\"1Ô∏è‚É£ Simple Imputation:\")\n",
    "\n",
    "# Numeric columns - mean/median imputation\n",
    "numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_imputed[col].isnull().sum() > 0:\n",
    "        # Use median for skewed data, mean for normal\n",
    "        skewness = df_imputed[col].skew()\n",
    "        if abs(skewness) > 1:\n",
    "            impute_value = df_imputed[col].median()\n",
    "            strategy = 'median'\n",
    "        else:\n",
    "            impute_value = df_imputed[col].mean()\n",
    "            strategy = 'mean'\n",
    "        \n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        df_imputed[col].fillna(impute_value, inplace=True)\n",
    "        print(f\"  {col}: Imputed {missing_count} values with {strategy} = {impute_value:.2f}\")\n",
    "\n",
    "# Categorical columns - mode imputation\n",
    "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_imputed[col].isnull().sum() > 0:\n",
    "        mode_value = df_imputed[col].mode()[0] if not df_imputed[col].mode().empty else 'Unknown'\n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        df_imputed[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"  {col}: Imputed {missing_count} values with mode = {mode_value}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Advanced Imputation - KNN:\")\n",
    "\n",
    "# Demonstrate KNN imputation on numeric data\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Select numeric columns with missing values for demonstration\n",
    "demo_cols = ['age', 'income', 'credit_score']\n",
    "df_knn = df[demo_cols].copy()\n",
    "\n",
    "# Apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(df_knn),\n",
    "    columns=demo_cols,\n",
    "    index=df_knn.index\n",
    ")\n",
    "\n",
    "# Compare imputation methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, col in enumerate(demo_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Original distribution\n",
    "    ax.hist(df[col].dropna(), bins=30, alpha=0.5, label='Original', color='blue')\n",
    "    \n",
    "    # Simple imputation\n",
    "    ax.hist(df_imputed[col], bins=30, alpha=0.5, label='Simple Impute', color='green')\n",
    "    \n",
    "    # KNN imputation\n",
    "    ax.hist(df_knn_imputed[col], bins=30, alpha=0.5, label='KNN Impute', color='red')\n",
    "    \n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{col} - Imputation Comparison')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° When to use which strategy:\")\n",
    "print(\"‚Ä¢ Mean/Median: When data is MCAR, small % missing\")\n",
    "print(\"‚Ä¢ Mode: For categorical variables\")\n",
    "print(\"‚Ä¢ KNN: When data is MAR, preserves relationships\")\n",
    "print(\"‚Ä¢ Forward/Backward fill: For time series\")\n",
    "print(\"‚Ä¢ Domain-specific: Use business knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 3: Outlier Detection and Treatment\n",
    "\n",
    "### üéØ Finding and Handling Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Outlier Detection Methods\n",
    "print(\"üîç OUTLIER DETECTION\\n\" + \"=\"*40)\n",
    "\n",
    "def detect_outliers(df, column, method='IQR'):\n",
    "    \"\"\"Detect outliers using various methods\"\"\"\n",
    "    \n",
    "    if method == 'IQR':\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        \n",
    "    elif method == 'Z-score':\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        outliers = df[column].dropna()[z_scores > 3]\n",
    "        \n",
    "    elif method == 'Isolation Forest':\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        outliers_pred = iso_forest.fit_predict(df[[column]].dropna())\n",
    "        outliers = df[column].dropna()[outliers_pred == -1]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in income\n",
    "outliers_iqr = detect_outliers(df_imputed, 'income', 'IQR')\n",
    "outliers_zscore = detect_outliers(df_imputed, 'income', 'Z-score')\n",
    "\n",
    "print(f\"Outliers detected in 'income':\")\n",
    "print(f\"  IQR method: {len(outliers_iqr)} outliers\")\n",
    "print(f\"  Z-score method: {len(outliers_zscore)} outliers\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Box plot\n",
    "ax = axes[0, 0]\n",
    "ax.boxplot([df_imputed['income'], df_imputed['age'], df_imputed['credit_score']], \n",
    "           labels=['Income', 'Age', 'Credit Score'])\n",
    "ax.set_title('Box Plot - Outlier Detection')\n",
    "ax.set_ylabel('Value')\n",
    "\n",
    "# Scatter plot with outliers\n",
    "ax = axes[0, 1]\n",
    "is_outlier = df_imputed['income'].isin(outliers_iqr['income'])\n",
    "ax.scatter(df_imputed.index[~is_outlier], df_imputed.loc[~is_outlier, 'income'], \n",
    "          alpha=0.5, label='Normal')\n",
    "ax.scatter(df_imputed.index[is_outlier], df_imputed.loc[is_outlier, 'income'], \n",
    "          color='red', label='Outlier', s=50)\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Income Outliers (IQR Method)')\n",
    "ax.legend()\n",
    "\n",
    "# Distribution with outlier boundaries\n",
    "ax = axes[0, 2]\n",
    "ax.hist(df_imputed['income'], bins=50, alpha=0.7, edgecolor='black')\n",
    "Q1 = df_imputed['income'].quantile(0.25)\n",
    "Q3 = df_imputed['income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "ax.axvline(Q1 - 1.5*IQR, color='red', linestyle='--', label='Lower Bound')\n",
    "ax.axvline(Q3 + 1.5*IQR, color='red', linestyle='--', label='Upper Bound')\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Income Distribution with IQR Bounds')\n",
    "ax.legend()\n",
    "\n",
    "# Z-score visualization\n",
    "ax = axes[1, 0]\n",
    "z_scores = stats.zscore(df_imputed['income'])\n",
    "ax.scatter(df_imputed.index, z_scores, alpha=0.5)\n",
    "ax.axhline(y=3, color='red', linestyle='--', label='Z=3')\n",
    "ax.axhline(y=-3, color='red', linestyle='--', label='Z=-3')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Z-score')\n",
    "ax.set_title('Z-scores for Income')\n",
    "ax.legend()\n",
    "\n",
    "# Multivariate outlier detection\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(df_imputed['age'], df_imputed['income'], alpha=0.5)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Bivariate Outlier Detection')\n",
    "\n",
    "# Local Outlier Factor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "outlier_labels = lof.fit_predict(df_imputed[['age', 'income']])\n",
    "ax.scatter(df_imputed['age'][outlier_labels == 1], \n",
    "          df_imputed['income'][outlier_labels == 1], \n",
    "          alpha=0.5, label='Normal')\n",
    "ax.scatter(df_imputed['age'][outlier_labels == -1], \n",
    "          df_imputed['income'][outlier_labels == -1], \n",
    "          color='red', label='Outlier', s=50)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Local Outlier Factor')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Outlier Treatment Strategies\n",
    "print(\"üîß OUTLIER TREATMENT\\n\" + \"=\"*40)\n",
    "\n",
    "# Create copies for different treatment methods\n",
    "df_capped = df_imputed.copy()\n",
    "df_transformed = df_imputed.copy()\n",
    "df_removed = df_imputed.copy()\n",
    "\n",
    "# Method 1: Capping (Winsorization)\n",
    "print(\"1Ô∏è‚É£ Capping/Winsorization:\")\n",
    "for col in ['income', 'age']:\n",
    "    Q1 = df_capped[col].quantile(0.01)\n",
    "    Q99 = df_capped[col].quantile(0.99)\n",
    "    df_capped[col] = df_capped[col].clip(Q1, Q99)\n",
    "    print(f\"  {col}: Capped to [{Q1:.2f}, {Q99:.2f}]\")\n",
    "\n",
    "# Method 2: Log Transformation\n",
    "print(\"\\n2Ô∏è‚É£ Log Transformation:\")\n",
    "df_transformed['income_log'] = np.log1p(df_transformed['income'])\n",
    "print(f\"  Applied log transformation to income\")\n",
    "\n",
    "# Method 3: Removal\n",
    "print(\"\\n3Ô∏è‚É£ Outlier Removal:\")\n",
    "before_shape = df_removed.shape\n",
    "for col in ['income', 'age']:\n",
    "    Q1 = df_removed[col].quantile(0.25)\n",
    "    Q3 = df_removed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_removed = df_removed[(df_removed[col] >= Q1 - 1.5*IQR) & \n",
    "                            (df_removed[col] <= Q3 + 1.5*IQR)]\n",
    "after_shape = df_removed.shape\n",
    "print(f\"  Removed {before_shape[0] - after_shape[0]} rows\")\n",
    "\n",
    "# Visualize treatment effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original vs Capped\n",
    "ax = axes[0, 0]\n",
    "ax.hist(df_imputed['income'], bins=50, alpha=0.5, label='Original', color='blue')\n",
    "ax.hist(df_capped['income'], bins=50, alpha=0.5, label='Capped', color='green')\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Original vs Capped')\n",
    "ax.legend()\n",
    "\n",
    "# Log transformation\n",
    "ax = axes[0, 1]\n",
    "ax.hist(df_transformed['income_log'], bins=50, alpha=0.7, color='purple')\n",
    "ax.set_xlabel('Log(Income)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Log Transformed Income')\n",
    "\n",
    "# Before/After boxplots\n",
    "ax = axes[1, 0]\n",
    "ax.boxplot([df_imputed['income'], df_capped['income'], df_removed['income']], \n",
    "           labels=['Original', 'Capped', 'Removed'])\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Outlier Treatment Comparison')\n",
    "\n",
    "# Statistics comparison\n",
    "ax = axes[1, 1]\n",
    "stats_comparison = pd.DataFrame({\n",
    "    'Original': [df_imputed['income'].mean(), df_imputed['income'].std(), \n",
    "                df_imputed['income'].skew(), len(df_imputed)],\n",
    "    'Capped': [df_capped['income'].mean(), df_capped['income'].std(), \n",
    "              df_capped['income'].skew(), len(df_capped)],\n",
    "    'Removed': [df_removed['income'].mean(), df_removed['income'].std(), \n",
    "               df_removed['income'].skew(), len(df_removed)]\n",
    "}, index=['Mean', 'Std', 'Skew', 'Count'])\n",
    "\n",
    "stats_comparison.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Statistics After Treatment')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend(title='Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° When to use which method:\")\n",
    "print(\"‚Ä¢ Capping: Preserve all data, reduce extreme values\")\n",
    "print(\"‚Ä¢ Transformation: When distribution is skewed\")\n",
    "print(\"‚Ä¢ Removal: When outliers are errors or tiny %\")\n",
    "print(\"‚Ä¢ Keep: When outliers are valid and important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 4: Feature Scaling and Transformation\n",
    "\n",
    "### üéØ Preparing Features for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Feature Scaling Methods\n",
    "print(\"‚öñÔ∏è FEATURE SCALING\\n\" + \"=\"*40)\n",
    "\n",
    "# Select numeric features\n",
    "features = ['age', 'income', 'credit_score', 'purchase_amount', 'membership_days']\n",
    "X = df_capped[features].copy()\n",
    "\n",
    "# Different scaling methods\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "\n",
    "scalers = {\n",
    "    'Original': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson')\n",
    "}\n",
    "\n",
    "# Apply scaling\n",
    "scaled_data = {}\n",
    "for name, scaler in scalers.items():\n",
    "    if scaler is None:\n",
    "        scaled_data[name] = X\n",
    "    else:\n",
    "        scaled_data[name] = pd.DataFrame(\n",
    "            scaler.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(len(features), len(scalers), figsize=(20, 15))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    for j, (name, data) in enumerate(scaled_data.items()):\n",
    "        ax = axes[i, j]\n",
    "        ax.hist(data[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f'{feature}\\n{name}')\n",
    "        \n",
    "        # Add statistics\n",
    "        mean = data[feature].mean()\n",
    "        std = data[feature].std()\n",
    "        ax.axvline(mean, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.text(0.7, 0.9, f'Œº={mean:.2f}\\nœÉ={std:.2f}', \n",
    "               transform=ax.transAxes, fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Scaling Comparison', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Scaling Method Comparison:\")\n",
    "print(\"‚Ä¢ StandardScaler: Mean=0, Std=1 (assumes normal distribution)\")\n",
    "print(\"‚Ä¢ MinMaxScaler: Range [0,1] (preserves original distribution)\")\n",
    "print(\"‚Ä¢ RobustScaler: Uses median/IQR (robust to outliers)\")\n",
    "print(\"‚Ä¢ PowerTransformer: Makes data more Gaussian-like\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 5: Feature Engineering\n",
    "\n",
    "### üéØ Creating Powerful Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Feature Engineering Techniques\n",
    "print(\"üõ†Ô∏è FEATURE ENGINEERING\\n\" + \"=\"*40)\n",
    "\n",
    "# Create new features\n",
    "df_engineered = df_capped.copy()\n",
    "\n",
    "# Temporal features\n",
    "df_engineered['account_age_years'] = df_engineered['membership_days'] / 365\n",
    "df_engineered['account_age_category'] = pd.cut(df_engineered['account_age_years'], \n",
    "                                                bins=[0, 1, 2, 5, 10], \n",
    "                                                labels=['New', 'Regular', 'Loyal', 'VIP'])\n",
    "\n",
    "# Ratio features\n",
    "df_engineered['income_per_age'] = df_engineered['income'] / df_engineered['age']\n",
    "df_engineered['purchase_to_income_ratio'] = df_engineered['purchase_amount'] / df_engineered['income']\n",
    "df_engineered['credit_score_category'] = pd.cut(df_engineered['credit_score'],\n",
    "                                                 bins=[300, 580, 670, 740, 800, 850],\n",
    "                                                 labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
    "\n",
    "# Interaction features\n",
    "df_engineered['high_income_good_credit'] = (\n",
    "    (df_engineered['income'] > df_engineered['income'].median()) & \n",
    "    (df_engineered['credit_score'] > 700)\n",
    ").astype(int)\n",
    "\n",
    "# Polynomial features\n",
    "df_engineered['age_squared'] = df_engineered['age'] ** 2\n",
    "df_engineered['income_log'] = np.log1p(df_engineered['income'])\n",
    "\n",
    "# Binning continuous variables\n",
    "df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
    "                                    bins=[0, 25, 35, 50, 65, 100],\n",
    "                                    labels=['<25', '25-35', '35-50', '50-65', '65+'])\n",
    "\n",
    "print(\"üìä New Features Created:\")\n",
    "new_features = ['account_age_years', 'account_age_category', 'income_per_age', \n",
    "                'purchase_to_income_ratio', 'credit_score_category', \n",
    "                'high_income_good_credit', 'age_squared', 'income_log', 'age_group']\n",
    "for feat in new_features:\n",
    "    print(f\"  ‚Ä¢ {feat}\")\n",
    "\n",
    "# Visualize engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Age groups distribution\n",
    "ax = axes[0, 0]\n",
    "df_engineered['age_group'].value_counts().plot(kind='bar', ax=ax)\n",
    "ax.set_title('Age Group Distribution')\n",
    "ax.set_xlabel('Age Group')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Credit score categories\n",
    "ax = axes[0, 1]\n",
    "df_engineered['credit_score_category'].value_counts().plot(kind='pie', ax=ax, autopct='%1.1f%%')\n",
    "ax.set_title('Credit Score Categories')\n",
    "\n",
    "# Income per age\n",
    "ax = axes[0, 2]\n",
    "ax.scatter(df_engineered['age'], df_engineered['income_per_age'], alpha=0.5)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income per Age')\n",
    "ax.set_title('Income/Age Ratio')\n",
    "\n",
    "# Purchase to income ratio\n",
    "ax = axes[1, 0]\n",
    "ax.hist(df_engineered['purchase_to_income_ratio'], bins=30, edgecolor='black')\n",
    "ax.set_xlabel('Purchase/Income Ratio')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Purchase to Income Ratio')\n",
    "\n",
    "# Account age categories\n",
    "ax = axes[1, 1]\n",
    "df_engineered['account_age_category'].value_counts().plot(kind='bar', ax=ax)\n",
    "ax.set_title('Customer Loyalty Categories')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Feature correlation heatmap\n",
    "ax = axes[1, 2]\n",
    "numeric_features = df_engineered[['age', 'income', 'credit_score', \n",
    "                                  'income_per_age', 'purchase_to_income_ratio']].corr()\n",
    "sns.heatmap(numeric_features, annot=True, fmt='.2f', cmap='coolwarm', ax=ax)\n",
    "ax.set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
