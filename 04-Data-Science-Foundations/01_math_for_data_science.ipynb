{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Math for Data Science: Essential Foundations\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*L76A5gL6176UbMgn7q4Ybg.jpeg' width='600' alt='Math for DS'>\n",
    "\n",
    "## üéØ Why Math Matters in Data Science\n",
    "\n",
    "**Don't worry!** You don't need to be a math genius. We'll learn just enough math to understand:\n",
    "- How machine learning algorithms work\n",
    "- Why certain techniques are used\n",
    "- How to optimize models\n",
    "- What's happening \"under the hood\"\n",
    "\n",
    "### üìö What We'll Master Today:\n",
    "1. **Linear Algebra Basics** - Vectors, matrices, operations\n",
    "2. **Essential Calculus** - Derivatives for optimization\n",
    "3. **Key Concepts** - Distance metrics, dimensionality\n",
    "4. **Practical Applications** - See math in action\n",
    "5. **ML Connections** - Where each concept is used\n",
    "\n",
    "### üéì Learning Approach:\n",
    "- **Visual First** - See it, then understand it\n",
    "- **Code Everything** - NumPy makes math tangible\n",
    "- **Real Examples** - Not abstract theory\n",
    "- **Build Intuition** - Understanding > memorization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Make Math Fun and Practical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üìê Math for Data Science - Ready to Learn!\")\n",
    "print(\"\\nüí° Remember: We're learning practical math, not theory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 1: Vectors - The Building Blocks\n",
    "\n",
    "### üéØ What is a Vector?\n",
    "\n",
    "Think of a vector as:\n",
    "- **In ML**: A list of features for one data point\n",
    "- **In Math**: An arrow with direction and magnitude\n",
    "- **In Code**: A 1D NumPy array\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*AdhTIAfILudMOc0kfHHLkw.png' width='400' alt='Vector'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Creating and Understanding Vectors\n",
    "print(\"üéØ VECTORS IN DATA SCIENCE\\n\" + \"=\"*40)\n",
    "\n",
    "# Example: House features as a vector\n",
    "# [bedrooms, bathrooms, size_sqft, age_years, price_1000s]\n",
    "house_vector = np.array([3, 2, 1500, 10, 250])\n",
    "\n",
    "print(\"House Features Vector:\")\n",
    "print(f\"Vector: {house_vector}\")\n",
    "print(f\"Dimensions: {house_vector.shape[0]}\")\n",
    "print(f\"Type: {type(house_vector)}\")\n",
    "\n",
    "# Visualize 2D vector\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 2D Vector visualization\n",
    "vector_2d = np.array([3, 4])\n",
    "ax1.quiver(0, 0, vector_2d[0], vector_2d[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.01)\n",
    "ax1.set_xlim(-1, 6)\n",
    "ax1.set_ylim(-1, 6)\n",
    "ax1.grid(True)\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_title(f'2D Vector: {vector_2d}')\n",
    "ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Multiple vectors (dataset)\n",
    "np.random.seed(42)\n",
    "dataset = np.random.randn(50, 2) * 2 + 3\n",
    "ax2.scatter(dataset[:, 0], dataset[:, 1], alpha=0.6)\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.set_title('Dataset: Each Point is a Vector')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Vector Operations\n",
    "print(\"‚ûï VECTOR OPERATIONS\\n\" + \"=\"*40)\n",
    "\n",
    "# Define two vectors\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"Vector 1: {v1}\")\n",
    "print(f\"Vector 2: {v2}\")\n",
    "print()\n",
    "\n",
    "# Basic operations\n",
    "print(\"üìä Basic Operations:\")\n",
    "print(f\"Addition: v1 + v2 = {v1 + v2}\")\n",
    "print(f\"Subtraction: v1 - v2 = {v1 - v2}\")\n",
    "print(f\"Scalar multiplication: 3 * v1 = {3 * v1}\")\n",
    "print(f\"Element-wise multiplication: v1 * v2 = {v1 * v2}\")\n",
    "print()\n",
    "\n",
    "# Important vector operations\n",
    "print(\"üéØ Key Operations for ML:\")\n",
    "print(f\"Dot product: v1 ¬∑ v2 = {np.dot(v1, v2)}\")\n",
    "print(f\"Magnitude (norm) of v1: ||v1|| = {np.linalg.norm(v1):.3f}\")\n",
    "print(f\"Unit vector of v1: {v1 / np.linalg.norm(v1)}\")\n",
    "print()\n",
    "\n",
    "# Angle between vectors\n",
    "cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "angle_rad = np.arccos(cos_angle)\n",
    "angle_deg = np.degrees(angle_rad)\n",
    "print(f\"Angle between v1 and v2: {angle_deg:.2f}¬∞\")\n",
    "\n",
    "# Practical example: Cosine similarity (used in recommendation systems)\n",
    "print(\"\\nüìà Practical Application: Cosine Similarity\")\n",
    "user1_ratings = np.array([5, 3, 0, 4, 4])  # Movie ratings\n",
    "user2_ratings = np.array([4, 0, 0, 5, 4])\n",
    "\n",
    "cos_sim = np.dot(user1_ratings, user2_ratings) / (np.linalg.norm(user1_ratings) * np.linalg.norm(user2_ratings))\n",
    "print(f\"User 1 ratings: {user1_ratings}\")\n",
    "print(f\"User 2 ratings: {user2_ratings}\")\n",
    "print(f\"Cosine similarity: {cos_sim:.3f}\")\n",
    "print(f\"Interpretation: Users are {cos_sim*100:.1f}% similar in taste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Exercise 1: Vector Practice\n",
    "\n",
    "Calculate:\n",
    "1. The distance between two points\n",
    "2. The dot product interpretation\n",
    "3. Normalize a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "\n",
    "# Solution:\n",
    "print(\"üìù VECTOR EXERCISES\\n\" + \"=\"*40)\n",
    "\n",
    "# 1. Euclidean distance\n",
    "point1 = np.array([1, 2, 3])\n",
    "point2 = np.array([4, 6, 8])\n",
    "distance = np.linalg.norm(point2 - point1)\n",
    "print(f\"1. Distance between {point1} and {point2}: {distance:.3f}\")\n",
    "\n",
    "# 2. Dot product as projection\n",
    "v_a = np.array([3, 4])\n",
    "v_b = np.array([1, 0])  # Unit vector along x-axis\n",
    "projection = np.dot(v_a, v_b)\n",
    "print(f\"\\n2. Projection of {v_a} onto x-axis: {projection}\")\n",
    "print(f\"   (This is the x-component of the vector!)\")\n",
    "\n",
    "# 3. Normalize vector\n",
    "vector = np.array([3, 4, 0])\n",
    "normalized = vector / np.linalg.norm(vector)\n",
    "print(f\"\\n3. Original vector: {vector}\")\n",
    "print(f\"   Normalized: {normalized}\")\n",
    "print(f\"   New magnitude: {np.linalg.norm(normalized):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 2: Matrices - The Data Tables\n",
    "\n",
    "### üéØ What is a Matrix?\n",
    "\n",
    "- **In ML**: Your entire dataset (rows = samples, columns = features)\n",
    "- **In Math**: A 2D array of numbers\n",
    "- **In Code**: A 2D NumPy array or DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Creating and Understanding Matrices\n",
    "print(\"üìä MATRICES IN DATA SCIENCE\\n\" + \"=\"*40)\n",
    "\n",
    "# Dataset as a matrix\n",
    "# Each row = one house, Each column = one feature\n",
    "houses_matrix = np.array([\n",
    "    [3, 2, 1500, 10, 250],  # House 1\n",
    "    [4, 3, 2000, 5, 350],   # House 2\n",
    "    [2, 1, 900, 20, 150],   # House 3\n",
    "    [5, 4, 3000, 2, 500]    # House 4\n",
    "])\n",
    "\n",
    "features = ['Bedrooms', 'Bathrooms', 'SqFt', 'Age', 'Price($1000)']\n",
    "\n",
    "print(\"Housing Dataset Matrix:\")\n",
    "print(pd.DataFrame(houses_matrix, columns=features))\n",
    "print(f\"\\nShape: {houses_matrix.shape} (samples √ó features)\")\n",
    "\n",
    "# Visualize matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Heatmap of matrix\n",
    "sns.heatmap(houses_matrix, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "            xticklabels=features, yticklabels=[f'House {i+1}' for i in range(4)],\n",
    "            ax=ax1)\n",
    "ax1.set_title('Dataset as Matrix Heatmap')\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = np.corrcoef(houses_matrix.T)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            xticklabels=features, yticklabels=features,\n",
    "            center=0, ax=ax2)\n",
    "ax2.set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Matrix Operations\n",
    "print(\"‚úñÔ∏è MATRIX OPERATIONS\\n\" + \"=\"*40)\n",
    "\n",
    "# Define matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "v = np.array([1, 2])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "print(f\"\\nVector v: {v}\")\n",
    "print()\n",
    "\n",
    "# Basic operations\n",
    "print(\"üìä Basic Matrix Operations:\")\n",
    "print(\"A + B:\")\n",
    "print(A + B)\n",
    "print(\"\\n2 * A:\")\n",
    "print(2 * A)\n",
    "print(\"\\nElement-wise multiplication (A * B):\")\n",
    "print(A * B)\n",
    "\n",
    "# Matrix multiplication\n",
    "print(\"\\nüéØ Matrix Multiplication (A @ B):\")\n",
    "print(A @ B)\n",
    "print(\"\\nüìù How it works:\")\n",
    "print(f\"Result[0,0] = (1√ó5 + 2√ó7) = {1*5 + 2*7}\")\n",
    "print(f\"Result[0,1] = (1√ó6 + 2√ó8) = {1*6 + 2*8}\")\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "print(\"\\nüîÑ Matrix-Vector Multiplication (A @ v):\")\n",
    "result = A @ v\n",
    "print(f\"Result: {result}\")\n",
    "print(\"This transforms the vector v using matrix A!\")\n",
    "\n",
    "# Transpose\n",
    "print(\"\\nüîÑ Transpose (A.T):\")\n",
    "print(\"Original A:\")\n",
    "print(A)\n",
    "print(\"Transposed A:\")\n",
    "print(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Special Matrices\n",
    "print(\"üåü SPECIAL MATRICES\\n\" + \"=\"*40)\n",
    "\n",
    "# Identity matrix\n",
    "I = np.eye(3)\n",
    "print(\"Identity Matrix (like the number 1 for matrices):\")\n",
    "print(I)\n",
    "print(\"Property: A @ I = A\")\n",
    "\n",
    "# Diagonal matrix\n",
    "D = np.diag([2, 3, 4])\n",
    "print(\"\\nDiagonal Matrix:\")\n",
    "print(D)\n",
    "\n",
    "# Symmetric matrix\n",
    "S = np.array([[1, 2, 3],\n",
    "              [2, 4, 5],\n",
    "              [3, 5, 6]])\n",
    "print(\"\\nSymmetric Matrix (S = S.T):\")\n",
    "print(S)\n",
    "print(f\"Is symmetric? {np.allclose(S, S.T)}\")\n",
    "\n",
    "# Practical application: Covariance matrix is always symmetric\n",
    "data = np.random.randn(100, 3)\n",
    "cov_matrix = np.cov(data.T)\n",
    "print(\"\\nüìä Covariance Matrix (always symmetric):\")\n",
    "print(cov_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 3: Eigenvalues & Eigenvectors - The Hidden Structure\n",
    "\n",
    "### üéØ Why They Matter:\n",
    "\n",
    "- **PCA** uses them for dimensionality reduction\n",
    "- **PageRank** uses them to rank web pages\n",
    "- **Recommendation systems** use them for matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Understanding Eigenvalues & Eigenvectors\n",
    "print(\"üîÆ EIGENVALUES & EIGENVECTORS\\n\" + \"=\"*40)\n",
    "\n",
    "# Simple example\n",
    "A = np.array([[3, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"\\nEigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Verify the eigenvalue equation: A @ v = Œª @ v\n",
    "for i in range(len(eigenvalues)):\n",
    "    eigval = eigenvalues[i]\n",
    "    eigvec = eigenvectors[:, i]\n",
    "    \n",
    "    # Check if A @ v = Œª @ v\n",
    "    left = A @ eigvec\n",
    "    right = eigval * eigvec\n",
    "    \n",
    "    print(f\"\\n‚úÖ Verification for eigenvalue {eigval:.2f}:\")\n",
    "    print(f\"A @ v = {left}\")\n",
    "    print(f\"Œª * v = {right}\")\n",
    "    print(f\"Equal? {np.allclose(left, right)}\")\n",
    "\n",
    "# Visualize eigenvectors\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot eigenvectors\n",
    "for i in range(len(eigenvalues)):\n",
    "    eigvec = eigenvectors[:, i]\n",
    "    eigval = eigenvalues[i]\n",
    "    \n",
    "    # Original eigenvector\n",
    "    ax.quiver(0, 0, eigvec[0], eigvec[1], angles='xy', scale_units='xy', \n",
    "              scale=1, color=f'C{i}', width=0.01, label=f'Eigenvector {i+1}')\n",
    "    \n",
    "    # Transformed by matrix (scaled by eigenvalue)\n",
    "    transformed = A @ eigvec\n",
    "    ax.quiver(0, 0, transformed[0], transformed[1], angles='xy', scale_units='xy',\n",
    "              scale=1, color=f'C{i}', width=0.01, alpha=0.5, linestyle='--',\n",
    "              label=f'A @ eigenvector {i+1}')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.grid(True)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Eigenvectors: Special Directions That Only Scale')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 PCA Application\n",
    "print(\"üéØ PCA USING EIGENVALUES\\n\" + \"=\"*40)\n",
    "\n",
    "# Create correlated 2D data\n",
    "np.random.seed(42)\n",
    "mean = [5, 5]\n",
    "cov = [[2, 1.5],\n",
    "       [1.5, 1]]\n",
    "data = np.random.multivariate_normal(mean, cov, 200)\n",
    "\n",
    "# Center the data\n",
    "data_centered = data - np.mean(data, axis=0)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = np.cov(data_centered.T)\n",
    "\n",
    "# Get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalue (largest first)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"Eigenvalues (variance explained): {eigenvalues}\")\n",
    "print(f\"\\nVariance explained:\")\n",
    "total_var = np.sum(eigenvalues)\n",
    "for i, eigval in enumerate(eigenvalues):\n",
    "    print(f\"PC{i+1}: {eigval/total_var*100:.1f}%\")\n",
    "\n",
    "# Visualize PCA\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original data with principal components\n",
    "ax1.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
    "mean = np.mean(data, axis=0)\n",
    "\n",
    "# Draw principal components\n",
    "for i in range(2):\n",
    "    eigvec = eigenvectors[:, i]\n",
    "    eigval = eigenvalues[i]\n",
    "    \n",
    "    # Scale eigenvector by sqrt of eigenvalue\n",
    "    pc = eigvec * np.sqrt(eigval) * 2\n",
    "    \n",
    "    ax1.arrow(mean[0], mean[1], pc[0], pc[1], \n",
    "              head_width=0.2, head_length=0.1, \n",
    "              fc=f'C{i}', ec=f'C{i}', linewidth=2,\n",
    "              label=f'PC{i+1}')\n",
    "\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Original Data with Principal Components')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Transform data to PC space\n",
    "data_pca = data_centered @ eigenvectors\n",
    "ax2.scatter(data_pca[:, 0], data_pca[:, 1], alpha=0.5)\n",
    "ax2.set_xlabel('First Principal Component')\n",
    "ax2.set_ylabel('Second Principal Component')\n",
    "ax2.set_title('Data in PCA Space')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 4: Calculus Basics - Understanding Change\n",
    "\n",
    "### üéØ Why Calculus in ML?\n",
    "\n",
    "- **Gradient Descent** - How models learn\n",
    "- **Backpropagation** - How neural networks train\n",
    "- **Optimization** - Finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Derivatives - Rate of Change\n",
    "print(\"üìà DERIVATIVES INTUITION\\n\" + \"=\"*40)\n",
    "\n",
    "# Function: f(x) = x^2\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Derivative: f'(x) = 2x\n",
    "def f_derivative(x):\n",
    "    return 2*x\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = f(x)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot function\n",
    "ax1.plot(x, y, 'b-', linewidth=2, label='f(x) = x¬≤')\n",
    "\n",
    "# Show tangent lines at different points\n",
    "points = [-2, 0, 2]\n",
    "colors = ['red', 'green', 'orange']\n",
    "\n",
    "for point, color in zip(points, colors):\n",
    "    # Calculate tangent line\n",
    "    slope = f_derivative(point)\n",
    "    y_point = f(point)\n",
    "    \n",
    "    # Tangent line equation: y - y0 = m(x - x0)\n",
    "    x_tangent = np.linspace(point-1, point+1, 50)\n",
    "    y_tangent = slope * (x_tangent - point) + y_point\n",
    "    \n",
    "    ax1.plot(x_tangent, y_tangent, color=color, linestyle='--', alpha=0.7,\n",
    "            label=f'Tangent at x={point}, slope={slope}')\n",
    "    ax1.plot(point, y_point, 'o', color=color, markersize=8)\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.set_title('Function and Tangent Lines')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot derivative\n",
    "y_derivative = f_derivative(x)\n",
    "ax2.plot(x, y_derivative, 'r-', linewidth=2, label=\"f'(x) = 2x\")\n",
    "ax2.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel(\"f'(x)\")\n",
    "ax2.set_title('Derivative (Slope at Each Point)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"‚Ä¢ Derivative tells us the slope at any point\")\n",
    "print(\"‚Ä¢ Positive derivative = function increasing\")\n",
    "print(\"‚Ä¢ Negative derivative = function decreasing\")\n",
    "print(\"‚Ä¢ Zero derivative = minimum or maximum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Gradient Descent Visualization\n",
    "print(\"üéØ GRADIENT DESCENT IN ACTION\\n\" + \"=\"*40)\n",
    "\n",
    "# Cost function: J(Œ∏) = (Œ∏ - 3)^2\n",
    "def cost_function(theta):\n",
    "    return (theta - 3)**2\n",
    "\n",
    "def gradient(theta):\n",
    "    return 2 * (theta - 3)\n",
    "\n",
    "# Gradient descent\n",
    "learning_rate = 0.1\n",
    "theta = 0  # Starting point\n",
    "history = [theta]\n",
    "costs = [cost_function(theta)]\n",
    "\n",
    "# Run gradient descent\n",
    "for i in range(20):\n",
    "    grad = gradient(theta)\n",
    "    theta = theta - learning_rate * grad\n",
    "    history.append(theta)\n",
    "    costs.append(cost_function(theta))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Cost function and path\n",
    "theta_range = np.linspace(-1, 6, 100)\n",
    "cost_range = cost_function(theta_range)\n",
    "\n",
    "ax1.plot(theta_range, cost_range, 'b-', linewidth=2, label='Cost Function')\n",
    "ax1.plot(history, [cost_function(h) for h in history], 'ro-', \n",
    "         markersize=5, linewidth=1, alpha=0.6, label='Gradient Descent Path')\n",
    "ax1.plot(3, 0, 'g*', markersize=15, label='Minimum (Œ∏=3)')\n",
    "ax1.set_xlabel('Œ∏ (Parameter)')\n",
    "ax1.set_ylabel('Cost J(Œ∏)')\n",
    "ax1.set_title('Gradient Descent Finding Minimum')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Convergence\n",
    "ax2.plot(costs, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Cost')\n",
    "ax2.set_title('Cost Decreasing Over Iterations')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Starting Œ∏: {history[0]:.3f}\")\n",
    "print(f\"Final Œ∏: {history[-1]:.3f}\")\n",
    "print(f\"True minimum: Œ∏ = 3\")\n",
    "print(f\"Final cost: {costs[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Partial Derivatives - Multiple Variables\n",
    "print(\"üîÑ PARTIAL DERIVATIVES\\n\" + \"=\"*40)\n",
    "\n",
    "# Function: f(x,y) = x^2 + y^2\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Partial derivatives\n",
    "def df_dx(x, y):\n",
    "    return 2*x\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 2*y\n",
    "\n",
    "# Create mesh\n",
    "x = np.linspace(-3, 3, 50)\n",
    "y = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('f(x,y) = x¬≤ + y¬≤')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_title('Contour Plot')\n",
    "\n",
    "# Gradient field\n",
    "ax3 = fig.add_subplot(133)\n",
    "x_sparse = np.linspace(-3, 3, 10)\n",
    "y_sparse = np.linspace(-3, 3, 10)\n",
    "X_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\n",
    "U = -df_dx(X_sparse, Y_sparse)  # Negative for descent\n",
    "V = -df_dy(X_sparse, Y_sparse)\n",
    "\n",
    "ax3.quiver(X_sparse, Y_sparse, U, V, alpha=0.6)\n",
    "ax3.contour(X, Y, Z, levels=20, alpha=0.3)\n",
    "ax3.set_xlabel('X')\n",
    "ax3.set_ylabel('Y')\n",
    "ax3.set_title('Gradient Field (Arrows Point Downhill)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"The gradient points in the direction of steepest increase.\")\n",
    "print(\"For gradient descent, we move in the opposite direction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 5: Distance Metrics - Measuring Similarity\n",
    "\n",
    "### üéØ Essential for:\n",
    "- **KNN** - Finding nearest neighbors\n",
    "- **Clustering** - Grouping similar points\n",
    "- **Anomaly Detection** - Finding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Different Distance Metrics\n",
    "print(\"üìè DISTANCE METRICS\\n\" + \"=\"*40)\n",
    "\n",
    "# Two points\n",
    "p1 = np.array([1, 2, 3])\n",
    "p2 = np.array([4, 6, 8])\n",
    "\n",
    "# Euclidean distance (L2 norm)\n",
    "euclidean = np.linalg.norm(p2 - p1)\n",
    "print(f\"Points: p1 = {p1}, p2 = {p2}\")\n",
    "print(f\"\\n1. Euclidean Distance: {euclidean:.3f}\")\n",
    "print(f\"   Formula: ‚àö[(x‚ÇÇ-x‚ÇÅ)¬≤ + (y‚ÇÇ-y‚ÇÅ)¬≤ + (z‚ÇÇ-z‚ÇÅ)¬≤]\")\n",
    "\n",
    "# Manhattan distance (L1 norm)\n",
    "manhattan = np.sum(np.abs(p2 - p1))\n",
    "print(f\"\\n2. Manhattan Distance: {manhattan:.3f}\")\n",
    "print(f\"   Formula: |x‚ÇÇ-x‚ÇÅ| + |y‚ÇÇ-y‚ÇÅ| + |z‚ÇÇ-z‚ÇÅ|\")\n",
    "\n",
    "# Chebyshev distance (L‚àû norm)\n",
    "chebyshev = np.max(np.abs(p2 - p1))\n",
    "print(f\"\\n3. Chebyshev Distance: {chebyshev:.3f}\")\n",
    "print(f\"   Formula: max(|x‚ÇÇ-x‚ÇÅ|, |y‚ÇÇ-y‚ÇÅ|, |z‚ÇÇ-z‚ÇÅ|)\")\n",
    "\n",
    "# Cosine distance\n",
    "cosine_sim = np.dot(p1, p2) / (np.linalg.norm(p1) * np.linalg.norm(p2))\n",
    "cosine_dist = 1 - cosine_sim\n",
    "print(f\"\\n4. Cosine Distance: {cosine_dist:.3f}\")\n",
    "print(f\"   Measures angle between vectors, not magnitude\")\n",
    "\n",
    "# Visualize in 2D\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Points for visualization\n",
    "origin = np.array([0, 0])\n",
    "point = np.array([3, 4])\n",
    "\n",
    "for ax, title, dist_type in zip(axes, \n",
    "                                ['Euclidean', 'Manhattan', 'Chebyshev'],\n",
    "                                ['euclidean', 'manhattan', 'chebyshev']):\n",
    "    \n",
    "    # Draw grid\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-1, 5)\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Plot points\n",
    "    ax.plot(0, 0, 'ro', markersize=10, label='Origin')\n",
    "    ax.plot(3, 4, 'bo', markersize=10, label='Point')\n",
    "    \n",
    "    if dist_type == 'euclidean':\n",
    "        # Straight line\n",
    "        ax.plot([0, 3], [0, 4], 'g-', linewidth=2, label=f'Distance: {5:.1f}')\n",
    "        \n",
    "    elif dist_type == 'manhattan':\n",
    "        # L-shaped path\n",
    "        ax.plot([0, 3, 3], [0, 0, 4], 'g-', linewidth=2, label=f'Distance: {7:.1f}')\n",
    "        \n",
    "    elif dist_type == 'chebyshev':\n",
    "        # Show max dimension\n",
    "        ax.plot([0, 3], [0, 0], 'r--', alpha=0.5)\n",
    "        ax.plot([3, 3], [0, 4], 'g-', linewidth=3, label=f'Distance: {4:.1f}')\n",
    "    \n",
    "    ax.set_title(f'{title} Distance')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 KNN Example with Distance Metrics\n",
    "print(\"üéØ K-NEAREST NEIGHBORS EXAMPLE\\n\" + \"=\"*40)\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 30\n",
    "\n",
    "# Class 0: bottom-left\n",
    "class0 = np.random.randn(n_samples//2, 2) + [2, 2]\n",
    "# Class 1: top-right\n",
    "class1 = np.random.randn(n_samples//2, 2) + [6, 6]\n",
    "\n",
    "X = np.vstack([class0, class1])\n",
    "y = np.array([0] * (n_samples//2) + [1] * (n_samples//2))\n",
    "\n",
    "# New point to classify\n",
    "new_point = np.array([4, 4])\n",
    "\n",
    "# Calculate distances\n",
    "distances = np.linalg.norm(X - new_point, axis=1)\n",
    "\n",
    "# Find k nearest neighbors\n",
    "k = 5\n",
    "nearest_indices = np.argsort(distances)[:k]\n",
    "nearest_classes = y[nearest_indices]\n",
    "\n",
    "# Predict class (majority vote)\n",
    "prediction = int(np.mean(nearest_classes) > 0.5)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot training data\n",
    "colors = ['blue', 'red']\n",
    "for i in range(2):\n",
    "    mask = y == i\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[i], \n",
    "               label=f'Class {i}', alpha=0.6, s=50)\n",
    "\n",
    "# Plot new point\n",
    "plt.scatter(new_point[0], new_point[1], c='green', \n",
    "           marker='*', s=500, label='New Point', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Highlight nearest neighbors\n",
    "plt.scatter(X[nearest_indices, 0], X[nearest_indices, 1], \n",
    "           facecolors='none', edgecolors='green', s=200, linewidth=2,\n",
    "           label=f'K={k} Nearest')\n",
    "\n",
    "# Draw connections to nearest neighbors\n",
    "for idx in nearest_indices:\n",
    "    plt.plot([new_point[0], X[idx, 0]], [new_point[1], X[idx, 1]], \n",
    "            'g--', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(f'KNN Classification (K={k})\\nPrediction: Class {prediction}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Nearest {k} neighbors belong to classes: {nearest_classes}\")\n",
    "print(f\"Prediction for new point: Class {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 6: Dimensionality - The Curse and The Blessing\n",
    "\n",
    "### üéØ Understanding High-Dimensional Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 The Curse of Dimensionality\n",
    "print(\"üìä CURSE OF DIMENSIONALITY\\n\" + \"=\"*40)\n",
    "\n",
    "# Volume of hypersphere vs hypercube\n",
    "dimensions = range(1, 21)\n",
    "volume_ratios = []\n",
    "\n",
    "for d in dimensions:\n",
    "    # Volume of unit hypersphere / Volume of unit hypercube\n",
    "    if d == 1:\n",
    "        ratio = 2  # Line segment\n",
    "    elif d == 2:\n",
    "        ratio = np.pi / 4  # Circle/Square\n",
    "    elif d == 3:\n",
    "        ratio = (4/3 * np.pi) / 8  # Sphere/Cube\n",
    "    else:\n",
    "        # General formula\n",
    "        ratio = (np.pi ** (d/2)) / (2**d * np.math.gamma(d/2 + 1))\n",
    "    \n",
    "    volume_ratios.append(ratio)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Volume ratio\n",
    "ax1.plot(dimensions, volume_ratios, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Dimensions')\n",
    "ax1.set_ylabel('Volume Ratio (Sphere/Cube)')\n",
    "ax1.set_title('Sphere Volume Vanishes in High Dimensions')\n",
    "ax1.grid(True)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Distance concentration\n",
    "np.random.seed(42)\n",
    "dims = [2, 5, 10, 20, 50, 100]\n",
    "distance_stds = []\n",
    "\n",
    "for d in dims:\n",
    "    # Generate random points\n",
    "    points = np.random.randn(100, d)\n",
    "    \n",
    "    # Calculate all pairwise distances\n",
    "    distances = []\n",
    "    for i in range(len(points)):\n",
    "        for j in range(i+1, len(points)):\n",
    "            dist = np.linalg.norm(points[i] - points[j])\n",
    "            distances.append(dist)\n",
    "    \n",
    "    # Calculate coefficient of variation\n",
    "    cv = np.std(distances) / np.mean(distances)\n",
    "    distance_stds.append(cv)\n",
    "\n",
    "ax2.plot(dims, distance_stds, 'r-', linewidth=2, marker='o')\n",
    "ax2.set_xlabel('Dimensions')\n",
    "ax2.set_ylabel('Distance Variation (CV)')\n",
    "ax2.set_title('Distances Become Similar in High Dimensions')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"1. In high dimensions, most of the volume is near the surface\")\n",
    "print(\"2. All points become approximately equidistant\")\n",
    "print(\"3. This makes distance-based algorithms challenging\")\n",
    "print(\"4. Solution: Dimensionality reduction (PCA, t-SNE, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Section 7: Practical ML Applications\n",
    "\n",
    "### Connecting Math to Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Linear Regression from Scratch\n",
    "print(\"üìà LINEAR REGRESSION USING LINEAR ALGEBRA\\n\" + \"=\"*40)\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2 + 5\n",
    "true_slope = 3\n",
    "true_intercept = 10\n",
    "y = true_slope * X + true_intercept + np.random.randn(n_samples, 1) * 2\n",
    "\n",
    "# Add intercept term to X\n",
    "X_with_intercept = np.hstack([np.ones((n_samples, 1)), X])\n",
    "\n",
    "# Normal equation: Œ∏ = (X'X)^(-1)X'y\n",
    "theta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n",
    "\n",
    "print(\"Using Normal Equation:\")\n",
    "print(f\"Estimated intercept: {theta[0, 0]:.3f} (True: {true_intercept})\")\n",
    "print(f\"Estimated slope: {theta[1, 0]:.3f} (True: {true_slope})\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = X_with_intercept @ theta\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "plt.plot(X, y_pred, 'r-', linewidth=2, label='Fitted Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression using Linear Algebra')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate R-squared\n",
    "ss_res = np.sum((y - y_pred)**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"\\nR¬≤ Score: {r_squared:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Logistic Regression Sigmoid\n",
    "print(\"üìä LOGISTIC REGRESSION MATHEMATICS\\n\" + \"=\"*40)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Visualize\n",
    "z = np.linspace(-10, 10, 100)\n",
    "y_sigmoid = sigmoid(z)\n",
    "y_derivative = sigmoid_derivative(z)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Sigmoid function\n",
    "ax1.plot(z, y_sigmoid, 'b-', linewidth=2)\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax1.set_xlabel('z')\n",
    "ax1.set_ylabel('œÉ(z)')\n",
    "ax1.set_title('Sigmoid Function')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Sigmoid derivative\n",
    "ax2.plot(z, y_derivative, 'r-', linewidth=2)\n",
    "ax2.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax2.set_xlabel('z')\n",
    "ax2.set_ylabel(\"œÉ'(z)\")\n",
    "ax2.set_title('Sigmoid Derivative')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Properties:\")\n",
    "print(f\"‚Ä¢ œÉ(0) = {sigmoid(0):.3f}\")\n",
    "print(f\"‚Ä¢ œÉ(‚àû) ‚Üí 1\")\n",
    "print(f\"‚Ä¢ œÉ(-‚àû) ‚Üí 0\")\n",
    "print(f\"‚Ä¢ Maximum derivative at z=0: {sigmoid_derivative(0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Final Project: Neural Network Math\n",
    "\n",
    "### Build a Simple Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project: Neural Network Mathematics\n",
    "print(\"üß† NEURAL NETWORK FROM SCRATCH\\n\" + \"=\"*50)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate=0.1):\n",
    "        # Backward propagation\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        self.dz2 = output - y\n",
    "        self.dW2 = (self.a1.T @ self.dz2) / m\n",
    "        self.db2 = np.sum(self.dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        self.da1 = self.dz2 @ self.W2.T\n",
    "        self.dz1 = self.da1 * self.sigmoid_derivative(self.a1)\n",
    "        self.dW1 = (X.T @ self.dz1) / m\n",
    "        self.db1 = np.sum(self.dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "\n",
    "# XOR problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create and train network\n",
    "nn = SimpleNeuralNetwork(2, 4, 1)\n",
    "losses = []\n",
    "\n",
    "print(\"Training XOR Network...\")\n",
    "for epoch in range(5000):\n",
    "    # Forward pass\n",
    "    output = nn.forward(X)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = np.mean((output - y) ** 2)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    nn.backward(X, y, output)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final predictions\n",
    "final_output = nn.forward(X)\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(\"Input\\t\\tTarget\\tPrediction\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"{X[i]}\\t{y[i, 0]}\\t{final_output[i, 0]:.3f}\")\n",
    "\n",
    "# Visualize training\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "Z = nn.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], colors=['blue', 'red'], alpha=0.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='RdBu', s=100, edgecolor='black')\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('XOR Decision Boundary')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Network successfully learned XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Summary & Next Steps\n",
    "\n",
    "### üèÜ What You've Learned:\n",
    "\n",
    "‚úÖ **Linear Algebra**\n",
    "- Vectors and operations\n",
    "- Matrices and transformations\n",
    "- Eigenvalues/eigenvectors\n",
    "- Applications in ML\n",
    "\n",
    "‚úÖ **Calculus**\n",
    "- Derivatives and optimization\n",
    "- Gradient descent\n",
    "- Partial derivatives\n",
    "- Backpropagation intuition\n",
    "\n",
    "‚úÖ **Key Concepts**\n",
    "- Distance metrics\n",
    "- Dimensionality\n",
    "- Mathematical foundations of ML\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Practice with Code** - Implement algorithms from scratch\n",
    "2. **Learn Statistics** - Next notebook!\n",
    "3. **Apply to ML** - Use math to understand algorithms\n",
    "4. **Deep Learning Math** - More complex architectures\n",
    "\n",
    "### üí° Key Takeaways:\n",
    "\n",
    "- **Math is a tool**, not a barrier\n",
    "- **Visualize everything** to build intuition\n",
    "- **Code makes math concrete**\n",
    "- **You don't need to be a mathematician** to do data science\n",
    "\n",
    "### üìö Resources:\n",
    "\n",
    "- 3Blue1Brown YouTube Channel\n",
    "- Khan Academy Linear Algebra\n",
    "- Fast.ai Computational Linear Algebra\n",
    "- Andrew Ng's ML Course\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You now understand the essential math for data science!\n",
    "\n",
    "Remember: **Every formula has a purpose, every concept has an application.**\n",
    "\n",
    "**Keep learning, keep coding, and keep building!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéä Chapter Complete!\n",
    "print(\"üéä\" * 20)\n",
    "print(\"\\n    üèÜ MATH FOR DATA SCIENCE COMPLETE! üèÜ\")\n",
    "print(\"\\n    You've conquered:\")\n",
    "print(\"    ‚úÖ Vectors & Matrices\")\n",
    "print(\"    ‚úÖ Linear Algebra\")\n",
    "print(\"    ‚úÖ Calculus Basics\")\n",
    "print(\"    ‚úÖ ML Mathematics\")\n",
    "print(\"\\n    Ready for: Statistics for Data Science!\")\n",
    "print(\"\\n\" + \"üéä\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}