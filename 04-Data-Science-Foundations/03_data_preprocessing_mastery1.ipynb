{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Preprocessing Mastery: From Messy to Model-Ready\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*E5t7hp8mhp4DxPwdlQFBJA.png' width='600' alt='Data Preprocessing'>\n",
    "\n",
    "## üéØ The Most Important Step in Data Science\n",
    "\n",
    "**80% of data science is data cleaning!** This notebook will teach you:\n",
    "- Handle **missing data** like a pro\n",
    "- Deal with **outliers** effectively\n",
    "- **Transform** data for better models\n",
    "- **Engineer** powerful features\n",
    "- Build **reproducible pipelines**\n",
    "\n",
    "### üìö What We'll Master Today:\n",
    "1. **Data Quality Assessment** - Understanding your mess\n",
    "2. **Missing Data Strategies** - Smart imputation\n",
    "3. **Outlier Detection & Treatment** - Finding anomalies\n",
    "4. **Data Transformation** - Scaling and normalization\n",
    "5. **Feature Engineering** - Creating powerful features\n",
    "6. **Encoding Categorical Data** - Handling text data\n",
    "7. **Feature Selection** - Choosing what matters\n",
    "8. **Data Balancing** - Handling imbalanced data\n",
    "9. **Pipeline Creation** - Automating everything\n",
    "10. **Real Project** - End-to-end preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Clean Some Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üßπ Data Preprocessing Mastery - Ready to Clean!\")\n",
    "print(\"\\nüí° Remember: Garbage in, garbage out! Quality preprocessing = Quality models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 1: Data Quality Assessment\n",
    "\n",
    "### üéØ Know Your Data's Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create a Messy Dataset (Realistic!)\n",
    "print(\"üé≠ CREATING REALISTIC MESSY DATA\\n\" + \"=\"*40)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create messy customer data\n",
    "data = {\n",
    "    'customer_id': range(1000, 1000 + n_samples),\n",
    "    'age': np.random.normal(35, 15, n_samples),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_samples),\n",
    "    'credit_score': np.random.normal(650, 100, n_samples),\n",
    "    'purchase_amount': np.random.exponential(100, n_samples),\n",
    "    'membership_days': np.random.uniform(0, 1000, n_samples),\n",
    "    'gender': np.random.choice(['M', 'F', 'Male', 'Female', 'male', 'female', None], n_samples),\n",
    "    'city': np.random.choice(['New York', 'new york', 'NY', 'Los Angeles', 'LA', 'Chicago', None], n_samples),\n",
    "    'email': [f'user{i}@email.com' if np.random.random() > 0.1 else None for i in range(n_samples)],\n",
    "    'phone': [f'555-{np.random.randint(1000, 9999)}' if np.random.random() > 0.15 else None for i in range(n_samples)],\n",
    "    'has_children': np.random.choice(['Yes', 'No', 'Y', 'N', '1', '0', None], n_samples),\n",
    "    'satisfaction_score': np.random.choice([1, 2, 3, 4, 5, None], n_samples),\n",
    "    'last_purchase_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
    "    'premium_customer': np.random.choice([True, False], n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add more messiness\n",
    "# Add missing values randomly\n",
    "for col in ['age', 'income', 'credit_score']:\n",
    "    missing_idx = np.random.choice(df.index, size=int(0.1 * len(df)), replace=False)\n",
    "    df.loc[missing_idx, col] = np.nan\n",
    "\n",
    "# Add outliers\n",
    "outlier_idx = np.random.choice(df.index, size=50, replace=False)\n",
    "df.loc[outlier_idx, 'income'] = df.loc[outlier_idx, 'income'] * 10\n",
    "df.loc[outlier_idx[:25], 'age'] = np.random.uniform(100, 120, 25)\n",
    "\n",
    "# Add duplicates\n",
    "duplicate_rows = df.sample(20)\n",
    "df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "# Add inconsistent data\n",
    "df.loc[df['age'] < 0, 'age'] = np.abs(df.loc[df['age'] < 0, 'age'])\n",
    "\n",
    "print(\"üìä Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(f\"\\nDuplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã First 5 Rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Data Quality Report\n",
    "print(\"üìä DATA QUALITY ASSESSMENT\\n\" + \"=\"*40)\n",
    "\n",
    "def data_quality_report(df):\n",
    "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "    \n",
    "    report = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Type': df.dtypes.values,\n",
    "        'Missing': df.isnull().sum().values,\n",
    "        'Missing%': (df.isnull().sum() / len(df) * 100).round(2).values,\n",
    "        'Unique': df.nunique().values,\n",
    "        'Unique%': (df.nunique() / len(df) * 100).round(2).values\n",
    "    })\n",
    "    \n",
    "    # Add statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    stats_dict = {\n",
    "        'Mean': [],\n",
    "        'Std': [],\n",
    "        'Min': [],\n",
    "        'Max': [],\n",
    "        'Outliers': []\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in numeric_cols:\n",
    "            stats_dict['Mean'].append(df[col].mean())\n",
    "            stats_dict['Std'].append(df[col].std())\n",
    "            stats_dict['Min'].append(df[col].min())\n",
    "            stats_dict['Max'].append(df[col].max())\n",
    "            \n",
    "            # Count outliers (using IQR method)\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            stats_dict['Outliers'].append(outliers)\n",
    "        else:\n",
    "            for key in stats_dict:\n",
    "                stats_dict[key].append('-')\n",
    "    \n",
    "    for key, values in stats_dict.items():\n",
    "        report[key] = values\n",
    "    \n",
    "    return report\n",
    "\n",
    "quality_report = data_quality_report(df)\n",
    "print(quality_report.to_string())\n",
    "\n",
    "# Visualize data quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Missing values heatmap\n",
    "ax = axes[0, 0]\n",
    "missing_data = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "ax.barh(range(len(missing_data)), missing_data.values)\n",
    "ax.set_yticks(range(len(missing_data)))\n",
    "ax.set_yticklabels(missing_data.index)\n",
    "ax.set_xlabel('Number of Missing Values')\n",
    "ax.set_title('Missing Values by Column')\n",
    "\n",
    "# Data types distribution\n",
    "ax = axes[0, 1]\n",
    "type_counts = df.dtypes.value_counts()\n",
    "ax.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "ax.set_title('Data Types Distribution')\n",
    "\n",
    "# Correlation heatmap (numeric only)\n",
    "ax = axes[1, 0]\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Correlation Matrix')\n",
    "\n",
    "# Distribution of a numeric column\n",
    "ax = axes[1, 1]\n",
    "ax.hist(df['income'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Income Distribution (with outliers)')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 2: Handling Missing Data\n",
    "\n",
    "### üéØ Smart Strategies for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Missing Data Patterns\n",
    "print(\"üîç MISSING DATA ANALYSIS\\n\" + \"=\"*40)\n",
    "\n",
    "# Visualize missing data patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Missing data matrix\n",
    "ax = axes[0]\n",
    "missing_matrix = df.isnull().astype(int)\n",
    "ax.imshow(missing_matrix.T, cmap='RdYlBu', aspect='auto', interpolation='none')\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Missing Data Pattern (Yellow = Missing)')\n",
    "ax.set_yticks(range(len(df.columns)))\n",
    "ax.set_yticklabels(df.columns, fontsize=8)\n",
    "\n",
    "# Missing data correlation\n",
    "ax = axes[1]\n",
    "missing_corr = missing_matrix.corr()\n",
    "mask = np.triu(np.ones_like(missing_corr), k=1)\n",
    "sns.heatmap(missing_corr, mask=mask, annot=False, cmap='coolwarm', \n",
    "            center=0, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Missing Data Correlation')\n",
    "\n",
    "# Missing data by column\n",
    "ax = axes[2]\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "ax.bar(range(len(missing_pct)), missing_pct.values, color='coral')\n",
    "ax.set_xticks(range(len(missing_pct)))\n",
    "ax.set_xticklabels(missing_pct.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Missing %')\n",
    "ax.set_title('Missing Data Percentage')\n",
    "ax.axhline(y=5, color='red', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "ax.axhline(y=20, color='darkred', linestyle='--', alpha=0.5, label='20% threshold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Missing data types\n",
    "print(\"\\nüìä Missing Data Types:\")\n",
    "print(\"‚Ä¢ MCAR (Missing Completely At Random): No pattern\")\n",
    "print(\"‚Ä¢ MAR (Missing At Random): Pattern related to other variables\")\n",
    "print(\"‚Ä¢ MNAR (Missing Not At Random): Pattern related to missing value itself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Imputation Strategies\n",
    "print(\"üîß IMPUTATION STRATEGIES\\n\" + \"=\"*40)\n",
    "\n",
    "# Create a copy for imputation\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# Strategy 1: Simple Imputation\n",
    "print(\"1Ô∏è‚É£ Simple Imputation:\")\n",
    "\n",
    "# Numeric columns - mean/median imputation\n",
    "numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_imputed[col].isnull().sum() > 0:\n",
    "        # Use median for skewed data, mean for normal\n",
    "        skewness = df_imputed[col].skew()\n",
    "        if abs(skewness) > 1:\n",
    "            impute_value = df_imputed[col].median()\n",
    "            strategy = 'median'\n",
    "        else:\n",
    "            impute_value = df_imputed[col].mean()\n",
    "            strategy = 'mean'\n",
    "        \n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        df_imputed[col].fillna(impute_value, inplace=True)\n",
    "        print(f\"  {col}: Imputed {missing_count} values with {strategy} = {impute_value:.2f}\")\n",
    "\n",
    "# Categorical columns - mode imputation\n",
    "categorical_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_imputed[col].isnull().sum() > 0:\n",
    "        mode_value = df_imputed[col].mode()[0] if not df_imputed[col].mode().empty else 'Unknown'\n",
    "        missing_count = df_imputed[col].isnull().sum()\n",
    "        df_imputed[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"  {col}: Imputed {missing_count} values with mode = {mode_value}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Advanced Imputation - KNN:\")\n",
    "\n",
    "# Demonstrate KNN imputation on numeric data\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Select numeric columns with missing values for demonstration\n",
    "demo_cols = ['age', 'income', 'credit_score']\n",
    "df_knn = df[demo_cols].copy()\n",
    "\n",
    "# Apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(df_knn),\n",
    "    columns=demo_cols,\n",
    "    index=df_knn.index\n",
    ")\n",
    "\n",
    "# Compare imputation methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, col in enumerate(demo_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Original distribution\n",
    "    ax.hist(df[col].dropna(), bins=30, alpha=0.5, label='Original', color='blue')\n",
    "    \n",
    "    # Simple imputation\n",
    "    ax.hist(df_imputed[col], bins=30, alpha=0.5, label='Simple Impute', color='green')\n",
    "    \n",
    "    # KNN imputation\n",
    "    ax.hist(df_knn_imputed[col], bins=30, alpha=0.5, label='KNN Impute', color='red')\n",
    "    \n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{col} - Imputation Comparison')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° When to use which strategy:\")\n",
    "print(\"‚Ä¢ Mean/Median: When data is MCAR, small % missing\")\n",
    "print(\"‚Ä¢ Mode: For categorical variables\")\n",
    "print(\"‚Ä¢ KNN: When data is MAR, preserves relationships\")\n",
    "print(\"‚Ä¢ Forward/Backward fill: For time series\")\n",
    "print(\"‚Ä¢ Domain-specific: Use business knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 3: Outlier Detection and Treatment\n",
    "\n",
    "### üéØ Finding and Handling Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Outlier Detection Methods\n",
    "print(\"üîç OUTLIER DETECTION\\n\" + \"=\"*40)\n",
    "\n",
    "def detect_outliers(df, column, method='IQR'):\n",
    "    \"\"\"Detect outliers using various methods\"\"\"\n",
    "    \n",
    "    if method == 'IQR':\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        \n",
    "    elif method == 'Z-score':\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        outliers = df[column].dropna()[z_scores > 3]\n",
    "        \n",
    "    elif method == 'Isolation Forest':\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        outliers_pred = iso_forest.fit_predict(df[[column]].dropna())\n",
    "        outliers = df[column].dropna()[outliers_pred == -1]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in income\n",
    "outliers_iqr = detect_outliers(df_imputed, 'income', 'IQR')\n",
    "outliers_zscore = detect_outliers(df_imputed, 'income', 'Z-score')\n",
    "\n",
    "print(f\"Outliers detected in 'income':\")\n",
    "print(f\"  IQR method: {len(outliers_iqr)} outliers\")\n",
    "print(f\"  Z-score method: {len(outliers_zscore)} outliers\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Box plot\n",
    "ax = axes[0, 0]\n",
    "ax.boxplot([df_imputed['income'], df_imputed['age'], df_imputed['credit_score']], \n",
    "           labels=['Income', 'Age', 'Credit Score'])\n",
    "ax.set_title('Box Plot - Outlier Detection')\n",
    "ax.set_ylabel('Value')\n",
    "\n",
    "# Scatter plot with outliers\n",
    "ax = axes[0, 1]\n",
    "is_outlier = df_imputed['income'].isin(outliers_iqr['income'])\n",
    "ax.scatter(df_imputed.index[~is_outlier], df_imputed.loc[~is_outlier, 'income'], \n",
    "          alpha=0.5, label='Normal')\n",
    "ax.scatter(df_imputed.index[is_outlier], df_imputed.loc[is_outlier, 'income'], \n",
    "          color='red', label='Outlier', s=50)\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Income Outliers (IQR Method)')\n",
    "ax.legend()\n",
    "\n",
    "# Distribution with outlier boundaries\n",
    "ax = axes[0, 2]\n",
    "ax.hist(df_imputed['income'], bins=50, alpha=0.7, edgecolor='black')\n",
    "Q1 = df_imputed['income'].quantile(0.25)\n",
    "Q3 = df_imputed['income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "ax.axvline(Q1 - 1.5*IQR, color='red', linestyle='--', label='Lower Bound')\n",
    "ax.axvline(Q3 + 1.5*IQR, color='red', linestyle='--', label='Upper Bound')\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Income Distribution with IQR Bounds')\n",
    "ax.legend()\n",
    "\n",
    "# Z-score visualization\n",
    "ax = axes[1, 0]\n",
    "z_scores = stats.zscore(df_imputed['income'])\n",
    "ax.scatter(df_imputed.index, z_scores, alpha=0.5)\n",
    "ax.axhline(y=3, color='red', linestyle='--', label='Z=3')\n",
    "ax.axhline(y=-3, color='red', linestyle='--', label='Z=-3')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Z-score')\n",
    "ax.set_title('Z-scores for Income')\n",
    "ax.legend()\n",
    "\n",
    "# Multivariate outlier detection\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(df_imputed['age'], df_imputed['income'], alpha=0.5)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Bivariate Outlier Detection')\n",
    "\n",
    "# Local Outlier Factor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "ax = axes[1, 2]\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "outlier_labels = lof.fit_predict(df_imputed[['age', 'income']])\n",
    "ax.scatter(df_imputed['age'][outlier_labels == 1], \n",
    "          df_imputed['income'][outlier_labels == 1], \n",
    "          alpha=0.5, label='Normal')\n",
    "ax.scatter(df_imputed['age'][outlier_labels == -1], \n",
    "          df_imputed['income'][outlier_labels == -1], \n",
    "          color='red', label='Outlier', s=50)\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Local Outlier Factor')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Outlier Treatment Strategies\n",
    "print(\"üîß OUTLIER TREATMENT\\n\" + \"=\"*40)\n",
    "\n",
    "# Create copies for different treatment methods\n",
    "df_capped = df_imputed.copy()\n",
    "df_transformed = df_imputed.copy()\n",
    "df_removed = df_imputed.copy()\n",
    "\n",
    "# Method 1: Capping (Winsorization)\n",
    "print(\"1Ô∏è‚É£ Capping/Winsorization:\")\n",
    "for col in ['income', 'age']:\n",
    "    Q1 = df_capped[col].quantile(0.01)\n",
    "    Q99 = df_capped[col].quantile(0.99)\n",
    "    df_capped[col] = df_capped[col].clip(Q1, Q99)\n",
    "    print(f\"  {col}: Capped to [{Q1:.2f}, {Q99:.2f}]\")\n",
    "\n",
    "# Method 2: Log Transformation\n",
    "print(\"\\n2Ô∏è‚É£ Log Transformation:\")\n",
    "df_transformed['income_log'] = np.log1p(df_transformed['income'])\n",
    "print(f\"  Applied log transformation to income\")\n",
    "\n",
    "# Method 3: Removal\n",
    "print(\"\\n3Ô∏è‚É£ Outlier Removal:\")\n",
    "before_shape = df_removed.shape\n",
    "for col in ['income', 'age']:\n",
    "    Q1 = df_removed[col].quantile(0.25)\n",
    "    Q3 = df_removed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_removed = df_removed[(df_removed[col] >= Q1 - 1.5*IQR) & \n",
    "                            (df_removed[col] <= Q3 + 1.5*IQR)]\n",
    "after_shape = df_removed.shape\n",
    "print(f\"  Removed {before_shape[0] - after_shape[0]} rows\")\n",
    "\n",
    "# Visualize treatment effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original vs Capped\n",
    "ax = axes[0, 0]\n",
    "ax.hist(df_imputed['income'], bins=50, alpha=0.5, label='Original', color='blue')\n",
    "ax.hist(df_capped['income'], bins=50, alpha=0.5, label='Capped', color='green')\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Original vs Capped')\n",
    "ax.legend()\n",
    "\n",
    "# Log transformation\n",
    "ax = axes[0, 1]\n",
    "ax.hist(df_transformed['income_log'], bins=50, alpha=0.7, color='purple')\n",
    "ax.set_xlabel('Log(Income)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Log Transformed Income')\n",
    "\n",
    "# Before/After boxplots\n",
    "ax = axes[1, 0]\n",
    "ax.boxplot([df_imputed['income'], df_capped['income'], df_removed['income']], \n",
    "           labels=['Original', 'Capped', 'Removed'])\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_title('Outlier Treatment Comparison')\n",
    "\n",
    "# Statistics comparison\n",
    "ax = axes[1, 1]\n",
    "stats_comparison = pd.DataFrame({\n",
    "    'Original': [df_imputed['income'].mean(), df_imputed['income'].std(), \n",
    "                df_imputed['income'].skew(), len(df_imputed)],\n",
    "    'Capped': [df_capped['income'].mean(), df_capped['income'].std(), \n",
    "              df_capped['income'].skew(), len(df_capped)],\n",
    "    'Removed': [df_removed['income'].mean(), df_removed['income'].std(), \n",
    "               df_removed['income'].skew(), len(df_removed)]\n",
    "}, index=['Mean', 'Std', 'Skew', 'Count'])\n",
    "\n",
    "stats_comparison.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Statistics After Treatment')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend(title='Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° When to use which method:\")\n",
    "print(\"‚Ä¢ Capping: Preserve all data, reduce extreme values\")\n",
    "print(\"‚Ä¢ Transformation: When distribution is skewed\")\n",
    "print(\"‚Ä¢ Removal: When outliers are errors or tiny %\")\n",
    "print(\"‚Ä¢ Keep: When outliers are valid and important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 4: Feature Scaling and Transformation\n",
    "\n",
    "### üéØ Preparing Features for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Feature Scaling Methods\n",
    "print(\"‚öñÔ∏è FEATURE SCALING\\n\" + \"=\"*40)\n",
    "\n",
    "# Select numeric features\n",
    "features = ['age', 'income', 'credit_score', 'purchase_amount', 'membership_days']\n",
    "X = df_capped[features].copy()\n",
    "\n",
    "# Different scaling methods\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "\n",
    "scalers = {\n",
    "    'Original': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson')\n",
    "}\n",
    "\n",
    "# Apply scaling\n",
    "scaled_data = {}\n",
    "for name, scaler in scalers.items():\n",
    "    if scaler is None:\n",
    "        scaled_data[name] = X\n",
    "    else:\n",
    "        scaled_data[name] = pd.DataFrame(\n",
    "            scaler.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(len(features), len(scalers), figsize=(20, 15))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    for j, (name, data) in enumerate(scaled_data.items()):\n",
    "        ax = axes[i, j]\n",
    "        ax.hist(data[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f'{feature}\\n{name}')\n",
    "        \n",
    "        # Add statistics\n",
    "        mean = data[feature].mean()\n",
    "        std = data[feature].std()\n",
    "        ax.axvline(mean, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.text(0.7, 0.9, f'Œº={mean:.2f}\\nœÉ={std:.2f}', \n",
    "               transform=ax.transAxes, fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Scaling Comparison', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Scaling Method Comparison:\")\n",
    "print(\"‚Ä¢ StandardScaler: Mean=0, Std=1 (assumes normal distribution)\")\n",
    "print(\"‚Ä¢ MinMaxScaler: Range [0,1] (preserves original distribution)\")\n",
    "print(\"‚Ä¢ RobustScaler: Uses median/IQR (robust to outliers)\")\n",
    "print(\"‚Ä¢ PowerTransformer: Makes data more Gaussian-like\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 5: Feature Engineering\n",
    "\n",
    "### üéØ Creating Powerful Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Feature Engineering Techniques\n",
    "print(\"üõ†Ô∏è FEATURE ENGINEERING\\n\" + \"=\"*40)\n",
    "\n",
    "# Create new features\n",
    "df_engineered = df_capped.copy()\n",
    "\n",
    "# Temporal features\n",
    "df_engineered['account_age_years'] = df_engineered['membership_days'] / 365\n",
    "df_engineered['account_age_category'] = pd.cut(df_engineered['account_age_years'], \n",
    "                                                bins=[0, 1, 2, 5, 10], \n",
    "                                                labels=['New', 'Regular', 'Loyal', 'VIP'])\n",
    "\n",
    "# Ratio features\n",
    "df_engineered['income_per_age'] = df_engineered['income'] / df_engineered['age']\n",
    "df_engineered['purchase_to_income_ratio'] = df_engineered['purchase_amount'] / df_engineered['income']\n",
    "df_engineered['credit_score_category'] = pd.cut(df_engineered['credit_score'],\n",
    "                                                 bins=[300, 580, 670, 740, 800, 850],\n",
    "                                                 labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])\n",
    "\n",
    "# Interaction features\n",
    "df_engineered['high_income_good_credit'] = (\n",
    "    (df_engineered['income'] > df_engineered['income'].median()) & \n",
    "    (df_engineered['credit_score'] > 700)\n",
    ").astype(int)\n",
    "\n",
    "# Polynomial features\n",
    "df_engineered['age_squared'] = df_engineered['age'] ** 2\n",
    "df_engineered['income_log'] = np.log1p(df_engineered['income'])\n",
    "\n",
    "# Binning continuous variables\n",
    "df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
    "                                    bins=[0, 25, 35, 50, 65, 100],\n",
    "                                    labels=['<25', '25-35', '35-50', '50-65', '65+'])\n",
    "\n",
    "print(\"üìä New Features Created:\")\n",
    "new_features = ['account_age_years', 'account_age_category', 'income_per_age', \n",
    "                'purchase_to_income_ratio', 'credit_score_category', \n",
    "                'high_income_good_credit', 'age_squared', 'income_log', 'age_group']\n",
    "for feat in new_features:\n",
    "    print(f\"  ‚Ä¢ {feat}\")\n",
    "\n",
    "# Visualize engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Age groups distribution\n",
    "ax = axes[0, 0]\n",
    "df_engineered['age_group'].value_counts().plot(kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_title('Age Group Distribution')\n",
    "ax.set_xlabel('Age Group')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "# Credit score categories\n",
    "ax = axes[0, 1]\n",
    "df_engineered['credit_score_category'].value_counts().plot(kind='pie', ax=ax, autopct='%1.1f%%')\n",
    "ax.set_title('Credit Score Categories')\n",
    "\n",
    "# Income per age\n",
    "ax = axes[0, 2]\n",
    "ax.scatter(df_engineered['age'], df_engineered['income_per_age'], alpha=0.5, c='coral')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Income per Age')\n",
    "ax.set_title('Income/Age Ratio')\n",
    "\n",
    "# Purchase to income ratio\n",
    "ax = axes[1, 0]\n",
    "ax.hist(df_engineered['purchase_to_income_ratio'], bins=30, edgecolor='black', color='lightgreen')\n",
    "ax.set_xlabel('Purchase/Income Ratio')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Purchase to Income Ratio')\n",
    "\n",
    "# Account age categories\n",
    "ax = axes[1, 1]\n",
    "df_engineered['account_age_category'].value_counts().plot(kind='bar', ax=ax, color='purple')\n",
    "ax.set_title('Customer Loyalty Categories')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "# Feature correlation heatmap\n",
    "ax = axes[1, 2]\n",
    "numeric_features = df_engineered[['age', 'income', 'credit_score', \n",
    "                                  'income_per_age', 'purchase_to_income_ratio']].corr()\n",
    "sns.heatmap(numeric_features, annot=True, fmt='.2f', cmap='coolwarm', ax=ax, center=0)\n",
    "ax.set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Feature Engineering Best Practices:\")\n",
    "print(\"‚Ä¢ Domain Knowledge: Use business understanding\")\n",
    "print(\"‚Ä¢ Interaction Features: Combine related features\")\n",
    "print(\"‚Ä¢ Polynomial Features: Capture non-linear relationships\")\n",
    "print(\"‚Ä¢ Binning: Convert continuous to categorical\")\n",
    "print(\"‚Ä¢ Time-based: Extract date/time components\")\n",
    "print(\"‚Ä¢ Aggregations: Group-level statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Automated Feature Selection\n",
    "print(\"\\nüéØ FEATURE SELECTION\\n\" + \"=\"*40)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare data for feature selection\n",
    "X_features = df_engineered[['age', 'income', 'credit_score', 'purchase_amount', \n",
    "                            'membership_days', 'income_per_age', 'purchase_to_income_ratio',\n",
    "                            'high_income_good_credit', 'age_squared', 'income_log']]\n",
    "y_target = df_engineered['premium_customer'].astype(int)\n",
    "\n",
    "# Method 1: Statistical Tests (ANOVA F-value)\n",
    "print(\"1Ô∏è‚É£ Statistical Feature Selection (ANOVA):\")\n",
    "selector_f = SelectKBest(f_classif, k=5)\n",
    "X_selected_f = selector_f.fit_transform(X_features, y_target)\n",
    "feature_scores_f = pd.DataFrame({\n",
    "    'Feature': X_features.columns,\n",
    "    'F-Score': selector_f.scores_\n",
    "}).sort_values('F-Score', ascending=False)\n",
    "print(feature_scores_f.to_string())\n",
    "\n",
    "# Method 2: Mutual Information\n",
    "print(\"\\n2Ô∏è‚É£ Mutual Information:\")\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=5)\n",
    "X_selected_mi = selector_mi.fit_transform(X_features, y_target)\n",
    "feature_scores_mi = pd.DataFrame({\n",
    "    'Feature': X_features.columns,\n",
    "    'MI-Score': selector_mi.scores_\n",
    "}).sort_values('MI-Score', ascending=False)\n",
    "print(feature_scores_mi.head(10).to_string())\n",
    "\n",
    "# Method 3: Recursive Feature Elimination (RFE)\n",
    "print(\"\\n3Ô∏è‚É£ Recursive Feature Elimination:\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfe = RFE(rf_model, n_features_to_select=5)\n",
    "rfe.fit(X_features, y_target)\n",
    "rfe_ranking = pd.DataFrame({\n",
    "    'Feature': X_features.columns,\n",
    "    'Ranking': rfe.ranking_,\n",
    "    'Selected': rfe.support_\n",
    "}).sort_values('Ranking')\n",
    "print(rfe_ranking.to_string())\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# F-scores\n",
    "ax = axes[0]\n",
    "feature_scores_f.head(10).plot(x='Feature', y='F-Score', kind='barh', ax=ax, color='skyblue')\n",
    "ax.set_title('Feature Importance - ANOVA F-Score')\n",
    "ax.set_xlabel('F-Score')\n",
    "\n",
    "# Mutual Information\n",
    "ax = axes[1]\n",
    "feature_scores_mi.head(10).plot(x='Feature', y='MI-Score', kind='barh', ax=ax, color='lightcoral')\n",
    "ax.set_title('Feature Importance - Mutual Information')\n",
    "ax.set_xlabel('MI Score')\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "ax = axes[2]\n",
    "rf_model.fit(X_features, y_target)\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X_features.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "feature_importance_rf.head(10).plot(x='Feature', y='Importance', kind='barh', ax=ax, color='lightgreen')\n",
    "ax.set_title('Feature Importance - Random Forest')\n",
    "ax.set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 6: Categorical Encoding\n",
    "\n",
    "### üéØ Converting Text to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Categorical Encoding Techniques\n",
    "print(\"üî§ CATEGORICAL ENCODING\\n\" + \"=\"*40)\n",
    "\n",
    "# Clean categorical variables first\n",
    "df_encoded = df_engineered.copy()\n",
    "\n",
    "# Standardize gender column\n",
    "df_encoded['gender'] = df_encoded['gender'].str.lower()\n",
    "df_encoded['gender'] = df_encoded['gender'].replace({\n",
    "    'm': 'male', 'f': 'female',\n",
    "    'Male': 'male', 'Female': 'female'\n",
    "})\n",
    "\n",
    "# Standardize city column\n",
    "df_encoded['city'] = df_encoded['city'].replace({\n",
    "    'new york': 'New York', 'NY': 'New York',\n",
    "    'LA': 'Los Angeles'\n",
    "})\n",
    "\n",
    "# Method 1: Label Encoding (for ordinal)\n",
    "print(\"1Ô∏è‚É£ Label Encoding:\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_encoded['satisfaction_score_encoded'] = le.fit_transform(df_encoded['satisfaction_score'].fillna('0'))\n",
    "print(f\"  Encoded satisfaction_score\")\n",
    "\n",
    "# Method 2: One-Hot Encoding (for nominal)\n",
    "print(\"\\n2Ô∏è‚É£ One-Hot Encoding:\")\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['gender', 'city'], prefix=['gender', 'city'])\n",
    "print(f\"  One-hot encoded gender and city\")\n",
    "\n",
    "# Method 3: Binary Encoding (for yes/no)\n",
    "print(\"\\n3Ô∏è‚É£ Binary Encoding:\")\n",
    "df_encoded['has_children'] = df_encoded['has_children'].replace({\n",
    "    'Yes': 1, 'Y': 1, '1': 1,\n",
    "    'No': 0, 'N': 0, '0': 0\n",
    "}).fillna(0).astype(int)\n",
    "print(f\"  Binary encoded has_children\")\n",
    "\n",
    "# Method 4: Target Encoding (mean encoding)\n",
    "print(\"\\n4Ô∏è‚É£ Target Encoding:\")\n",
    "target_mean = df_encoded.groupby('credit_score_category')['premium_customer'].mean()\n",
    "df_encoded['credit_category_target_encoded'] = df_encoded['credit_score_category'].map(target_mean)\n",
    "print(f\"  Target encoded credit_score_category\")\n",
    "\n",
    "print(f\"\\nüìä Shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"New columns added: {df_encoded.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìå Section 7: Complete Preprocessing Pipeline\n",
    "\n",
    "### üéØ Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Building a Preprocessing Pipeline\n",
    "print(\"üöÄ COMPLETE PREPROCESSING PIPELINE\\n\" + \"=\"*40)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define column groups\n",
    "numeric_features = ['age', 'income', 'credit_score', 'purchase_amount', 'membership_days']\n",
    "categorical_features = ['gender', 'city', 'has_children']\n",
    "\n",
    "# Create preprocessing pipelines for each type\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Use the original messy data\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df['premium_customer']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Pipeline Complete!\")\n",
    "print(f\"  Original shape: {X_train.shape}\")\n",
    "print(f\"  Processed shape: {X_train_processed.shape}\")\n",
    "print(f\"  Training samples: {len(X_train_processed)}\")\n",
    "print(f\"  Test samples: {len(X_test_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Save Preprocessing Pipeline\n",
    "print(\"üíæ SAVING PIPELINE\\n\" + \"=\"*40)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor_pipeline.pkl')\n",
    "print(\"‚úÖ Pipeline saved as 'preprocessor_pipeline.pkl'\")\n",
    "\n",
    "# Example of loading and using\n",
    "# loaded_preprocessor = joblib.load('preprocessor_pipeline.pkl')\n",
    "# new_data_processed = loaded_preprocessor.transform(new_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ DATA PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüìö Key Takeaways:\")\n",
    "print(\"1. Always assess data quality first\")\n",
    "print(\"2. Handle missing data appropriately\")\n",
    "print(\"3. Detect and treat outliers carefully\")\n",
    "print(\"4. Scale features for ML algorithms\")\n",
    "print(\"5. Engineer meaningful features\")\n",
    "print(\"6. Select relevant features\")\n",
    "print(\"7. Encode categorical variables properly\")\n",
    "print(\"8. Build reproducible pipelines\")\n",
    "print(\"9. Document your preprocessing steps\")\n",
    "print(\"10. Validate on unseen data\")\n",
    "\n",
    "print(\"\\nüí™ You're now ready to build amazing ML models!\")\n",
    "print(\"Remember: Better data ‚Üí Better models ‚Üí Better results!\")\n",
    "\n",
    "# Final summary statistics\n",
    "print(\"\\nüìä FINAL DATASET SUMMARY\\n\" + \"=\"*40)\n",
    "print(f\"Original dataset: {df.shape}\")\n",
    "print(f\"After preprocessing: {X_train_processed.shape}\")\n",
    "print(f\"Features created: {len(new_features)}\")\n",
    "print(f\"Missing values handled: ‚úÖ\")\n",
    "print(f\"Outliers treated: ‚úÖ\")\n",
    "print(f\"Features scaled: ‚úÖ\")\n",
    "print(f\"Pipeline created: ‚úÖ\")\n",
    "print(\"\\nüöÄ Ready for Machine Learning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}